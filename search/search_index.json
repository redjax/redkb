{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"RedKB","text":"<p>My personal knowledgebase. Use the sections along the top (i.e. <code>Programming</code>) to navigate areas of the KB. Check the tags section to quickly find articles on a specific topic.</p> <p>Find a mistake?</p> <p>Did you spot a spelling error or some other inaccuracy? Feel free to open a PR on the Github repository for this site! I welcome contributions \ud83d\ude0a</p> Warning on content accuracy &amp; freshness <p>I make no guarantees regarding total accuracy, relevant/updated information, or topic depth for any of the content on this site. My goal is to keep reference notes for myself in a place accessible over the Internet in an easily-editable format (i.e. the Github repository).</p> <p>I aim to be accurate, and notes I reference frequently will be updated more often. Each article on this site has a published and last modified timestamp at the bottom of the page, which you should take note of when referencing content on this site.</p>"},{"location":"index.html#quick-links","title":"Quick Links","text":"<p>The menus below are like a table of contents for the sections I most frequently reference or send to other people. Unless you're coming here for something specific, I recommend browsing through the sections at the top.</p> \ud83d\udc0d Python <ul> <li>Python standard project files<ul> <li>Python Docker image templates</li> <li><code>ruff</code><ul> <li><code>ruff</code> pyproject.toml <code>[tool.ruff]</code> section</li> </ul> </li> <li>Python <code>.gitignore</code> file</li> <li><code>dynaconf</code></li> <li><code>nox</code><ul> <li><code>noxfile.py</code></li> <li><code>nox_extra</code>: Make <code>nox</code> more modular</li> </ul> </li> </ul> </li> <li>Manage your Python install with <code>pyenv</code></li> <li>Manage your Python projects with the <code>pdm</code> package manager</li> <li>Add logging to your project with the stdlib <code>logging</code> module</li> </ul> \ud83d\udc1a Shells \ud83c\udd7f\ufe0f Powershell\ud83d\udcb2 Bash <ul> <li>Powershell profiles</li> <li>Powershell code snippets</li> <li>Powershell modules</li> </ul> <ul> <li>Bash code snippets</li> </ul> \ud83d\udee0\ufe0f Utilities \ud83d\udd11 SSH\u2197\ufe0f rsync <ul> <li>Understanding the difference between public and private SSH keys</li> <li>Create an SSH keypair </li> <li>Install SSH key on remote machine</li> <li>SSH chmod permissions for <code>~/.ssh/</code> directory and key files</li> </ul> <ul> <li><code>rsync</code> CLI args</li> <li>Create alias for <code>cp</code> to call <code>rsync</code> instead</li> <li>Example <code>rsync</code> commands</li> </ul> \ud83d\udcdd Templates \ud83d\udc0b Docker Templates <ul> <li>Databases<ul> <li>postgres</li> <li>mariadb</li> <li>redis</li> </ul> </li> <li>Python containers</li> </ul>"},{"location":"index.html#about","title":"About","text":"<p>This is a hobby project, something I maintain as I have time/interest. The notes on this site are mostly for myself, but are sometimes helpful to others. The site is loosely based on the concept of a mind garden.</p> <p>I frequently write Markdown notes in whatever Markdown notes app has most recently caught my attention (honorable mentions to Obsidian and Logseq). I will occasionally dump pages/sections into this KB.</p> <p>I mainly use Linux for my homelab, but work in a Windows environment. I code mainly in Python, Bash, and Powershell, and spend most of my time on the Linux side. The contents of this site skew more heavily towards Linux and the scripting languages I use, but there are also articles on cross-platform tools like SSH and Docker.</p> <p>\ud83d\udee0\ufe0f Built with</p> <p>This site was built with <code>MkDocs</code> and <code>Material for MkDocs</code>. Check out the source code on Github</p>"},{"location":"linux/index.html","title":"Linux","text":"<p>Check out the Bash snippets page for small bits of Bash to help you write scripts &amp; use the shell.</p> <p>For more detailed write-ups on troubleshooting, configuring, &amp; using Bash, check the Bash page</p>","tags":["linux"]},{"location":"linux/index.html#links","title":"Links","text":"<ul> <li>The Linux Foundation</li> <li>Linux.org</li> </ul>","tags":["linux"]},{"location":"linux/Tips%20%26%20Tricks/index.html","title":"Tips &amp; Tricks","text":"<p>Miscellaneous Linux tips &amp; tricks. If an example would only work on a specific OS (i.e. Debian-family only), there will be a message stating so; otherwise, these commands should work across different Linux OSes.</p>","tags":["linux","bash","reference","debian","fedora","arch"]},{"location":"linux/Tips%20%26%20Tricks/index.html#configurations","title":"Configurations","text":"","tags":["linux","bash","reference","debian","fedora","arch"]},{"location":"linux/Tips%20%26%20Tricks/index.html#enable-cli-boot-disable-gui","title":"Enable CLI boot (disable GUI)","text":"<p>Set your machine to \"CLI\" boot, where the computer will start at a shell prompt and without a GUI:</p> Set CLI boot<pre><code>sudo systemctl set-default multi-user.target\n</code></pre> <p>To undo this change, run:</p> Set GUI boot<pre><code>sudo systemctl set-default graphical.target\n</code></pre>","tags":["linux","bash","reference","debian","fedora","arch"]},{"location":"linux/Tips%20%26%20Tricks/index.html#allow-passwordless-sudo","title":"Allow passwordless sudo","text":"<p>Warning</p> <p>This is not recommended! This configuration is insecure, and allows running all commands as root without entering a password.</p> <p>There is almost no environment where this is suitable or advisable. The main reason this is documented is so you know where to undo it if you come across a machine that allows sudo commands without a password.</p> <p>Allowing <code>sudo</code> commands without a password is very risky and inadvisable. This is the state most Windows machines run in (the user is admin/root by default). With the guardrails off, you are free to mistakenly edit or delete files/directories, and your machine is highly insecure; any attacker able to access the user's account could run any command as root without being prompted for a password.</p> <p>To grant a user password-less <code>sudo</code> rights, run the command <code>visudo</code> (the <code>sudo</code> package must be installed) as root/with <code>sudo</code>, and add the following below the line that reads <code># Allow members of group sudo to execute any command</code>:</p> Allow passwordless sudo<pre><code># Allow passwordless sudo for specified user(s)\n&lt;username&gt;    ALL=(ALL) NOPASSWD:ALL\n</code></pre>","tags":["linux","bash","reference","debian","fedora","arch"]},{"location":"linux/bash/index.html","title":"Bash","text":"<p>Check out the Bash snippets page for small bits of Bash to help you write scripts &amp; use the shell.</p> <p>Warning</p> <p>In progress...</p>","tags":["linux","bash"]},{"location":"linux/bash/index.html#bash-cheat-sheet","title":"Bash Cheat Sheet","text":"<p>A table of common commands &amp; variables I use.</p> <p>Tip</p> <p>\ud83d\udd17 See more Bash snippets</p> Command Example Description <code>pwd</code> <code>pwd # /path/where/shell/is</code> The current working directory, i.e. 'this path' where your shell is. <code>$CWD</code> <code>CWD=$(pwd) &amp;&amp; echo \"$CWD\"</code> Create a variable <code>$CWD</code>, which is your current working directory (the path where you shell is). <code>$THIS_DIR</code> <code>THIS_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" &amp;&amp; pwd)\"</code> A variable with the path where a script that is called exists. Different from <code>$CWD</code>. Where <code>$CWD</code> is the path the shell was called from, <code>$THIS_DIR</code> is the actual path to the script that is being executed where the variable is declared. <code>mkdir -p</code> <code>mkdir -p ~/path/that/does-not/exist</code> Create a nested directory path where some or all of the parents do not exist yet. Use <code>-pv</code> to show verbose output. <code>exec $SHELL</code> <code>exec $SHELL</code> Reload the current shell. Useful after modifying auto-sourced files like <code>~/.bashrc</code>. Equivalent to <code>~/.bashrc</code> and <code>source ~/.bashrc</code>. <code>while true; do &lt;something&gt;; done</code> <code>while true; do echo \"Loop!\" &amp;&amp; sleep 2; done</code> Loop/repeat a command or phrase until you cancel the command with <code>CTRL-D</code> or <code>CTRL-C</code>, or the loop finishes, exits, or errors.","tags":["linux","bash"]},{"location":"linux/deb_family/index.html","title":"Debian family","text":"<p>For descendents of Debian Linux, or Debian itself.</p> <p>Todo</p> <ul> <li> Make Debian docs</li> <li> Make Ubuntu docs</li> <li> Make Pop_OS! docs</li> </ul>"},{"location":"linux/deb_family/networking.html","title":"Debian Networking","text":"<p>...</p>","tags":["linux","debian","networking"]},{"location":"linux/deb_family/networking.html#set-a-static-ip","title":"Set a static IP","text":"","tags":["linux","debian","networking"]},{"location":"linux/deb_family/networking.html#with-the-cli","title":"With the CLI","text":"<p>First, get your network interface name by running <code>ip a</code>. Look for a line that starts with <code>eno1</code>, <code>ens18</code>, <code>eth01</code>, etc. This interface should have an IP address on your subnet (i.e. for a <code>192.168.1.0/24</code> network, you might see an address like <code>192.168.1.102</code>). Note the interface name.</p> <p>Create a backup of the <code>/etc/network/interfaces</code> file (<code>cp /etc/network/interfaces /etc/network/interfaces.orig</code>), then edit the file with <code>nano</code>, <code>(neo)vi(m)</code>, or some other terminal editor. Find your interface name in the file (referencing the interface you noted above) and change it to a <code>static</code> connection, with your desired IP address, gateway, and DNS nameservers. Replace any <code>xxx</code> values below with your own networking values.</p> Set static IP on Debian<pre><code># This file describes the network interfaces available on your system\n# and how to activate them. For more information, see interfaces(5).\n\nsource /etc/network/interfaces.d/*\n\n# The loopback network interface\nauto lo\niface lo inet loopback\n\n# The primary network interface\nallow-hotplug ens18\niface ens18 inet static\n    address 192.168.1.xxx/24\n    gateway 192.168.1.1\n    nameservers 192.168.1.xxx,1.1.1.1,1.0.0.1\n</code></pre> <p>Restart your machine, or run one of the following (if you do not have systemd, use the 2nd method with <code>ifup</code> and <code>ifdown</code>):</p> <ul> <li>Restart systemd service: <code>sudo systemctl restart networking</code></li> <li>Use the <code>ifup</code> and <code>ifdown</code> utility:<ul> <li><code>sudo ifdown &lt;eth0, eno1, ens18, ...&gt;</code> (use your interface name, do not copy the <code>&lt;angle brackets&gt;</code>)</li> <li><code>sudo ifup &lt;eth0, eno1, ens18&gt;</code></li> </ul> </li> </ul>","tags":["linux","debian","networking"]},{"location":"linux/deb_family/networking.html#with-a-gui","title":"With a GUI","text":"<p>...</p>","tags":["linux","debian","networking"]},{"location":"programming/index.html","title":"Programming","text":"<p>Notes &amp; code snippets I want to remember/reference.</p> <p>Use the sections along the left to read through my notes and see code examples.</p>"},{"location":"programming/VSCode/index.html","title":"Visual Studio Code","text":"<p>I mainly use Visual Studio Code as my IDE. This section has tips, example configurations, guides, &amp; more related to VS Code.</p>"},{"location":"programming/VSCode/ad-hoc-codecell.html","title":"Ad-Hoc, Jupyter-like code \"cells\"","text":"<p>Note</p> <ul> <li>Your environment must have the <code>ipykernel</code> package installed.</li> <li>I have only tested this with VSCode. It should work in any IDE that supports <code>ipykernel</code> (i.e. JetBrains), from what I've read, but I have not tested it anywhere else.</li> </ul> <p>You can create ad-hoc \"cells\" in any <code>.py</code> file (i.e. <code>notebook.py</code>, <code>nb.py</code>, etc) by adding <code># %%</code> line(s) to the file.</p> Example 'notebook.py' file<pre><code># %%\n</code></pre> <p>You can also create multiple code cells by adding more <code># %%</code> lines:</p> notebook.py<pre><code># %%\nmsg = \"This code is executed in the first code cell. It sets the value of 'msg' to this string.\"\n\n# %%\n## Display the message\nmsg\n\n# %%\n150 + 150\n</code></pre> Notebook cells in VSCode","tags":["vscode","ide","python","jupyter","snippet"]},{"location":"programming/VSCode/import-export-extensions.html","title":"Import/Export VSCode Extensions","text":"<p>You can export a list of your VSCode extensions to a text file, then loop that text file to install the extensions. This is how plugins like VSCode's \"settings sync\" work, but you can do it manually.</p> <p>This is also useful for Docker containers like <code>openvscode-server</code>, where you can add a list of VSXi extensions to install from [open-vsx.org].</p>"},{"location":"programming/VSCode/import-export-extensions.html#export-vscode-extensions","title":"Export VSCode extensions","text":""},{"location":"programming/VSCode/import-export-extensions.html#linux","title":"Linux","text":"Export VSCode extensions to text file<pre><code>code --list-extensions &gt; vscode-extensions.list\n</code></pre>"},{"location":"programming/VSCode/import-export-extensions.html#windows","title":"Windows","text":"Export VSCode extensions to text file<pre><code>code --list-extensions &gt; vscode-extensions.list\n</code></pre>"},{"location":"programming/VSCode/import-export-extensions.html#import-vscode-extensions","title":"Import VSCode extensions","text":""},{"location":"programming/VSCode/import-export-extensions.html#linux_1","title":"Linux","text":"Import VSCode extensions from text file<pre><code>cat vscode-extensions.list | xargs -L 1 code --install-extension\n</code></pre>"},{"location":"programming/VSCode/import-export-extensions.html#windows_1","title":"Windows","text":"Import VSCode extensions from text file<pre><code>cat vscode-extensions.list |% { code --install-extension $_ }\n</code></pre>"},{"location":"programming/VSCode/settings-workspaces.html","title":"VSCode settings.json and .code-workspace files","text":"<p>You can configure VSCode settings per-project using either a <code>settings.json</code> or <code>*.code-workspace</code> file. You must create these files manually, or create a Workspace in VSCode and save it as a file.</p> <p>Your settings/workspace files should exist in a directory at the project's root, called <code>.vscode/</code>.</p> <p>A <code>settings.json</code> file takes precedence, meaning if you set 2 different values for a single configuration, one in <code>settings.json</code> and one in a <code>.code-workspace</code> file, the option in <code>settings.json</code> will take precedence and be applied.</p>"},{"location":"programming/VSCode/settings-workspaces.html#settingsjson-configuration","title":"settings.json configuration","text":"<p>If a file <code>.vscode/settings.json</code> exists at the project root, VSCode will load its configuration from that file.</p>"},{"location":"programming/VSCode/settings-workspaces.html#example-settingsjson-file","title":"Example settings.json file","text":".vscode/settings.json<pre><code>{\n    // ========================\n    // Workbench Configurations\n    // ========================\n    \"workbench.editor.labelFormat\": \"medium\",\n    \"workbench.editor.highlightModifiedTabs\": true,\n    \"workbench.editor.limit.enabled\": true,\n    \"workbench.editor.limit.excludeDirty\": true,\n    \"workbench.editor.limit.perEditorGroup\": true,\n    \"workbench.editor.limit.value\": 12,\n    \"workbench.editor.preferHistoryBasedLanguageDetection\": true,\n    \"workbench.editor.pinnedTabSizing\": \"shrink\",\n    \"workbench.editor.restoreViewState\": true,\n    \"workbench.editor.revealIfOpen\": true,\n    \"workbench.editor.scrollToSwitchTabs\": true,\n    \"workbench.editor.showTabs\": \"multiple\",\n    \"workbench.editor.sharedViewState\": true,\n    \"workbench.editor.tabCloseButton\": \"left\",\n    \"workbench.editor.tabSizing\": \"shrink\",\n    \"workbench.editor.wrapTabs\": true,\n    // =====================\n    // Editor Configurations\n    // =====================\n    \"editor.acceptSuggestionOnEnter\": \"smart\",\n    \"editor.autoClosingBrackets\": \"always\",\n    \"editor.autoClosingQuotes\": \"always\",\n    \"editor.autoIndent\": \"advanced\",\n    \"editor.autoSurround\": \"languageDefined\",\n    \"editor.codeLens\": true,\n    \"editor.colorDecorators\": true,\n    \"editor.comments.insertSpace\": true,\n    \"editor.cursorSmoothCaretAnimation\": \"on\",\n    \"editor.cursorStyle\": \"block\",\n    \"editor.cursorSurroundingLines\": 8,\n    \"editor.defaultFormatter\": \"esbenp.prettier-vscode\",\n    \"editor.detectIndentation\": true,\n    \"editor.find.addExtraSpaceOnTop\": true,\n    \"editor.find.autoFindInSelection\": \"multiline\",\n    \"editor.find.cursorMoveOnType\": false,\n    \"editor.find.loop\": true,\n    \"editor.find.seedSearchStringFromSelection\": \"selection\",\n    \"editor.folding\": true,\n    \"editor.foldingHighlight\": true,\n    \"editor.foldingStrategy\": \"auto\",\n    \"editor.fontFamily\": \"Consolas, 'Courier New', monospace\",\n    \"editor.fontLigatures\": true,\n    \"editor.fontSize\": 14,\n    \"editor.fontWeight\": \"normal\",\n    \"editor.formatOnPaste\": true,\n    \"editor.formatOnSaveMode\": \"modificationsIfAvailable\",\n    \"editor.formatOnSave\": true,\n    \"editor.guides.bracketPairs\": \"active\",\n    \"editor.guides.bracketPairsHorizontal\": \"active\",\n    \"editor.guides.highlightActiveBracketPair\": true,\n    \"editor.guides.highlightActiveIndentation\": \"always\",\n    \"editor.guides.indentation\": true,\n    \"editor.inlayHints.enabled\": \"onUnlessPressed\",\n    \"editor.inlineSuggest.enabled\": true,\n    \"editor.largeFileOptimizations\": true,\n    \"editor.linkedEditing\": true,\n    \"editor.links\": true,\n    \"editor.matchBrackets\": \"always\",\n    \"editor.minimap.scale\": 2,\n    \"editor.mouseWheelZoom\": true,\n    \"editor.multiCursorMergeOverlapping\": true,\n    \"editor.multiCursorModifier\": \"alt\",\n    \"editor.occurrencesHighlight\": \"singleFile\",\n    \"editor.overviewRulerBorder\": true,\n    \"editor.parameterHints.enabled\": true,\n    \"editor.parameterHints.cycle\": true,\n    \"editor.quickSuggestions\": {\n        \"other\": \"on\",\n        \"comments\": \"off\",\n        \"strings\": \"off\"\n    },\n    \"editor.rename.enablePreview\": true,\n    \"editor.roundedSelection\": true,\n    \"editor.selectionHighlight\": true,\n    \"editor.semanticHighlighting.enabled\": \"configuredByTheme\",\n    \"editor.showDeprecated\": true,\n    \"editor.showFoldingControls\": \"always\",\n    \"editor.showUnused\": true,\n    \"editor.smoothScrolling\": true,\n    \"editor.suggest.preview\": true,\n    \"editor.suggest.shareSuggestSelections\": false,\n    \"editor.suggest.showStatusBar\": true,\n    \"editor.tabSize\": 2,\n    \"editor.codeActionsOnSave\": {\n        \"source.fixAll\": \"never\",\n        \"source.fixAll.eslint\": \"never\"\n    },\n    \"diffEditor.codeLens\": true,\n    // Python Configurations\n    // =====================\n    \"python.terminal.activateEnvironment\": true,\n    \"python.analysis.extraPaths\": [\n        // If .nox_ext directory exists, add to path\n        \"./.nox_ext\"\n    ],\n    \"[python]\": {\n        \"editor.insertSpaces\": true,\n        \"editor.tabSize\": 4,\n        \"editor.formatOnSave\": true,\n        \"editor.wordBasedSuggestions\": \"off\"\n    },\n    \"workbench.editor.tabActionLocation\": \"left\"\n}\n</code></pre>"},{"location":"programming/VSCode/settings-workspaces.html#code-workspaces","title":"Code Workspaces","text":"<p>VSCode will read files in <code>.vscode/</code> (at the project's root) with a file extension of <code>.code-workspace</code> as a \"workspace configuration.\" You can define settings in a <code>.code-workspace</code> files, like language settings or animations, and when VSCode loads the workspace, it will apply the configuration found in the file.</p> <p>Note: <code>.code-workspace</code> configurations only apply when you open the file as a workspace. You can open the file in VSCode as if you were editing it and use the \"Open Workspace\" button in the bottom right of the VSCode window to open the workspace, or from the command pallette (<code>CTRL+SHIFT+P</code>) and search for: <code>File: Open Workspace from File</code>.</p>"},{"location":"programming/VSCode/settings-workspaces.html#example-code-workspace","title":"Example code workspace","text":"Example VSCode .code-workspace file<pre><code>{\n    \"folders\": [\n        // Workspace directories are JSON arrays with a `\"path\"` and `\"name\"` param.\n        // `\"name\"` is optional, but you must provice a `\"path\"`.\n        // {\n        //   \"path\": \"../\",\n        //   \"name\": \"Git Root\"\n        // }\n    ],\n    \"settings\": {\n        // ========================\n        // Workbench Configurations\n        // ========================\n        \"workbench.editor.labelFormat\": \"medium\",\n        \"workbench.editor.highlightModifiedTabs\": true,\n        \"workbench.editor.limit.enabled\": true,\n        \"workbench.editor.limit.excludeDirty\": true,\n        \"workbench.editor.limit.perEditorGroup\": true,\n        \"workbench.editor.limit.value\": 12,\n        \"workbench.editor.preferHistoryBasedLanguageDetection\": true,\n        \"workbench.editor.pinnedTabSizing\": \"shrink\",\n        \"workbench.editor.restoreViewState\": true,\n        \"workbench.editor.revealIfOpen\": true,\n        \"workbench.editor.scrollToSwitchTabs\": true,\n        \"workbench.editor.showTabs\": \"multiple\",\n        \"workbench.editor.sharedViewState\": true,\n        \"workbench.editor.tabCloseButton\": \"left\",\n        \"workbench.editor.tabSizing\": \"shrink\",\n        \"workbench.editor.wrapTabs\": true,\n        // =====================\n        // Editor Configurations\n        // =====================\n        \"editor.acceptSuggestionOnEnter\": \"smart\",\n        \"editor.autoClosingBrackets\": \"always\",\n        \"editor.autoClosingQuotes\": \"always\",\n        \"editor.autoIndent\": \"advanced\",\n        \"editor.autoSurround\": \"languageDefined\",\n        \"editor.codeLens\": true,\n        \"editor.colorDecorators\": true,\n        \"editor.comments.insertSpace\": true,\n        \"editor.cursorSmoothCaretAnimation\": \"on\",\n        \"editor.cursorStyle\": \"block\",\n        \"editor.cursorSurroundingLines\": 8,\n        \"editor.defaultFormatter\": \"esbenp.prettier-vscode\",\n        \"editor.detectIndentation\": true,\n        \"editor.find.addExtraSpaceOnTop\": true,\n        \"editor.find.autoFindInSelection\": \"multiline\",\n        \"editor.find.cursorMoveOnType\": false,\n        \"editor.find.loop\": true,\n        \"editor.find.seedSearchStringFromSelection\": \"selection\",\n        \"editor.folding\": true,\n        \"editor.foldingHighlight\": true,\n        \"editor.foldingStrategy\": \"auto\",\n        \"editor.fontFamily\": \"Consolas, 'Courier New', monospace\",\n        \"editor.fontLigatures\": true,\n        \"editor.fontSize\": 14,\n        \"editor.fontWeight\": \"normal\",\n        \"editor.formatOnPaste\": true,\n        \"editor.formatOnSaveMode\": \"modificationsIfAvailable\",\n        \"editor.formatOnSave\": true,\n        \"editor.guides.bracketPairs\": \"active\",\n        \"editor.guides.bracketPairsHorizontal\": \"active\",\n        \"editor.guides.highlightActiveBracketPair\": true,\n        \"editor.guides.highlightActiveIndentation\": \"always\",\n        \"editor.guides.indentation\": true,\n        \"editor.inlayHints.enabled\": \"onUnlessPressed\",\n        \"editor.inlineSuggest.enabled\": true,\n        \"editor.largeFileOptimizations\": true,\n        \"editor.linkedEditing\": true,\n        \"editor.links\": true,\n        \"editor.matchBrackets\": \"always\",\n        \"editor.minimap.scale\": 2,\n        \"editor.mouseWheelZoom\": true,\n        \"editor.multiCursorMergeOverlapping\": true,\n        \"editor.multiCursorModifier\": \"alt\",\n        \"editor.occurrencesHighlight\": \"singleFile\",\n        \"editor.overviewRulerBorder\": true,\n        \"editor.parameterHints.enabled\": true,\n        \"editor.parameterHints.cycle\": true,\n        \"editor.quickSuggestions\": {\n            \"other\": \"on\",\n            \"comments\": \"off\",\n            \"strings\": \"off\"\n        },\n        \"editor.rename.enablePreview\": true,\n        \"editor.roundedSelection\": true,\n        \"editor.selectionHighlight\": true,\n        \"editor.semanticHighlighting.enabled\": \"configuredByTheme\",\n        \"editor.showDeprecated\": true,\n        \"editor.showFoldingControls\": \"always\",\n        \"editor.showUnused\": true,\n        \"editor.smoothScrolling\": true,\n        \"editor.suggest.preview\": true,\n        \"editor.suggest.shareSuggestSelections\": false,\n        \"editor.suggest.showStatusBar\": true,\n        \"editor.tabSize\": 2,\n        \"editor.codeActionsOnSave\": {\n            \"source.fixAll\": \"never\",\n            \"source.fixAll.eslint\": \"never\"\n        },\n        \"diffEditor.codeLens\": true,\n        // Python Configurations\n        // =====================\n        \"python.terminal.activateEnvironment\": true,\n        \"[python]\": {\n            \"editor.insertSpaces\": true,\n            \"editor.tabSize\": 4,\n            \"editor.formatOnSave\": true,\n            \"editor.wordBasedSuggestions\": \"off\"\n        },\n        \"workbench.editor.tabActionLocation\": \"left\"\n    }\n}\n</code></pre>"},{"location":"programming/VSCode/extension-lists/index.html","title":"Extension Lists","text":"<p>I've compiled some lists of VSCode extensions I use frequently or think are worth recommending. You can copy/paste one of the lists below into a file, i.e. <code>vscode-extensions.list</code>, then install the list using one of the vscode extension import commands.</p>"},{"location":"programming/VSCode/extension-lists/index.html#my-standard-extensions","title":"My Standard Extensions","text":"<p>List of extensions I always end up installing when I do a \"fresh start.\"</p> My Standard Extensions<pre><code>1yib.rust-bundle\nadamviola.parquet-explorer\nahmadalli.vscode-nginx-conf\nalefragnani.project-manager\nalmenon.arepl\nantfu.icons-carbon\naykutsarac.jsoncrack-vscode\nazurite.azurite\nbibhasdn.django-html\nbradgashler.htmltagwrap\nchadalen.vscode-jetbrains-icon-theme\ncharliermarsh.ruff\nchristian-kohler.path-intellisense\ncodeium.codeium\ncodezombiech.gitignore\ndamildrizzy.fastapi-snippets\ndetachhead.basedpyright\ndonjayamanne.githistory\ndonjayamanne.python-environment-manager\ndonjayamanne.python-extension-pack\ndunstontc.dark-plus-syntax\ndustypomerleau.rust-syntax\necmel.vscode-html-css\neditorconfig.editorconfig\nelanandkumar.el-vsc-product-icon-theme\nesbenp.prettier-vscode\nevidence.sqltools-duckdb-driver\nfabiospampinato.vscode-diff\nformulahendry.auto-close-tag\nformulahendry.auto-complete-tag\nformulahendry.auto-rename-tag\nformulahendry.azure-storage-explorer\nformulahendry.docker-explorer\nfoxundermoon.shell-format\ngamunu.opentofu\ngrapecity.gc-excelviewer\nhashicorp.hcl\nhediet.vscode-drawio\nhelgardrichard.helium-icon-theme\nhyesun.py-paste-indent\nidleberg.icon-fonts\njanisdd.vscode-edit-csv\njomeinaster.bracket-peek\nkaih2o.python-resource-monitor\nkevinrose.vsc-python-indent\nmads-hartmann.bash-ide-vscode\nmatsbolter.vscode-cookierunner\nmelishev.feather-vscode\nmhutchie.git-graph\nmiguelsolorio.fluent-icons\nmohsen1.prettify-json\nms-azure-devops.azure-pipelines\nms-azuretools.vscode-azurefunctions\nms-azuretools.vscode-azureresourcegroups\nms-azuretools.vscode-azurestorage\nms-azuretools.vscode-cosmosdb\nms-azuretools.vscode-docker\nms-pyright.pyright\nms-python.black-formatter\nms-python.debugpy\nms-python.isort\nms-python.python\nms-python.vscode-pylance\nms-vscode-remote.remote-containers\nms-vscode-remote.remote-ssh\nms-vscode-remote.remote-ssh-edit\nms-vscode-remote.remote-wsl\nms-vscode-remote.vscode-remote-extensionpack\nms-vscode.azure-account\nms-vscode.live-server\nms-vscode.powershell\nms-vscode.remote-explorer\nms-vscode.remote-server\nmtxr.sqltools\nmtxr.sqltools-driver-mssql\nmtxr.sqltools-driver-mysql\nmtxr.sqltools-driver-pg\nmtxr.sqltools-driver-sqlite\nmutantdino.resourcemonitor\nnjqdev.vscode-python-typehint\noderwat.indent-rainbow\npflannery.vscode-versionlens\npkief.material-icon-theme\npkief.material-product-icons\nqwtel.sqlite-viewer\nrandomfractalsinc.duckdb-sql-tools\nrastikerdar.vscode-seedling-icon-theme\nredhat.ansible\nredhat.vscode-xml\nredhat.vscode-yaml\nritwickdey.liveserver\nrogalmic.bash-debug\nrust-lang.rust-analyzer\nsamuelcolvin.jinjahtml\nshamanu4.django-intellisense\nstreetsidesoftware.code-spell-checker\ntal7aouy.icons\ntamasfe.even-better-toml\ntecher.open-in-browser\nteticio.python-envy\ntomaustin.azure-devops-yaml-pipeline-validator\ntomiy.gitea-auth-provider\nvincaslt.highlight-matching-tag\nvisualstudioexptteam.intellicode-api-usage-examples\nvisualstudioexptteam.vscodeintellicode\nvisualstudioexptteam.vscodeintellicode-completions\nvscode-icons-team.vscode-icons\nwattenberger.footsteps\nwholroyd.jinja\nwilliam-voyek.vscode-nginx\nyzhang.markdown-all-in-one\nshd101wyy.markdown-preview-enhanced\nzguolee.tabler-icons\nziyasal.vscode-open-in-github\n</code></pre>"},{"location":"programming/VSCode/extension-lists/index.html#git-extensions","title":"Git Extensions","text":"Git Extensions<pre><code>mhutchie.git-graph\ncodezombiech.gitignore\ndonjayamanne.githistory\nmhutchie.git-graph\nwaderyan.gitblame\n</code></pre>"},{"location":"programming/VSCode/extension-lists/index.html#python-extensions","title":"Python Extensions","text":"<p>Extensions for Python development. Some extensions, like <code>redhat.vscode-yaml</code> and <code>esbenp.prettier-vscode</code>, are not directly/specifically related to Python, but I use frequently for Python development.</p> Python extensions<pre><code>bibhasdn.django-html\ndamildrizzy.fastapi-snippets\nalmenon.arepl\nnjpwerner.autodocstring\nsamuelcolvin.jinjahtml\nms-python.black-formatter\ntamasfe.even-better-toml\njanisdd.vscode-edit-csv\nms-python.isort\nms-python.isort\nwholroyd.jinja\nms-toolsai.jupyter\nAdamViola.parquet-explorer\nchristian-kohler.path-intellisense\nmohsen1.prettify-json\nalefragnani.project-manager\nms-python.vscode-pylance\nms-python.python\nms-python.debugpy\ndonjayamanne.python-environment-manager\nteticio.python-envy\ndonjayamanne.python-extension-pack\nKevinRose.vsc-python-indent\nhyesun.py-paste-indent\nnjqdev.vscode-python-typehint\ncharliermarsh.ruff\nredhat.vscode-yaml\nshamanu4.django-intellisense\nVisualStudioExptTeam.vscodeintellicode\nVisualStudioExptTeam.intellicode-api-usage-examples\nVisualStudioExptTeam.vscodeintellicode-completions\nesbenp.prettier-vscode\ndetachhead.basedpyright\n</code></pre>"},{"location":"programming/VSCode/extension-lists/index.html#database-extensions","title":"Database Extensions","text":"Database Extensions<pre><code>mtxr.sqltools\nmtxr.sqltools-driver-mysql\nmtxr.sqltools-driver-pg\nmtxr.sqltools-driver-sqlite\nEvidence.sqltools-duckdb-driver\nRandomFractalsInc.duckdb-sql-tools\nqwtel.sqlite-viewer\n</code></pre>"},{"location":"programming/VSCode/extension-lists/index.html#tool-utility-extensions","title":"Tool &amp; Utility Extensions","text":"Tool &amp; Utility Extensions<pre><code>VisualStudioExptTeam.vscodeintellicode\nVisualStudioExptTeam.intellicode-api-usage-examples\nVisualStudioExptTeam.vscodeintellicode-completions\nmutantdino.resourcemonitor\nms-vscode-remote.remote-containers\nms-azuretools.vscode-docker\noderwat.indent-rainbow\nkaih2o.python-resource-monitor\njomeinaster.bracket-peek\nchristian-kohler.path-intellisense\nalefragnani.project-manager\ndonjayamanne.python-environment-manager\npflannery.vscode-versionlens\nesbenp.prettier-vscode\ngamunu.opentofu\nEditorConfig.EditorConfig\njomeinaster.bracket-peek\nfabiospampinato.vscode-diff\nms-vscode-remote.remote-wsl\nms-vscode-remote.vscode-remote-extensionpack\nalefragnani.project-manager\nziyasal.vscode-open-in-github\ntecher.open-in-browser\nyzhang.markdown-all-in-one\nshd101wyy.markdown-preview-enhanced\noderwat.indent-rainbow\nvincaslt.highlight-matching-tag\nWattenberger.footsteps\nms-vscode-remote.remote-containers\nstreetsidesoftware.code-spell-checker\nrogalmic.bash-debug\nmads-hartmann.bash-ide-vscode\n</code></pre>"},{"location":"programming/VSCode/extension-lists/index.html#azure-extensions","title":"Azure Extensions","text":"Azure Extensions<pre><code>ms-vscode.azure-account\nms-azuretools.vscode-cosmosdb\nms-azuretools.vscode-azurefunctions\nms-azure-devops.azure-pipelines\nTomAustin.azure-devops-yaml-pipeline-validator\nms-azuretools.vscode-azureresourcegroups\nms-azuretools.vscode-azurestorage\nformulahendry.azure-storage-explorer\nAzurite.azurite\n</code></pre>"},{"location":"programming/VSCode/extension-lists/index.html#themes-icons","title":"Themes &amp; Icons","text":"Themes &amp; icons<pre><code>miguelsolorio.fluent-icons\nantfu.icons-carbon\ntal7aouy.icons\nvscode.vscode-theme-seti\noguhpereira.spotify-color-theme\nhelgardrichard.helium-icon-theme\nBeardedBear.beardedicons\n</code></pre>"},{"location":"programming/docker/index.html","title":"Docker","text":"<p>Docker is an engine for running containerized apps. It's a powerful tool for development, and for simplifying deployment of production applications.</p> <p>Paired with an orchestrator like Docker Compose, Kubernetes, or Nomad, Docker containers can create a mostly-reproducible environment to ensure an application runs the same with the same inputs across hosts.</p>"},{"location":"programming/docker/docker_cli.html","title":"The Docker CLI","text":"<p>...</p>","tags":["docker"]},{"location":"programming/docker/docker_cli.html#docker-command-cheat-sheet","title":"Docker Command Cheat Sheet","text":"Command Description <code>docker ps [-a]</code> Show Docker containers. Use <code>-a</code>/<code>--all</code> to show all containers, instead of only running ones. <code>docker system [df,events,info,prune]</code> Manage Docker. <code>df</code> shows Docker disk usage, <code>events</code> shows real-time events from the server, <code>info</code> displays system-wide info, and <code>prune</code> handles Docker cleanup tasks like removing old images &amp; volumes that aren't in use. <code>docker cp /path/on/host &lt;container_name&gt;:/path/in/container</code> Copy a file from the host machine into a container. <code>docker cp &lt;container_name&gt;:/path/in/container /path/on/host</code> Copy a file from within the container to the host.","tags":["docker"]},{"location":"programming/docker/docker_compose.html","title":"Docker Compose","text":"<p>Docker Compose is a container orchestrator for Docker. A container can be built &amp; executed individually, but an orchestrator can handle all of the build &amp; deployment implementation details for the container. It can handle the container's networking, volume mounts (where data is stored), building the container, running it, environment variables for the container, links to other containers, and more.</p> <p>Container orchestration can get very complex very quickly, especially with lower level orchestrators like Kubernetes. Docker Compose is a relatively easy to learn orchestration tool that is installable as a plugin for the Docker engine, allowing you to run commands with <code>docker compose &lt;command&gt;</code>.</p> <p>Docker Compose can also orchestrate containers from Docker hub, allowing you to write container definitions and let Compose download, build, &amp; run the necessary containers.</p>","tags":["docker"]},{"location":"programming/docker/docker_compose.html#docker-compose-command-cheat-sheet","title":"Docker Compose Command Cheat-Sheet","text":"<p>For the most part, a Docker Compose command will look like this, with the exception of <code>exec</code> commands that put the command after the container's name:</p> Docker Compose syntax<pre><code>docker compose [-f /path/to/compose.yml] [command] [options] [container_name]\n</code></pre> Command Description <code>docker compose -f path/to/compose.yml</code> When your <code>compose.yml</code> file is in a different directory, tell Docker Compose where to find it. Paths are relative. <code>docker compose build</code> Build the containers defined in a <code>compose.yml</code> file. <code>docker compose build --no-cache</code> Build the containers, skipping the cache if there is one. This can be helpful if Docker does not detect changes in a stage, but you want to force a rebuild of the containers. <code>docker compose up</code> Bring up the containers defined in your <code>compose.yml</code> Use <code>-d</code> to run them in \"detached\" mode, so you can return to your shell and keep the stack running. <code>docker compose up --build</code> Re-build your containers, then bring them back up. Cannot be used with force-recreate`. <code>docker compose up --force-recreate</code> Bring up a stack, forcing container restarts. Cannot be used with <code>--build</code>. <code>docker compose logs -f container_name</code> View the logs for a running container. The <code>-f</code> parameter is for <code>--follow</code>, which will scroll the logs real-time. <code>docker compose down</code> Bring a Docker Compose stack down. <code>docker compose exec [-it] &lt;container_name&gt; &lt;command&gt;</code> Execute a command inside a given Docker container managed by Docker Compose. <code>-it</code> puts you into an interactive terminal (you can supply input), and <code>&lt;command&gt;</code> can be something like <code>/bin/bash</code> (top open a Bash prompt in the container), or executing commands against the application(s) within the container.","tags":["docker"]},{"location":"programming/docker/docker_compose.html#writing-a-docker-compose-composeyml-file","title":"Writing a Docker Compose compose.yml file","text":"<p>A <code>compose.yml</code> file is a definition for Docker Compose that tells it what resources to provision (volumes, networks, environment/build variables, etc), how containers should interact with each other, which \"stage\" of a build to run, and more.</p>","tags":["docker"]},{"location":"programming/docker/docker_compose.html#example-composeyml-file","title":"Example compose.yml file","text":"<p>Compose files require a <code>services:</code> section, where your container services are defined. Other options sections, like <code>volumes:</code> and <code>networks:</code>, define other resources Docker Compose should provision when it runs the stack.</p> <p>Warning</p> <p>The example <code>compose.yml</code> file below will not actually run. This file shows the structure/schema for a <code>compose.yml</code> file, but you would need to add your own values where you see a <code>...</code>.</p> <p>This is also not a complete example; Docker Compose has many more options you can configure as root-level keys (like <code>volumes</code> or <code>networks</code>), as well as service-level variables and keys.</p> <p>See the documentation for Docker Compose for more in-depth (and probably more up to date) instructions.</p> compose.yml skeleton<pre><code>---\n## Define named volumes, managed by Docker at /var/lib/docker/volumes\nvolumes:\n  volume_name: {}\n\nnetworks:\n  network_name: {}\n\nservices:\n  service_1:\n    image: ...:...\n    container_name: ... # optional\n    restart: ... # optional\n    ports: # optional\n      - 00:00/tcp\n      - 0000:0000/udp\n    volumes: # optional\n      ## Mount /path/in/container to a persistent named volume\n      - volume_name:/path/in/container\n      ## Mount /path2/in/container from the container to a directory named example_host_mount.\n      #  Paths are relative\n      - ./exmaple_host_mount:/path2/in/container\n\n      ## Alternative, more explicit version\n      - type: volume\n        source: ./path/on/host\n        target: /path3/in/container\n        readonly: true # optional\n    ## Tell Docker Compose to load environment variables for this container\n    #  from a file\n    env_file:\n      - ./path/to/dev.env\n    environment: # optional\n      VAR_NAME: value\n      VAR2_NAME: 0\n      VAR3_NAME: true\n      VAR4_NAME: ${VAR_NAME_USER_DEFINED:-\"default value\"}\n    networks:\n      - network_name\n    ## A healthcheck defines the conditions the container must meet to\n    #  be considered 'healthy' by Docker\n    healthcheck:\n      test: echo \"Write your healthcheck command here, like a cURL test, or checking that a service is running\"\n      interval: 1m30s # How often to run healtcheck test\n      timeout: 10s # How long test command is allowed to run before exiting unsuccessfully and reporting 'unhealthy'\n      retries: 3 # Number of failed attempts before a container is considered 'unhealthy'\n      start_period: 20s # Wait 20s to start the healthcheck, giving the container time to build\n\n  service_2:\n    image: ...:...\n    container_name: ... # optional\n    restart: ... # optional\n    ports: # optional\n      - 00:00\n    depends_on:\n      - service_1 # this container will not start/run until service_1 finishes is 'healthy'\n    networks:\n      - network_name\n</code></pre>","tags":["docker"]},{"location":"programming/docker/docker_compose.html#building-a-local-dockerfile","title":"Building a local Dockerfile","text":"<p>...</p>","tags":["docker"]},{"location":"programming/docker/docker_compose.html#healthchecks","title":"Healthchecks","text":"<p>...</p>","tags":["docker"]},{"location":"programming/docker/docker_compose.html#environment-variables","title":"Environment Variables","text":"<p>There are a number of ways to set environment variables in a Docker container using a <code>compose.yml</code> file. Environment variables can be loaded from the host environment, a <code>.env</code> file, using a Docker secrets file, and by prepending <code>docker compose</code> commands with the env variable.</p>","tags":["docker"]},{"location":"programming/docker/docker_compose.html#environment-variable-loading-precedence","title":"Environment variable loading precedence","text":"<p>Tip</p> <p>Docker documentation: Environment variables precedence in Docker Compose</p> <p>Docker Compose loads environment variables in the following order:</p> <ol> <li>Variables prepended to a command like <code>VAR_NAME=value docker compose ...</code></li> <li>Variables defined on the host (with <code>export VAR_NAME=value</code> on Linux and <code>$env:VAR_NAME = \"value\"</code> on Powershell)</li> <li>Variables set using <code>docker compose run -e</code></li> <li>A <code>.env</code> file</li> <li>The <code>environment:</code> attribute in a <code>compose.yml</code> service definition</li> <li>The <code>env_file:</code> attribute in a <code>compose.yml</code> service definition</li> <li>Variables set in a Dockerfile/container image with the <code>ENV</code> directive.</li> <li>Any default values set with <code>${VAR_NAME:-default}</code> in the <code>compose.yml</code> file</li> </ol>","tags":["docker"]},{"location":"programming/docker/docker_compose.html#docker-compose-environment-variable-loading-precedence","title":"Docker Compose environment variable loading precedence","text":"<p>Table representation of a Docker Compose command's precedence when loading environment variables. A lower number means higher precedence (loaded earlier).</p> Precedence Definition Notes 0 Prepended to a <code>docker compose</code> command <code>VAR_NAME=val docker compose ...</code> 1 Defined in the host environment [Linux: <code>export VAR_NAME=val</code>] [Windows: <code>$env:VAR_NAME=\"val\"</code>] 2 Added to <code>docker compose run</code> command with <code>docker compose run -e VAR_NAME=val</code> 3 A <code>.env</code> file Environment variables can be set/overridden with a <code>.env</code> file 4 <code>environment:</code> A service definition can define environment variables in an <code>environment:</code> attribute 5 <code>env_file:</code> A service definition can define an <code>env_file</code> where environment variables should be loaded from. 6 A Dockerfile's <code>ENV</code> directive A Dockerfile can set environment variables with <code>ENV</code> 7 Default values When setting environment variables in a <code>compose.yml</code> service definition, you can set a default value like <code>VAR_NAME: ${VAR_NAME:-default}</code>. If no value is found for <code>VAR_NAME</code> in one of the methods above, it will be set to <code>default</code>","tags":["docker"]},{"location":"programming/docker/docker_compose.html#load-by-prepending-docker-compose-command","title":"Load by prepending docker compose command","text":"<p>You can add your environment variable definitions during <code>docker compose</code> execution. Say you have wnat to assign variable <code>CONTAINER_PORT=8085</code>; when running your <code>docker compose</code> command, add the variable before the <code>docker compose</code> command to pass it to the orchestrator with the syntax <code>VAR_NAME=value docker compose [options]</code>:</p> Assign env variables during docker compose execution<pre><code>CONTAINER_PORT=8085 docker compose up -d\n</code></pre>","tags":["docker"]},{"location":"programming/docker/docker_compose.html#load-from-host-environment","title":"Load from host environment","text":"<p>When running <code>docker compose</code> commands, if you've set an environment variable on the host that matches a variable Docker Compose is looking for, it will load that value from the container.</p> <p>If one of your services has an environment variable <code>SERVICE_USERNAME</code>, you can set a variable in your environment like:</p> Set an environment variable<pre><code>## Bash\nexport SERVICE_USERNAME=user1\n\n## Windows\n$env:SERVICE_USERNAME = \"user1\"\n</code></pre> <p>When you run a Docker Compose stack (<code>docker compose up -d</code>) where one of the containers uses the <code>SERVICE_USERNAME</code> variable, it will use the environment's <code>user1</code> value.</p>","tags":["docker"]},{"location":"programming/docker/docker_compose.html#load-from-env-file","title":"Load from .env file","text":"<p>Create a file named <code>.env</code> (in the same path as your <code>compose.yml</code> file). Any values you set in this file will override defaults set lower in the evaluation precedence.</p> Example .env file<pre><code>TZ=Europe/Berlin\nROOT_PASSWORD=Super-Secure-Root-Password\n</code></pre>","tags":["docker"]},{"location":"programming/docker/docker_compose.html#load-from-environment-attribute","title":"Load from environment: attribute","text":"<p>Service defined in a <code>compose.yml</code> can add an <code>environment:</code> attribute, where you can define variables the container expects and the values you want to use. For example, many Docker images support the <code>TZ</code> environment variable, allowing you to set the timezone within the container. The example below shows how to define an <code>environment:</code> attribute, how to define a variable the container expects, and how to set a value that loads from the host environment, a <code>.env</code> file, or sets a default value of <code>Etc/UTC</code>:</p> Compose service 'environment:' attribute<pre><code>---\nservices:\n  service_1:\n    image: ...\n    container_name: ...\n    restart: unless-stopped\n    environment: # Pass environment variables to the container\n      TZ: ${TZ:-Etc/UTC} # Load value from TZ= (in the host env or a .env file), use Etc/UTC by default\n</code></pre>","tags":["docker"]},{"location":"programming/docker/docker_compose.html#load-from-a-file-defined-in-env_file","title":"Load from a file defined in env_file","text":"<p>You can set an <code>env_file:</code> attribute in a <code>compose.yml</code> service definition to pass a <code>.env</code> file the container should load variables from.</p> Compose service 'env_file:' attribute<pre><code>---\nservices:\n  service_1:\n    image: ...\n    container_name: ...\n    restart: unless-stopped\n    env_file:\n      - ./path/to/dev.env # Set this container's environment variables from a file\n    environment:\n      TZ: ${TZ:-Etc/UTC}\n      ## If ROOT_PASSWORD is not set in the env_file or somewhere else in the environment\n      #  this value will be blank/empty.\n      ROOT_PASSWORD: ${ROOT_PASSWORD}\n</code></pre> <p>An example <code>./path/to/dev.env</code> might be:</p> Example dev.env file<pre><code>## Set the container's TZ value to Europe/London,\n#  overriding the default value\nTZ=Europe/London\n## Set a root password so the container has a value\nROOT_PASSWORD=Super-Secure-Root-Password\n</code></pre>","tags":["docker"]},{"location":"programming/docker/docker_compose.html#set-in-a-dockerfiles-env-directive","title":"Set in a Dockerfile's ENV directive","text":"<p>When writing a Dockerfile, you can set environment variables at build time with the <code>ENV</code> directive. This is called \"hardcoding\" environment variables. Variables defined this way are evaluated last, if a value is defined in any of the previous places, it will be overridden in the Dockerfile.</p> Example Dockerfile with an ENV value<pre><code>FROM image:latest\n\n## Set the timezone in the Dockerfile.\n#  If no environment value is set in the prior evaluations,\n#  this value will be used\nENV TZ=\"Europe/Berlin\"\n</code></pre>","tags":["docker"]},{"location":"programming/docker/docker_compose.html#set-a-default-value-in-a-composeyml-file","title":"Set a default value in a compose.yml file","text":"<p>When adding environment variables to a <code>compose.yml</code> service definition, you can set a default value using this syntax: <code>${VAR_NAME:-default_value}</code> in the variable's value.</p> Set a default value for an environment variable<pre><code>---\nservices:\n  service_1:\n    image: ...\n    container_name: ...\n    restart: unless-stopped\n    environment:\n      TZ: ${TZ:-Etc/UTC}\n</code></pre>","tags":["docker"]},{"location":"programming/docker/tips_tricks.html","title":"Tips &amp; Tricks","text":"<p>This page has tips &amp; tricks for things I've run into with Docker.</p>","tags":["docker","reference"]},{"location":"programming/docker/tips_tricks.html#allow-non-root-user-to-use-docker","title":"Allow non-root user to use Docker","text":"<p>During installation, the user is given an option to add the current user's account to the <code>docker</code> group to allow them to run <code>docker</code>/<code>docker compose</code> commands without using <code>sudo</code>.</p> <p>If you get a permission error when trying to run <code>docker</code> commands, add your user account to the <code>docker</code> group:</p> Add user to Docker group<pre><code>sudo usermod -aG docker $USER\n</code></pre>","tags":["docker","reference"]},{"location":"programming/docker/tips_tricks.html#helper-containers","title":"Helper Containers","text":"<p>\"Helper\" containers use a small base image like <code>alpine</code> or <code>busybox</code> to perform actions like backing up a database or moving files to/from the host/a container. Using this method, you can move files into a container without bringing down another container using it, or copy files out of the container.</p>","tags":["docker","reference"]},{"location":"programming/docker/tips_tricks.html#copy-data-tofrom-the-host-and-a-volume","title":"Copy data to/from the host and a volume","text":"<p>Use the <code>busybox</code> container to facilitate copying files into a volume from the host, or out of a volume to the host.</p> Copy volume data to host<pre><code>docker run -v &lt;volume_name&gt;:/path/in/container --name helper busybox true  # Create the helper container\ndocker cp helper:/path/in/container ./path/on/host  # Copy data from path in container to the host\ndocker rm helper  # remove the container after operation\n</code></pre> Copy host data into a volume<pre><code>docker run -v &lt;volume_name&gt;:/path/in/container --name helper busybox true  # Create the helper container\ndocker cp ./path/on/host helper:/path/in/container  # Copy data from host to path in the helper container\ndocker rm helper  # remove the container after operation\n</code></pre>","tags":["docker","reference"]},{"location":"programming/docker/writing_dockerfiles.html","title":"The Dockerfile","text":"<p>A <code>Dockerfile</code> is how you define your Docker container. You can write a <code>Dockerfile</code> that builds an application (Python, Node, .NET, etc) in the container's environment, where you can provide build arguments &amp; environment variables, and run the resulting program within the container. This eliminates the need to install &amp; maintain build &amp; runtime tools on your host.</p> <p>The Docker hub is a repository of <code>Dockerfile</code>s others developers have created. You can download their containers and execute them on your own machine with <code>docker run</code>, or with a Docker Compose <code>compose.yml</code> file. This documentation will go over both, as well as building &amp; running your own containers.</p>","tags":["docker"]},{"location":"programming/docker/writing_dockerfiles.html#writing-a-dockerfile","title":"Writing a Dockerfile","text":"","tags":["docker"]},{"location":"programming/docker/writing_dockerfiles.html#building-a-dockerfile","title":"Building a Dockerfile","text":"","tags":["docker"]},{"location":"programming/docker/writing_dockerfiles.html#with-the-docker-cli","title":"With the docker CLI","text":"","tags":["docker"]},{"location":"programming/docker/writing_dockerfiles.html#with-docker-compose","title":"With Docker Compose","text":"","tags":["docker"]},{"location":"programming/docker/writing_dockerfiles.html#running-a-dockerfile","title":"Running a Dockerfile","text":"","tags":["docker"]},{"location":"programming/docker/writing_dockerfiles.html#with-the-docker-cli_1","title":"With the docker CLI","text":"","tags":["docker"]},{"location":"programming/docker/writing_dockerfiles.html#with-docker-compose_1","title":"With Docker Compose","text":"","tags":["docker"]},{"location":"programming/git/index.html","title":"git","text":"<p>Warning</p> <p>In progress...</p> <p>Todo</p> <ul> <li> Document creating a git repo</li> <li> Document branching<ul> <li> Strategies</li> <li> Commands</li> </ul> </li> <li> Document committing</li> <li> Document merging<ul> <li> Document merge conflicts</li> <li> Document rebasing</li> </ul> </li> <li> Document tagging</li> </ul>","tags":["git"]},{"location":"programming/git/git_scripts.html","title":"Git Scripts","text":"<p>Snippets and scripts for <code>git</code>, for Bash and PowerShell.</p>"},{"location":"programming/git/git_scripts.html#git-prune","title":"Git prune","text":"<p>When synching a Git repository to a remote like GitHub or GitLab, if you delete a branch on the remote, that branch is not deleted locally. If you create and delete branches frequently and don't regularly run <code>git branch -D &lt;branch-name&gt;</code>, you will end up with a very large local version of the repository, and a messy tree that's difficult to work with.</p> <p>The scripts below list the branches on your remote, compare them to the local branches, and remove any that exist locally but not on the remote.</p> <p>Warning</p> <p>Because these scripts work by comparison, if you have a local branch that has not been pushed to the remote yet, this script would delete that branch locally.</p>"},{"location":"programming/git/git_scripts.html#bash-version","title":"Bash version","text":"prune_local_branches.sh<pre><code>#!/bin/bash\n\ngit fetch -p &amp;&amp; for branch in $(git for-each-ref --format '%(refname) %(upstream:track)' refs/heads | awk '$2 == \"[gone]\" {sub(\"refs/heads/\", \"\", $1); print $1}'); do git branch -D $branch; done\n\nexit $?\n</code></pre>"},{"location":"programming/git/git_scripts.html#powershell-version","title":"PowerShell version","text":"Run-GitBranchPrune.ps1<pre><code>&lt;#\n    .SYNOPSIS\n    Clean up local branches that have been deleted on the remote\n\n    .DESCRIPTION\n        This script checks out the main branch, fetches from the remote, then\n        deletes (prunes) any local branches that still exist after being deleted\n        on the remote.\n\n        WARNING: This is a destructive script. Make sure you don't need the local\n        copy of your branch before pruning.\n\n    .PARAMETER MainBranch\n        The name of the main branch.\n\n    .EXAMPLE\n        .\\scripts\\Prune-GitBranches.ps1\n#&gt;\nparam(\n    [String]$MainBranch = \"main\"\n)\n\nWrite-Host \"Pruning local branches that have been deleted on the remote.\" -ForegroundColor Green\n\ntry {\n    git checkout $($MainBranch); `\n        git remote update origin --prune; `\n        git branch -vv `\n    | Select-String -Pattern \": gone]\" `\n    | ForEach-Object {\n        $_.toString().Trim().Split(\" \")[0]\n    } `\n    | ForEach-Object {\n        git branch -D $_ \n    }\n\n    Write-Host \"Local branches pruned.\" -ForegroundColor Green\n\n    exit 0\n}\ncatch {\n    Write-Warning \"Error pruning local branches. Details: $($_.Exception.Message)\"\n    exit 1\n}\n</code></pre>"},{"location":"programming/git/rewrite_git_history.html","title":"Rewrite Git History","text":"<p>If you have multiple <code>git</code> profiles, you will eventually mistakenly push a commit from the wrong author. For example, if you have a work and personal git account, and you write a quick patch for a personal project and commit the code from your work account, your \"collaborators\" on Github will show your work account.</p> <p>If you wish to fix this, you can use the steps below to rewrite Git history, replacing any reference to your work profile with your personal one.</p>","tags":["git"]},{"location":"programming/git/rewrite_git_history.html#requirements","title":"Requirements","text":"<ul> <li>Python (if you're on Linux or Mac, you do not need to install this).</li> <li>The <code>git-filter-repo</code> package<ul> <li>Install with <code>pip install git-filter-repo</code></li> </ul> </li> </ul>","tags":["git"]},{"location":"programming/git/rewrite_git_history.html#steps","title":"Steps","text":"<p>Clone the repository in a new directory using the <code>--bare</code> flag, i.e.: <code>git clone --bare git@github.com:user/repo.git</code>. This will create a local copy of your repository's HEAD refs; you will not see your code, but instead will see directories like <code>branches/</code>, <code>tags/</code>, etc. This is essentially the metadata for your repository, as well as the git history.</p> <p>Run a command like the following, replacing the <code>Old Name</code>, <code>old.email@example.com</code>, <code>New Name</code>, and <code>new.email@example.com</code> with your old/new author:</p> Replace git author in history<pre><code>git filter-branch --env-filter '\nif [ \"$GIT_COMMITTER_NAME\" = \"Old Name\" ] &amp;&amp; [ \"$GIT_COMMITTER_EMAIL\" = \"old.email@example.com\" ]; then\n    GIT_COMMITTER_NAME=\"New Name\"\n    GIT_COMMITTER_EMAIL=\"new.email@example.com\"\n    GIT_AUTHOR_NAME=\"New Name\"\n    GIT_AUTHOR_EMAIL=\"new.email@example.com\"\nfi\n' --tag-name-filter cat -- --branches --tags\n</code></pre> <p>Clean your refs and ensure proper pruning:</p> Clean git refs<pre><code>git reflog expire --expire=now --all\ngit gc --prune=now\n</code></pre> <p>Finally, force-push your changes:</p> Force push git changes<pre><code>git push --force --all\ngit push --force --tags\n</code></pre> <p>Re-clone the repository, this time without <code>--bare</code>, and use <code>git log --author=\"Old Name\"</code> to ensure all refs have been removed.</p>","tags":["git"]},{"location":"programming/git/sparse_checkout.html","title":"Git sparse checkouts","text":"<p>When working with large repositories, it can be difficult to or destructive to work on multiple branches. As you checkout code to modify things in one part of the repository, you can/will affect other areas of the repository.</p> <p>Instead, you can do a git sparse checkout. Using this method, you can clone your git repository, but only a certain path or set of paths.</p> <p>Tip</p> <p>Steps to do a sparse git clone:</p> <ul> <li><code>git clone git@github.com:user/repo-name.git &lt;optional clone path on filesystem&gt;</code></li> <li><code>cd &lt;cloned-repository-path&gt; &amp;&amp; git sparse-checkout init --cone</code></li> <li><code>git sparse-checkout set path/to/code optional-other/path/to/code</code></li> <li><code>git checkout &lt;branch-name&gt;</code></li> </ul> <p>read more in the sparse checkout steps section</p>","tags":["git"]},{"location":"programming/git/sparse_checkout.html#git-sparse-checkout-steps","title":"Git sparse checkout steps","text":"<ul> <li>Clone your repository with <code>--no-checkout</code>:<ul> <li><code>git clone git@github.com:user/repo-name.git &lt;optional clone path on filesystem&gt;</code><ul> <li>If you do not provide a clone path on the filesystem, the repo will be clones to <code>repo-name/</code> (or whatever your repository's name is).</li> </ul> </li> <li><code>cd</code> into your newly cloned repository.</li> </ul> </li> <li>Do a <code>sparse-checkout</code> in the newly cloned repository:<ul> <li><code>git sparse-checkout init --cone</code></li> </ul> </li> <li>Using <code>sparse-checkout set</code>, tell the repository which path(s) from the parent repository you want to clone in this sparse version of the repository:<ul> <li><code>git sparse-checkout set path/to/checkout &lt;optional other paths&gt;</code><ul> <li>You can checkout a single path, or multiple code paths, by simply adding more paths from the remote repository after the <code>git sparse-checkout set</code> command.</li> </ul> </li> </ul> </li> <li>Finally, checkout a code branch, i.e. <code>main</code> (you can use any other branch for <code>&lt;branch-name&gt;</code>):<ul> <li><code>git checkout &lt;branch-name&gt;</code></li> </ul> </li> </ul> <p>You now have a sparse clone! Only the path(s) you included with <code>git sparse-checkout set ...</code> will be present, and you can interact with this repository the same way you would a full clone. You can add new branches, commit code, do <code>git fetch</code>/<code>git pull</code>/<code>git push</code>, etc.</p>","tags":["git"]},{"location":"programming/git/sparse_checkout.html#example-scenario","title":"Example scenario","text":"<p>As an example, say you are working in a monorepo that contains all of your Docker container templates. You are running containers from various places in this repository, and every time you switch branches to a branch that does not have the code for one of your running services, you cause data corruption.</p> <p>You want to modify only a single container template, which lives at the following path in the repository: <code>templates/category/template1</code>. If you checkout a new branch, containers <code>templates/category/template2</code> and <code>templates/category/template3</code> will be affected.</p> <p>Instead of switching branches for the whole repository, you can do a \"sparse checkout\" for just the code in <code>templates/category/template1</code> in a new path on the filesystem, isolating this container from the rest of your repository.</p>","tags":["git"]},{"location":"programming/jupyter/index.html","title":"Jupyter","text":"<p>Warning</p> <p>In progress...</p> <p>Todo</p> <ul> <li> Document creating a kernel</li> <li> With <code>virtualenv</code></li> <li> With <code>conda</code></li> <li> With <code>pdm</code></li> <li> Document <code>jupyterlab</code></li> <li> Running with custom settings</li> <li> Installing plugins</li> <li> (Optional) in Docker</li> <li> Document importing custom Python modules</li> </ul>","tags":["python","jupyter"]},{"location":"programming/jupyter/index.html#useful-magic-cells","title":"Useful magic cells","text":"<p>Todo</p> <ul> <li> Set environment variables</li> <li> Install packages with pip</li> </ul>","tags":["python","jupyter"]},{"location":"programming/jupyter/index.html#reload-files-without-restarting-kernel","title":"Reload files without restarting kernel","text":"<p>When importing Python code into a Jupyter notebook, any changes made in the Python module(s) do not become active in the Jupyter kernel until it's restarted. This is undesirable when working with data that takes a long time up front to prepare.</p> <p>Add this code to a cell (I usually put it in the very first cell) to automatically reload Python modules on changes, without having to reload the whole notebook kernel.</p> Automatically reload file on changes<pre><code>## Set notebook to auto reload updated modules\n%load_ext autoreload\n%autoreload 2\n</code></pre>","tags":["python","jupyter"]},{"location":"programming/jupyter/index.html#automations","title":"Automations","text":"","tags":["python","jupyter"]},{"location":"programming/jupyter/index.html#automatically-strip-notebook-cell-output-when-committing-to-git","title":"Automatically strip notebook cell output when committing to git","text":"<p>Todo</p> <ul> <li> Describe VSCode error that happens after <code>pre-commit</code> runs and what to do to fix it</li> <li> Describe disabling <code>pre-commit</code></li> </ul> <p>When running Jupyter notebooks, it's usually good practice to clear the notebook's output when committing to git. This can be for privacy/security (if you're debugging PII or environment variables in the notebook), or just a tidy repository.</p> <p>Using <code>pre-commit</code> (check my section on <code>pre-commit</code>) and the <code>nbstripout action</code>, we can automate stripping the notebook each time a git commit is created.</p> <p>Instructions</p> <ul> <li>Install <code>pre-commit</code></li> </ul> <p>Note</p> <p>Warning</p> <p>If your preferred package manager is not listed below, check the documentation for that package manager for instructions on installing packages.</p> <ul> <li>With <code>pip</code>:<ul> <li><code>pip install pre-commit</code></li> </ul> </li> <li>With <code>pipx</code>:<ul> <li><code>pipx install pre-commit</code></li> </ul> </li> <li>With <code>pdm</code>:<ul> <li><code>pdm add pre-commit</code></li> </ul> </li> </ul> <p>TODO:</p> <ul> <li> <code>poetry</code></li> <li> <code>conda</code>/<code>miniconda</code>/<code>microconda</code></li> </ul> <ul> <li>Create a file in the root of your repository <code>.pre-commit-config.yml</code> with these contents:</li> </ul> .pre-commit-config.yml<pre><code>repos:\n    repo: https://github.com/kynan/nbstripout\n    rev: 0.6.1\n    hooks:\n        - id: nbstripout\n</code></pre> <ul> <li>Install the <code>pre-commit</code> hook with <code>$ pre-commit install</code></li> </ul> <p>Note</p> <p>If you installed <code>pre-commit</code> in a <code>virtualenv</code>, or with a tool like <code>pdm</code>, make sure the <code>.venv</code> is activated, or you run with <code>$ pdm run pre-commit ...</code></p> <p>Now, each time you make a <code>git commit</code>, after writing your commit message, <code>pre-commit</code> will execute <code>nbstripout</code> to strip your notebooks of their output.</p>","tags":["python","jupyter"]},{"location":"programming/mkdocs/index.html","title":"mkdocs","text":"<p>Warning</p> <p>In progress...</p> <p>Todo</p> <ul> <li> Briefly describe <code>mkdocs</code></li> <li> Add example <code>mkdocs.yml</code></li> <li> Add snippets for common plugins I use<ul> <li> Configurations<ul> <li> Top level keys<ul> <li> <code>site_name</code></li> <li> <code>site_description</code></li> <li> <code>repo_name</code></li> <li> <code>repo_url</code></li> <li> <code>exclude_docs</code></li> </ul> </li> <li> <code>theme</code> configuration for <code>mkdocs-material</code><ul> <li> light/dark toggle</li> <li> custom icons/fonts</li> <li> <code>mkdocs-material</code> <code>features</code></li> <li> <code>search</code></li> <li> <code>navigation</code></li> <li> <code>content</code></li> </ul> </li> <li> <code>plugins</code><ul> <li> <code>literate-nav</code></li> <li> <code>section-index</code></li> </ul> </li> <li> <code>markdown_extensions</code></li> </ul> </li> </ul> </li> </ul>","tags":["python","mkdocs"]},{"location":"programming/powershell/index.html","title":"Powershell","text":"<p>Warning</p> <p>In progress...</p> <ul> <li>Read about enhancing your Powershell sessions with a profile.</li> <li>Check the snippets page for code examples with little/no explanation.</li> <li>Learn more about grouping a collection of related scripts into a Powershell module</li> </ul> <p>Todo</p> <ul> <li> Document writing functions</li> <li> Document executing on remote host</li> <li> Document favorite modules<ul> <li> <code>Get-AdUser</code></li> </ul> </li> <li> Document environment variables</li> <li> Document params</li> <li> Document arrays</li> <li> Document loops<ul> <li> If</li> <li> ForEach</li> <li> ForEach-Object</li> </ul> </li> <li> Document custom objects</li> <li> Document logging</li> <li> Document working with JSON</li> </ul>","tags":["windows","powershell"]},{"location":"programming/powershell/modules/index.html","title":"Powershell Modules","text":"<p>A module is a self-contained reusable unit that can contain cmdlets, providers, functions, variables, and other types of resources that can be imported as a single unit.</p> <p>- Microsoft Docs: about Modules</p> <p>Powershell modules are a way to combine multiple different scripts that are logically \"grouped\" into a single, importable module. For example, the <code>Az</code> Powershell module contains scripts and functions all related to interacting with an Azure environment.</p> <p>Todo</p> <ul> <li> Document initializing a Powershell module<ul> <li> Manually, using the <code>New-PSModuleManifest</code> function</li> <li> Automated, using the <code>Add-NewPSModule</code> script</li> </ul> </li> <li> Document loading functions from scripts/internal modules<ul> <li> Manually, by declaring functions in the <code>.psd1</code> module manifest</li> <li> Automatically using an init script in the <code>.psm1</code> module entrypoint</li> </ul> </li> <li> Document <code>Private</code> and <code>Public</code> functions<ul> <li> \"Private\" for internal module usage</li> <li> \"Public\" for functions/code exposed to the user</li> </ul> </li> <li> Document passing parameters to scripts within the module</li> </ul>"},{"location":"programming/powershell/modules/index.html#creating-a-new-powershell-module","title":"Creating a new Powershell module","text":""},{"location":"programming/powershell/modules/index.html#the-manual-way","title":"The manual way","text":"<p>Initialize a new Powershell module using the <code>New-ModuleManifest</code> cmdlet. Create a <code>$manifest</code> hashtable to pass params to the <code>New-ModuleManifest</code> script:</p> Manually create a new Powershell module<pre><code>$manifest = @{\n  Path = \".\\ModuleName\\ModuleName.psd1\"\n  RootModule = \"ModuleName.psm1\"\n  Author = \"Your Name\"\n}\n\n## Call New-ModuleManifest, passing the $manifest var defined above.\nNew-ModuleManifest @manifest\n</code></pre> <p>You can also pass these params using <code>-Params</code>, like:</p> New-ModuleManifest with params<pre><code>New-ModuleManifest -Path .\\ModuleName\\module.psd1 -ModuleVersion \"2.0\" -Author \"Your Name\" -Description \"Description for the module\"\n</code></pre> <p>As you add scripts to your module, import them by editing the <code>module.psd1</code> manifest file within the folder that is created by the <code>New-ModuleManifest</code> command, adding functions to expose to the user to the <code>FunctionsToExport = @()</code> array.</p> <p>Optionally, you can also export the functions from within the Powershell script by adding <code>Export-ModuleMember &lt;function-name&gt;</code> to the bottom of your <code>.ps1</code> scripts within the module.</p>"},{"location":"programming/powershell/modules/index.html#the-automatic-way","title":"The automatic way","text":"<p>Creating a Powershell module by hand involves a lot of manual setup, and many steps must be taken each time you modify the code in the module.</p> <p>To avoid mistakes and simplify setup/execution of your module, you can use a script to aid in creating the module. I name this script <code>Add-NewPSModule.ps1</code>, but you can call it whatever you like:</p> <p>Warning</p> <p>When using the automated \"init script\" in a <code>.psm1</code> module (shown below), you must be deliberate where you put your code. Anything in the <code>Public/</code> directory is exposed to the user of your module; if you have code you want to be able to use within your module, or templates like a <code>.json</code> or <code>.csv</code> file, it should be placed in the <code>Private/</code> directory and referenced in scripts in the <code>Public/</code> directory.</p> Add-NewPSModule.ps1 script<pre><code>## Set directory separator character, i.e. '\\' on Windows\n$DirectorySeparator = [System.IO.Path]::DirectorySeparatorChar\n## Set name of module from $PSScriptRoot\n$ModuleName = $PSScriptRoot.Split($DirectorySeparator)[-1]\n## Look for module manifest file\n$ModuleManifest = $PSScriptRoot + $DirectorySeparator + $ModuleName + '.psd1'\n## Loop Public/ directory and load all .ps1 files into var\n$PublicFunctionsPath = $PSScriptRoot + $DirectorySeparator + 'Public' + $DirectorySeparator + 'ps1'\n## Loop Private/ directory and load all .ps1 files into var\n$PrivateFunctionsPath = $PSScriptRoot + $DirectorySeparator + 'Private' + $DirectorySeparator + 'ps1'\n\n## Test the module manifest\n$CurrentManifest = Test-ModuleManifest $ModuleManifest\n\n$Aliases = @()\n\n## Get list of .ps1 files in Public/ recursively\n$PublicFunctions = Get-ChildItem -Path $PublicFunctionsPath -Recurse -Filter *.ps1\n## Get list of .ps1 files in Private/ recursively\n$PrivateFunctions = Get-ChildItem -Path $PrivateFunctionsPath -Recurse -Filter *.ps1\n\n## Load all Powershell functions from script files\n$PrivateFunctions | ForEach-Object { \n    Write-Verbose \"Loading private function from: $($_.FullName)\"\n    . $_.FullName \n}  # Load private functions first\n\n$PublicFunctions | ForEach-Object { \n    Write-Verbose \"Loading public function from: $($_.FullName)\"\n    . $_.FullName \n}   # Load public functions after\n\n## Export all public functions\n$PublicFunctionNames = $PublicFunctions | ForEach-Object { $_.BaseName }\nExport-ModuleMember -Function $PublicFunctionNames\n\n## Handle aliases if needed\n$PublicFunctions | ForEach-Object {\n    $alias = Get-Alias -Definition $_.BaseName -ErrorAction SilentlyContinue\n    if ($alias) {\n        $Aliases += $alias\n        ## Export aliased function, if one is defined\n        Export-ModuleMember -Alias $alias\n    }\n}\n\n## Add all functions loaded from $PublicFunctions to an array\n$FunctionsAdded = $PublicFunctions | Where-Object { $_.BaseName -notin $CurrentManifest.ExportedFunctions.Keys }\n## Remove any undetected functions from module manifest\n$FunctionsRemoved = $CurrentManifest.ExportedFunctions.Keys | Where-Object { $_ -notin $PublicFunctions.BaseName }\n\n$AliasesAdded = $Aliases | Where-Object { $_ -notin $CurrentManifest.ExportedAliases.Keys }\n$AliasesRemoved = $CurrentManifest.ExportedAliases.Keys | Where-Object { $_ -notin $Aliases }\n\nif ($FunctionsAdded -or $FunctionsRemoved -or $AliasesAdded -or $AliasesRemoved) {\n    try {\n        ## Update module manifest when changes are detected\n        $UpdateModuleManifestParams = @{}\n        $UpdateModuleManifestParams.Add('Path', $ModuleManifest)\n        $UpdateModuleManifestParams.Add('ErrorAction', 'Stop')\n        if ($Aliases.Count -gt 0) { $UpdateModuleManifestParams.Add('AliasesToExport', $Aliases) }\n        if ($PublicFunctionNames.Count -gt 0) { $UpdateModuleManifestParams.Add('FunctionsToExport', $PublicFunctionNames) }\n\n        Update-ModuleManifest @updateModuleManifestParams\n    }\n    catch {\n        $_ | Write-Error\n    }\n}\n</code></pre> <p>When your script is imported with <code>Import-Module</code>, the <code>.psm1</code> script at the root of the module is sourced, executing the code within. In this case, that code is iterating over the <code>Public/</code> and <code>Private/</code> directories and sourcing <code>.ps1</code> Powershell scripts.</p>"},{"location":"programming/powershell/profiles/index.html","title":"Powershell Profiles","text":"<p>Todo</p> <ul> <li> Link to helpful articles about setting up profiles</li> <li> Document helpful snippets<ul> <li> Automatic transcription (logging)</li> </ul> </li> <li> Demo example full Powershell profile</li> </ul> <p>A Powershell profile is a <code>.ps1</code> file, which does not exist by default but is expected at <code>C:\\Users\\&lt;username&gt;\\Documents\\WindowsPowerShell\\Microsoft.PowerShell_profile.ps1</code>, is a file that is \"sourced\"/loaded each time a Powershell session is opened. You can use a profile to customize your Powershell session. Functions declared in your profile are accessible to your whole session. You can set variables with default values, customize your session's colors (by editing a <code>function Prompt {}</code> section), split behavior between regular/elevated prompts, and more.</p>","tags":["windows","powershell","configuration"]},{"location":"programming/powershell/profiles/index.html#profile-snippets","title":"Profile Snippets","text":"","tags":["windows","powershell","configuration"]},{"location":"programming/powershell/profiles/index.html#useful-params","title":"Useful Params","text":"<p>At the top of your Powershell script, below your docstring, you can declare <code>param()</code> to enable passing <code>-Args</code> to your script, our <code>switches</code>.</p> Example params()<pre><code>&lt;#\n    Description: Example script with params\n\n    Usage:\n        ...\n#&gt;\n\nparam(\n    ## Enable transcription, like ~/.bashhistory\n    [bool]$Transcript = $True,\n    [bool]$ClearNewSession = $True\n)\n</code></pre>","tags":["windows","powershell","configuration"]},{"location":"programming/powershell/profiles/index.html#how-tos","title":"How-tos","text":"","tags":["windows","powershell","configuration"]},{"location":"programming/powershell/profiles/index.html#how-to-switches","title":"How to: switches","text":"","tags":["windows","powershell","configuration"]},{"location":"programming/powershell/profiles/index.html#how-to-trycatch","title":"How to: try/catch","text":"","tags":["windows","powershell","configuration"]},{"location":"programming/powershell/profiles/index.html#how-to-case-statements","title":"How to: case statements","text":"","tags":["windows","powershell","configuration"]},{"location":"programming/powershell/snippets/index.html","title":"Snippets","text":"<p>Code snippets with little-to-no explanation.</p>","tags":["windows","powershell","snippets"]},{"location":"programming/powershell/snippets/index.html#winget","title":"Winget","text":"","tags":["windows","powershell","snippets"]},{"location":"programming/powershell/snippets/index.html#importexport-winget-packages","title":"Import/Export Winget packages","text":"<p>Export a list of installed packages discovered by winget to a <code>.json</code> file, then import the list to reinstall everything. Useful as a backup, or to move to a new computer.</p> <p>Note</p> <p>The filename in the examples below, <code>C:\\path\\to\\winget-pkgs.json</code>, can be named anything you want, as long as it has a <code>.json</code> file extension.</p>","tags":["windows","powershell","snippets"]},{"location":"programming/powershell/snippets/index.html#export","title":"Export","text":"winget export<pre><code>## Set a path, the file must be a .json\n$ExportfilePath = \"C:\\path\\to\\winget-pkgs.json\"\n\n## Export package list\nwinget export -o \"$($ExportfilePath)\"\n</code></pre>","tags":["windows","powershell","snippets"]},{"location":"programming/powershell/snippets/index.html#import","title":"Import","text":"winget import<pre><code>## Set the path to your export .json file\n$ImportfilePath = \"C:\\path\\to\\winget-pkgs.json\"\n\n## Import package list\nwinget import -i \"$($ImportfilePath)\"\n</code></pre>","tags":["windows","powershell","snippets"]},{"location":"programming/powershell/snippets/index.html#get-uptime","title":"Get uptime","text":"Get machine uptime<pre><code>(Get-Date) \u2013 (Get-CimInstance Win32_OperatingSystem).LastBootUpTime\n</code></pre>","tags":["windows","powershell","snippets"]},{"location":"programming/powershell/snippets/index.html#functions","title":"Functions","text":"<p>Functions in your profile will be executed automatically if you call them within the profile, but they are also available to your entire session. For example, the <code>Edit-Profile</code> function function can be executed in any session that loads a profile with that function declared!</p>","tags":["windows","powershell","snippets"]},{"location":"programming/powershell/snippets/index.html#check-elevatedadmin","title":"Check elevated/admin","text":"<p>The function below returns <code>$True</code> if the current Powershell session is elevated, otherwise returns <code>$False</code>.</p> Check elevated session<pre><code>function Get-ElevatedShellStatus {\n    ## Check if current user is admin\n    $Identity = [Security.Principal.WindowsIdentity]::GetCurrent()\n    $Principal = New-Object Security.Principal.WindowsPrincipal $Identity\n    $AdminUser = $Principal.IsInRole([Security.Principal.WindowsBuiltInRole]::Administrator)\n\n    return $AdminUser\n}\n\n## Declare variable for references throughout script.\n#  Can be used to prevent script from exiting/crashing.\n$isAdmin = $(Get-ElevatedShellStatus)\n</code></pre>","tags":["windows","powershell","snippets"]},{"location":"programming/powershell/snippets/index.html#openexecute-as-admin","title":"Open/execute as admin","text":"Open as admin<pre><code>function Open-AsAdmin {\n    &lt;#\n        Run command as admin, or start new admin session if no args are passed\n    #&gt;\n    if ($args.Count -gt 0) {   \n        $argList = \"&amp; '\" + $args + \"'\"\n        Start-Process \"$psHome\\powershell.exe\" -Verb runAs -ArgumentList $argList\n    }\n    else {\n        Start-Process \"$psHome\\powershell.exe\" -Verb runAs\n    }\n}\n</code></pre>","tags":["windows","powershell","snippets"]},{"location":"programming/powershell/snippets/index.html#open-powershell-profileps1-file-for-editing","title":"Open Powershell profile.ps1 file for editing","text":"Edit profile<pre><code>function Edit-Profile {\n    &lt;#\n        Open current profile.ps1 in PowerShell ISE\n    #&gt;\n    If ($host.Name -match \"ise\") {\n        ## Edit in PowerShell ISE, if available\n        $psISE.CurrentPowerShellTab.Files.Add($profile.CurrentUserAllHosts)\n    }\n    Else {\n        ## Edit in Notepad if no PowerShell ISE found\n        notepad $profile.CurrentUserAllHosts\n    }\n}\n</code></pre>","tags":["windows","powershell","snippets"]},{"location":"programming/powershell/snippets/index.html#delay-conda-execution","title":"Delay Conda execution","text":"<p>Conda is a Python package manager. It's a very useful utility, but I've found adding it to my <code>$PATH</code> or Powershell profile results in a very slow session load in new tabs/windows. Adding the <code>SOURCE_CONDA</code> function below, and settings an alias to the <code>conda</code> command to call this function instead (<code>Set-Alias conda SOURCE_CONDA</code>), delays the sourcing of the <code>Conda</code> path. The first time you run <code>conda</code> in a new session, you will see a message that Conda has been initialized and you need to re-run your command. You can simply press the up key on your keyboard and run it again; now that Conda is initialized, it will execute, and once a Powershell session is loaded, sourcing Conda is much quicker!</p> Delay Conda sourcing<pre><code>function SOURCE_CONDA {\n    &lt;#\n      Initialize Conda only when the conda command is run.\n      Conda takes a while to initialize, and is not needed in\n      every PowerShell session\n    #&gt;\n    param(\n      [String]$CONDA_ROOT = \"%USERPROFILE%\\mambaforge\\Scripts\\conda.exe\"\n    )\n\n    #region conda initialize\n    # !! Contents within this block are managed by 'conda init' !!\n  (&amp; \"$CONDA_ROOT\" \"shell.powershell\" \"hook\") | Out-String | Invoke-Expression\n    #endregion\n\n    Write-Host \"Conda initialized. Run your command again.\"\n\n}\n</code></pre>","tags":["windows","powershell","snippets"]},{"location":"programming/powershell/snippets/index.html#get-system-uptime","title":"Get system uptime","text":"<p>Unix OSes have a very nice, simple command, <code>uptime</code>, that will simply print the number of days/hours/minutes your machine has been online. The Powershell syntax for this is difficult for me to remember, so my Powershell profile has an <code>uptime</code> function declared.</p> Get machine uptime<pre><code>function uptime {\n    ## Print system uptime\n\n    If ($PSVersionTable.PSVersion.Major -eq 5 ) {\n        Get-WmiObject win32_operatingsystem |\n        Select-Object @{EXPRESSION = { $_.ConverttoDateTime($_.lastbootuptime) } } | Format-Table -HideTableHeaders\n    }\n    Else {\n        net statistics workstation | Select-String \"since\" | foreach-object { $_.ToString().Replace('Statistics since ', '') }\n    }\n}\n</code></pre>","tags":["windows","powershell","snippets"]},{"location":"programming/powershell/snippets/index.html#unzip-function","title":"Unzip function","text":"<p>Unix OSes have a simple, easy to remember <code>unzip</code> command. This function tries to emulate that simplicity.</p> Unzip a file<pre><code>function unzip ($file) {\n    ## Extract zip archive to current directory\n\n    Write-Output(\"Extracting\", $file, \"to\", $pwd)\n    $fullFile = Get-ChildItem -Path $pwd -Filter .\\cove.zip | ForEach-Object { $_.FullName }\n    Expand-Archive -Path $fullFile -DestinationPath $pwd\n}\n</code></pre>","tags":["windows","powershell","snippets"]},{"location":"programming/powershell/snippets/index.html#touch-a-file-create-empty-file-if-one-doesnt-exist","title":"Touch a file (create empty file, if one doesn't exist)","text":"<p>Unix OSes have a useful utility called <code>touch</code>, which will create an empty file if one doesn't exist at the path you pass it, i.. <code>touch ./example.txt</code>. This function tries to emulate that usefulness and simplicity.</p> Powershell 'touch' equivalent<pre><code>function touch($file) {\n    ## Create a blank file at $file path\n\n    \"\" | Out-File $file -Encoding ASCII\n}\n</code></pre>","tags":["windows","powershell","snippets"]},{"location":"programming/powershell/snippets/index.html#lock-your-machine","title":"Lock your machine","text":"<p>Adding this function to your Powershell profile lets you lock your computer's screen by simply running <code>lock-screen</code> in a Powershell session.</p> Machine lock<pre><code>function lock-machine {\n    ## Set computer state to Locked\n\n    try {\n        rundll32.exe user32.dll, LockWorkStation\n    }\n    catch {\n        Write-Error \"Unhandled exception locking machine. Details: $($_.Exception.Message)\"\n    }\n\n}\n</code></pre>","tags":["windows","powershell","snippets"]},{"location":"programming/powershell/snippets/index.html#formatting","title":"Formatting","text":"","tags":["windows","powershell","snippets"]},{"location":"programming/powershell/snippets/index.html#inline-formatting-with-nonewline","title":"Inline formatting with -NoNewline","text":"<p>To format different parts of a <code>Write-Host</code> string without adding new lines, you can use the <code>-NoNewline;</code> param.</p> <p>A simple example of making the left part of a string green and the right part red:</p> -NoNewline example<pre><code>Write-Host \"I am green, \" -ForegroundColor Green -NoNewline; Write-Host \"and I am red!\" -ForegroundColor Red\n</code></pre> <p>Which outputs:</p> <p></p> <p>You can format long strings with <code>-NoNewline;</code> by entering a new line after the <code>;</code> character.</p> Powershell help menu with -NoNewline;<pre><code>Param(\n    [switch]$Help\n)\n\nIf ( $Help ) {\n    Write-Host \"[[ Get-SystemSpecReport Help ]]\" -ForegroundColor Green\n    Write-Host (\"-\" * 31)\n    Write-Host \"\"\n\n    Write-Host \"-Save\" -ForegroundColor Green -NoNewline; Write-Host \": Save report to file.\"\n    Write-Host \"-Debug\" -ForegroundColor Green -NoNewline; Write-Host \": Enable debug mode.\"\n    Write-Host \"-OutputDirectory\" -ForegroundColor Green -NoNewline; Write-Host \": Specify the output directory for the report file.\"\n    Write-Host \"-OutputFilename\" -ForegroundColor Green -NoNewline; Write-Host \": Specify the filename for the report file.\"\n    Write-Host \"-OutputFormat\" -ForegroundColor Green -NoNewline; Write-Host \": Specify the format for the report file (json, xml, txt).\"\n    Write-Host \"\"\n\n    ## Format shell code example using -NoNewline;\n    Write-Host \"Example\" -ForegroundColor Magenta -NoNewline; Write-Host \": Save report to C:\\Temp\\SystemReport.xml\"\n    Write-Host \"    $&gt; \" -NoNewline;\n    Write-Host \".\\Get-SystemSpecReport.ps1 \" -ForegroundColor Yellow -NoNewline;\n    Write-Host \"-Save \" -ForegroundColor Blue -NoNewline;\n    Write-Host \"-OutputDirectory \" -ForegroundColor Blue -NoNewline;\n    Write-Host \"C:\\Temp \" -NoNewline;\n    Write-Host \"-OutputFilename \" -ForegroundColor Blue -NoNewline;\n    Write-Host \"SystemReport \" -NoNewline;\n    Write-Host \"-OutputFormat \" -ForegroundColor Blue -NoNewline; \n    Write-Host  \"xml\"\n\n    exit 0\n}\n</code></pre> <p>The script content above outputs:</p> <p></p>","tags":["windows","powershell","snippets"]},{"location":"programming/python/index.html","title":"Python","text":"<p>Notes, links, &amp; reference code for Python programming. Also check the Python \"standard project files\" section for copy/paste-able code for things like a <code>[tool.ruff]</code> section for your <code>pyproject.toml</code>.</p> <p>This section has documentation for things like:</p> <ul> <li>Managing your Python install with <code>pyenv</code></li> <li>Managing your project with the <code>pdm</code> tool</li> <li>Documentation &amp; example sessions for <code>nox</code></li> <li>Building logging into your project with the stdlib <code>logging</code> module</li> </ul> <p>And more. Use the navigation on the left side of the page to browse through sections.</p>","tags":["python"]},{"location":"programming/python/dataclasses.html","title":"Dataclasses","text":"","tags":["python","stdlib","dataclasses"]},{"location":"programming/python/dataclasses.html#what-is-a-dataclass","title":"What is a <code>dataclass</code>?","text":"<p>A Python data class is a regular Python class that has the <code>@dataclass</code> decorator. It is specifically created to hold data (from python.land).</p> <p>Dataclasses reduce the boilerplate code when creating a Python class. As an example, below are 2 Python classes: the first is written with Python's standard class syntax, and the second is the simplified dataclass:</p> Standard class vs dataclass<pre><code>from dataclasses import dataclass\n\n## Standard Python class\nclass User:\n    def __init__(self, name: str, age: int, enabled: bool):\n        self.name = name\n        self.age = age\n        self.enabled = enabled\n\n\n## Python dataclass\n@dataclass\nclass User:\n    user: str\n    age: int\n    enabled: bool\n</code></pre> <p>With a regular Python class, you must write an <code>__init__()</code> method, define all your parameters, and assign the values to the <code>self</code> object. The dataclass removes the need for this <code>__init__()</code> method and simplifies writing the class.</p> <p>This example is so simple, it's hard to see the benefits of using a dataclass over a regular class. Dataclasses are a great way to quickly write a \"data container,\" i.e. if you're passing results back from a function:</p> Example dataclass function return<pre><code>from dataclasses import dataclass\n\n@dataclass\nclass FunctionResults:\n    original_value: int\n    new_value: int\n\n\ndef some_function(x: int = 0, _add: int = 15) -&gt; FunctionResults:\n    y = x + _add\n\n    return FunctionResults(original_value=x, new_value=y)\n\nfunction_results: FunctionResults = some_function(x=15)\n\nprint(function_results.new_value)  # = 30\n</code></pre> <p>Instead of returning a <code>dict</code>, returning a <code>dataclass</code> allows for accessing parameters using <code>.dot.notation</code>, like <code>function_results.original_value</code>, instead of <code>function_results[\"original_value\"]</code>.</p>","tags":["python","stdlib","dataclasses"]},{"location":"programming/python/dataclasses.html#dataclass-mixins","title":"Dataclass Mixins","text":"<p>A <code>mixin</code> class is a pre-defined class you define with certain properties/methods, where any class inheriting from this class will have access to those methods.</p> <p>For example, the <code>DictMixin</code> dataclass below adds a method <code>.as_dict()</code> to any dataclass that inherits from <code>DictMixin</code>.</p>","tags":["python","stdlib","dataclasses"]},{"location":"programming/python/dataclasses.html#dictmixin-class","title":"DictMixin class","text":"<p>Adds a <code>.as_dict()</code> method to any dataclass inheriting from this class. This is an alternative to <code>dataclasses.asdict(_dataclass_instance)</code>, but also not as flexible.</p> DictMixin<pre><code>from dataclasses import dataclass\nfrom typing import Generic, TypeVar\n\n## Generic type for dataclass classes\nT = TypeVar(\"T\")\n\n\n@dataclass\nclass DictMixin:\n    \"\"\"Mixin class to add \"as_dict()\" method to classes. Equivalent to .__dict__.\n\n    Adds a `.as_dict()` method to classes that inherit from this mixin. For example,\n    to add `.as_dict()` method to a parent class, where all children inherit the .as_dict()\n    function, declare parent as:\n\n    # ``` py linenums=\"1\"\n    # @dataclass\n    # class Parent(DictMixin):\n    #     ...\n    # ```\n\n    # and call like:\n\n    # ```py linenums=\"1\"\n    # p = Parent()\n    # p_dict = p.as_dict()\n    # ```\n    \"\"\"\n\n    def as_dict(self: Generic[T]):\n        \"\"\"Return dict representation of a dataclass instance.\n\n        Description:\n            self (Generic[T]): Any class that inherits from `DictMixin` will automatically have a method `.as_dict()`.\n                There are no extra params.\n\n        Returns:\n            A Python `dict` representation of a Python `dataclass` class.\n\n        \"\"\"\n        try:\n            return self.__dict__.copy()\n\n        except Exception as exc:\n            raise Exception(\n                f\"Unhandled exception converting class instance to dict. Details: {exc}\"\n            )\n\n## Demo inheriting from DictMixin\n@dataclass\nclass ExampleDictClass(DictMixin):\n    x: int\n    y: int\n    z: str\n\n\nexample: ExampleDictclass = ExampleDictClass(x=1, y=2, z=\"Hello, world!\")\nexample_dict: dict = example.as_dict()\n\nprint(example_dict)  # {\"x\": 1, \"y\": 2, \"z\": \"Hello, world!\"}\n</code></pre>","tags":["python","stdlib","dataclasses"]},{"location":"programming/python/dataclasses.html#jsonencodemixin","title":"JSONEncodeMixin","text":"<p>Inherit from the <code>json.JSONEncoder</code> class to allow returning a DataClass as a JSON encode-able dict.</p> JSONEncoder class inheritance<pre><code>import json\nfrom dataclasses import asdict\n\nclass DataclassEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if hasattr(obj, '__dict__'):\n            return obj.__dict__\n        elif hasattr(obj, '__dataclass_fields__'):\n            return asdict(obj)\n        return super().default(obj)\n\nperson = Person(name=\"Alice\", age=25)\njson.dumps(person, cls=DataclassEncoder)  # Returns '{\"name\": \"Alice\", \"age\": 25}'\n</code></pre>","tags":["python","stdlib","dataclasses"]},{"location":"programming/python/dataclasses.html#validating-a-dataclass","title":"Validating a Dataclass","text":"<p>Python dataclasses do not have built-in validation, like a <code>Pydantic</code> class. You can still use type hints to define variables, like <code>name: str = None</code>, but it has no actual effect on the dataclass.</p> <p>You can use the <code>__post_init__(self)</code> method of a dataclass to perform data validation. A few examples below:</p> Dataclass validation<pre><code>from dataclasses import dataclass\nimport typing as t\nfrom pathlib import Path\n\n\ndef validate_path(p: t.Union[str, Path] = None) -&gt; Path:\n    assert p, ValueError(\"Missing an input path to validate.\")\n    assert isinstance(p, str) or isinstance(p, Path), TypeError(f\"p must be a str or Path. Got type: ({type(p)})\")\n\n    p: Path = Path(f\"{p}\")\n    if \"~\" in f\"{p}\":\n        p = p.expanduser()\n\n    return p\n\n\n@dataclass\nclass ComputerDirectory:\n    ## Use | None in the annotation to denote an optional value\n    dir_name: str | None = None\n    dir_path: t.Union[str, Path] = None\n\n    def __post_init__(self):\n        if self.dir_name is None:\n            ## self.dir_name is allowed to be None\n            pass\n        else:\n            if not isinstance(self.dir_name, str):\n                raise TypeError(\"dir_name should be a string.\")\n\n        if self.dir_path is None:\n            raise ValueError(\"Missing required parameter: dir_path\")\n        else:\n            ## Validate self.dir_path with the validate_path() function\n            self.dir_path = validate_path(p=self.dir_path)\n</code></pre>","tags":["python","stdlib","dataclasses"]},{"location":"programming/python/dataclasses.html#links-extra-reading","title":"Links &amp; Extra Reading","text":"<ul> <li>Python docs: dataclasses</li> <li>RealPython: Python Dataclasses</li> <li>JSON encoding Python dataclasses</li> </ul>","tags":["python","stdlib","dataclasses"]},{"location":"programming/python/logging.html","title":"Logging","text":"<p>The Python <code>logging</code> module is a powerful, highly configurable logger for applications. The downside to <code>logging</code> is that it can be a little difficult to wrap your head around at first, and there are many, many, many ways to configure it.</p> <p>I have settled on using <code>logging.config.dictConfig</code> to configure my logging, and that is how my notes/code are written.</p>","tags":["python","stdlib","logging","configuration"]},{"location":"programming/python/logging.html#configuring-logging-module","title":"Configuring logging module","text":"<p>The <code>logging</code> module can be configured a number of ways, most notably with <code>basicConfig()</code>, but my personal favorite is with <code>dictConfig()</code>, which lets you pre-define your entire applications' logging (all current and potential future loggers) in a Python <code>dict</code>. No need to import handlers, formatters, and other miscellaneous classes from the <code>logging</code> module; with <code>dictConfig()</code>, everything is a string or integer!</p> <p>Shameless-plug</p> <p>I wrote a module, <code>red-logging</code> to aid with <code>logging</code> boilerplate. This module is specific to my own needs and uses, and may not be suitable for you or your project(s), but can serve as a helpful reference for some patterns.</p> <p><code>red-logging</code> does not import any dependencies, it simply organizes some of <code>logging</code>'s functionality into classes, context managers, and functions.</p>","tags":["python","stdlib","logging","configuration"]},{"location":"programming/python/logging.html#configure-with-basicconfig","title":"Configure with basicConfig()","text":"<p>The simplest, quickest way to configure Python's <code>logging</code> module is by using the <code>basicConfig()</code> function. As with other methods of initializing <code>logging</code>, you only need to call this once, at your program's entrypoint (i.e. at the top of a <code>if __name__ == \"__main__\"</code> statement, or in a <code>__main__.py</code> file).</p> Example of configuring logging with basicConfig()<pre><code>logging.basicConfig(\n    ## You can use logging.LEVEL or an uppercase string\n    level=\"INFO\",\n    ## Configure the log message string's format\n    format=\"%(asctime)s | [%(levelname)s] | (%(name)s) &gt; %(module)s.%(funcName)s:%(lineno)s |&gt; %(message)s\",\n    ## Configure the format for timestamps/datetimes in log messages (i.e. the %(asctime)s value\n    datefmt=\"%Y-%m-%dT%H:%M:%S\"\n)\n</code></pre>","tags":["python","stdlib","logging","configuration"]},{"location":"programming/python/logging.html#setup_logging-function","title":"setup_logging() function","text":"<p>I use a function in most of my apps called <code>setup_logging()</code> to configure logging with <code>basicConfig()</code> (if I'm not using <code>dictConfig()</code>). The function accepts a level, format, and datefmt, as well as a list of <code>silence_loggers</code>, which are string names of 3rd party modules to \"silence\" by setting their log level to <code>WARNING</code>. When using a <code>DEBUG</code> log level, you will also see debug messages for any 3rd party dependencies in your package (i.e. <code>SQLAlchemy</code>, <code>httpx</code>, etc); the <code>silence_loggers</code> list will stop these debug messages.</p> setup_logging()<pre><code>def setup_logging(\n    level: str = \"INFO\",\n    format: str = \"%(asctime)s | [%(levelname)s] | (%(name)s) &gt; %(module)s.%(funcName)s:%(lineno)s |&gt; %(message)s\",\n    datefmt: str = \"%Y-%m-%d_%H-%M-%S\",\n    silence_loggers: list[str] = [\n        \"httpx\",\n        \"hishel\",\n        \"httpcore\",\n        \"urllib3\",\n        \"sqlalchemy\",\n    ],\n):\n    logging.basicConfig(level=level, format=format, datefmt=datefmt)\n\n    if silence_loggers:\n        for _logger in silence_loggers:\n            logging.getLogger(_logger).setLevel(\"WARNING\")\n</code></pre>","tags":["python","stdlib","logging","configuration"]},{"location":"programming/python/logging.html#configure-with-dictconfig","title":"Configure with dictConfig()","text":"<p>You can use a custom <code>dict</code> to configure an entire app's logging all in one place.</p> <p>Warning</p> <p>When using <code>logging.config.dictConfig(logging_config_dict)</code>, be careful to only call <code>dictConfig()</code> once per execution. Calling this method multiple times can lead to instability in your logging as the logging config <code>dict</code>s overwrite each other.</p> <p><code>dictConfig()</code> should be called very early in your program's execution. You can put it in your root <code>__init__.py</code> file, or very early in the <code>main.py</code> file (or whatever entrypoint you run). If your project has multiple entrypoints, you can put <code>dictConfig()</code> below your <code>if __name__ == \"__main__\"</code>, but imports may fire log messages that do not get caught before the logger is configured, or they may be missed entirely.</p> <p>Finally, you can use a <code>setup_logging()</code> function, which you can store in a module and import into whatever entrypoint script you target. This is a configurable and flexible way to manage logging.</p>","tags":["python","stdlib","logging","configuration"]},{"location":"programming/python/logging.html#dictconfig-keys","title":"dictConfig keys","text":"<p>The following options are available as <code>dict</code> keys for your logging config <code>dict</code>.</p> Key Description version an integer indicating the schema version that is being used. If the logging configuration schema changes in the future, the\u00a0<code>version</code>\u00a0key will be used to indicate which version of the schema the\u00a0<code>dictConfig</code>\u00a0is using. This allows the\u00a0<code>dictConfig</code>\u00a0function to handle both current and future schema versions. As of 12/19/2024, <code>1</code> is the only accepted value for <code>version</code>. Read more about the <code>version</code> parameter. formatters a dictionary with each key being a formatter id and its value describing how to configure the corresponding\u00a0Formatter\u00a0instance. filters a dictionary with each key being a filter id and its value describing how to configure the corresponding\u00a0Filter\u00a0instance. handlers a dictionary with each key being a handler id and its value describing how to configure the corresponding\u00a0Handler\u00a0instance. All other keys are passed through as keyword arguments to the handler\u2019s constructor. loggers a dictionary with each key being a logger name and its value describing how to configure the corresponding Logger instance. root the configuration for the root logger. It\u2019s processed like any other logger, except that the propagate setting is not applicable. incremental a boolean indicating whether the configuration specified in the dictionary should be merged with any existing configuration, or should replace entirely. Its default value is\u00a0<code>False</code>, which means that the specified configuration replaces any existing configuration. disable_existing_loggers a boolean indicating whether any non-root loggers that currently exist should be disabled. If absent, this parameter defaults to\u00a0<code>True</code>. Its value is ignored when incremental is\u00a0<code>True</code>.","tags":["python","stdlib","logging","configuration"]},{"location":"programming/python/logging.html#logging-config-dict-template","title":"Logging config dict template","text":"<p>This <code>logging_config_template</code> <code>dict</code> can serve as a base/root for your project. Copy/paste the code and modify it to your needs, adding formatters, handlers, etc.</p> logging_config dict template<pre><code>logging_config_template: dict = {\n\u00a0 \u00a0 \"version\": 1,\n\u00a0 \u00a0 \"disable_existing_loggers\": False,\n\u00a0 \u00a0 \"propagate\": True,\n\u00a0 \u00a0 \"root\": {},\n\u00a0 \u00a0 \"formatters\": {},\n\u00a0 \u00a0 \"handlers\": {},\n\u00a0 \u00a0 \"loggers\": {},\n}\n</code></pre>","tags":["python","stdlib","logging","configuration"]},{"location":"programming/python/logging.html#example-logging-config-dict","title":"Example logging config dict","text":"<p>This is an example of a fully configured <code>logging</code> config dict. It includes a couple of console handlers, one which filters messages to only show <code>CRITICAL</code> and above, a custom format, and a number of loggers.</p> <p>Note</p> <p>You only need to configure the root (<code>\"\"</code>) logger once; the configuration below configures te root logger in the <code>\"root\"</code> dictionary key, as well as in <code>\"loggers\": {\"\": {}}</code>. In a real logging configuration dict, you would only do one of these. I personally prefer putting my configuration in the top-level <code>\"root\"</code> key.</p> Example logging config dict<pre><code>## Root logger config. Use with .dictConfig(log_config)\nlog_config: dict = {\n    \"version\":1,\n    ## When True, clears all logging configuration.\n    #  Best used only on the root logger\n    \"disable_existing_loggers\": True,\n    ## Configure the root logger\n    \"root\":{\n        ## Set handlers\n        \"handlers\" : [\"console\"],\n        ## Set log level string\n        \"level\": \"DEBUG\"\n    },\n    ## Configure handlers\n    \"handlers\":{\n        ## Default console logger\n        \"console\":{\n            ## Use stdout stream\n            \"stream\": \"ext://sys.stdout\",\n            ## Name of formatter\n            \"formatter\": \"std_out\",\n            ## The logging module to subclass\n            \"class\": \"logging.StreamHandler\",\n            ## Log level for this logger\n            \"level\": \"DEBUG\"\n        },\n        ## Only log CRITICAL messages with with `.getLogger(\"cli\")`\n        \"cli\": {\n            \"console\": {\n                \"level\": \"CRITICAL\",\n                \"class\": \"logging.StreamHandler\",\n                \"formatter\": \"consoleFormatter\",\n            },\n        }\n    },\n    ## Configure formatters for log messages\n    \"formatters\": {\n        ## Set formatting for std_out formatter messages &amp; dates\n        \"std_out\": {\n            \"format\": \"%(asctime)s : %(levelname)s : %(module)s : %(funcName)s : %(lineno)d : (Process Details : (%(process)d, %(processName)s), Thread Details : (%(thread)d, %(threadName)s))\\nLog : %(message)s\",\n            \"datefmt\":\"%d-%m-%Y %I:%M:%S\"\n        }\n    },\n    ## (Optional) pre-configure logger namespaces.\n    #  Example: you have a module named `requests.py`; define\n    #  a \"requests\" logger below, and when `requests.py` uses\n    #  `logging.getLogger(__name__)`, it will use the \"requests\"\n    #  logger config.\n    #  This can also be used to override module loggers, like `uvicorn`\n    \"loggers\": {\n        \"\": {\n            \"handlers\": [\"console\"],\n            ## Hide all but warning messages on root logger\n            \"level\": \"WARNING\",\n            \"propagate\": False\n        },\n        ## if __name__ == '__main__'\n        '__main__': {\n            'handlers': ['console'],\n            'level': 'DEBUG',\n            'propagate': False\n        },\n        ## Suppress all log messages.\n        #  Usage: logging.getLogger(\"silent\")\n        \"silent\": {\n            \"level:\" \"NOTSET\"\n        },\n        ## Disable logs from the requests module by setting to WARNING\n        \"requests\": {\n            ## Set to warning to disable logging of 3rd party modules\n            \"level\": \"WARNING\"\n        },\n        ## Set logging for a module named my_module in advance\n        \"my_module\": {\n            \"level\": \"INFO\"\n        }\n    },\n}\n</code></pre>","tags":["python","stdlib","logging","configuration"]},{"location":"programming/python/logging.html#log-levels","title":"Log Levels","text":"Value Int Level Description <code>NOTSET</code> <code>0</code> It is the lowest level in the logging hierarchy and is used to indicate that no specific logging level has been set for a logger or a handler (more on handler later on in the article). It is essentially a placeholder level that is used when the logging level is not explicitly defined. <code>DEBUG</code> <code>10</code> It is used for low-level debugging messages that provide detailed information about the code\u2019s behavior. These messages are typically used during development and are not required in production. <code>INFO</code> <code>20</code> It is used to log informational messages about the program\u2019s behavior. These messages can be used to track the program\u2019s progress or to provide context about the user. <code>WARNING</code> <code>30</code> It is used to log messages that indicate potential issues or unexpected behavior in the program. These messages do not necessarily indicate an error but are useful for diagnosing problems. <code>ERROR</code> <code>40</code> It is used to log messages that indicate an error has occurred in the program. These messages can be used to identify and diagnose problems in the code. <code>CRITICAL</code> <code>50</code> It is used to log messages that indicate a critical error has occurred that prevents the program from functioning correctly. These messages should be used sparingly and only when necessary.","tags":["python","stdlib","logging","configuration"]},{"location":"programming/python/logging.html#creating-a-custom-log-level","title":"Creating a custom log level","text":"<p>Example: <code>VERBOSE</code></p> Example VERBOSE custom log level<pre><code>import logging\n\n# Define the custom log level\nVERBOSE = 15\nlogging.VERBOSE = VERBOSE\nlogging.addLevelName(logging.VERBOSE, 'VERBOSE')\n\n# Set up basic logging configuration for the root logger\nlogging.basicConfig(level=logging.DEBUG)\n\n\n# Define a custom logging method for the new level\ndef verbose(self, message, *args, **kwargs):\n    if self.isEnabledFor(logging.VERBOSE):\n        self._log(logging.VERBOSE, message, args, **kwargs)\n\n\n# Add the custom logging method to the logger class\nlogging.Logger.verbose = verbose\n\n# Create a logger instance\nlogger = logging.getLogger()\n\n# Log a message using the custom level and method\nlogger.verbose(\"This is a verbose message\")\n</code></pre>","tags":["python","stdlib","logging","configuration"]},{"location":"programming/python/logging.html#links","title":"Links","text":"Title URL Python docs: <code>logging</code> https://docs.python.org/3/library/logging.html Python docs: <code>logging.config</code> https://docs.python.org/3/library/logging.config.html Python docs: <code>logging.config.dictConfig</code> https://docs.python.org/3/library/logging.config.html#logging.config.dictConfig Python docs: <code>LogRecord</code> attributes (formatting) https://docs.python.org/3/library/logging.html#logrecord-attributes Python docs: <code>time.strftime</code> (timestamp formatting) https://docs.python.org/3/library/time.html#time.strftime Python docs: <code>logging</code> <code>Handler</code> objects https://docs.python.org/3/library/logging.html#handler-objects","tags":["python","stdlib","logging","configuration"]},{"location":"programming/python/package_imports.html","title":"Package imports","text":"<p>When developing Python packages/modules, you will often want to break your functions into multiple scripts and sub-modules. You will use the <code>__init__.py</code> files throughout the package to import functionality from the <code>.py</code> script files you write.</p> <p>Over time, as the module grows, certain methods of importing functions &amp; objects from the module's packages will cause slowdowns and errors. Python has an <code>__all__</code> dunder method to control what is exported from a <code>.py</code> file. Adding defined functions, variables, and objects to the <code>__all__</code> list for a package allows you to write a simple <code>from .script_name import *</code> in the module's <code>__init__.py</code>.</p>","tags":["python","stdlib","imports"]},{"location":"programming/python/package_imports.html#example-package","title":"Example package","text":"<p>The following is an example of a Python module. Pretend you're building an app, and you created a <code>packages/</code> directory in the repository root. Each directory in <code>packages/</code> represents a Python package, an isolated collection of modules which contain <code>.py</code> scripts with functionality you want to import into the rest of your app.</p> <p>In <code>packages/example/</code>, you are writing \"helper\" functions for operations like path and string manipulation.</p> example package layout<pre><code>example/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 io\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 pathio.py\n\u2514\u2500\u2500 str_ops\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 _manipulate.py\n</code></pre> <p>Each <code>__init__.py</code> imports the <code>__all__</code> list from the associated <code>.py</code> file. For example, the <code>example/io/__init__.py</code> file's contents are:</p> example/io/__init__.py<pre><code>## Import everything from pathio's __all__\nfrom .pathio import *\n</code></pre>","tags":["python","stdlib","imports"]},{"location":"programming/python/package_imports.html#exampleiopathiopy","title":"example/io/pathio.py","text":"<p>In <code>example/io/pathio.py</code>, there are functions defined and a variable called <code>__all__</code>, which is a list of strings matching functions and/or variables in the <code>pathio.py</code> file:</p> example/io/pathio.py<pre><code>from pathlib import Path\nimport typing as t\n\n## Export specific functions from the app\n__all__ = [\"return_path_as_str\", \"path_exists\", \"expand_path\"]\n\n## This variable is not in __all__, and won't be exported\nexample_var: str = \"I won't be exported!\"\n\n\ndef return_path_as_str(path: Path) -&gt; str:\n    \"\"\"Return a Path object as a string.\"\"\"\n    return str(expand_path(path))\n\n\ndef path_exists(path: t.Union[str, Path]) -&gt; bool:\n    \"\"\"Return True if path exists, else return False.\"\"\"\n    _path = expand_path(path)\n\n    return _path.exists()\n\n\ndef expand_path(path: t.Union[str, Path]) -&gt; Path:\n    \"\"\"Expand a Path to its absolute path.\"\"\"\n    return Path(str(path)).expanduser() if \"~\" in str(path) else Path(str(path))\n\n\n## Function is not in __all__, and won't be exported\ndef example_function() -&gt; None:\n    \"\"\"Example function that is not explicitly exported.\"\"\"\n    print(\"I am not exported!\")\n</code></pre>","tags":["python","stdlib","imports"]},{"location":"programming/python/package_imports.html#importing-the-exampleiopathio-module","title":"Importing the example.io.pathio module","text":"<p>Only the functions/variables defined in <code>__all__</code> will be available with <code>from example.io import pathio</code>. For example:</p> Script that imports the example io.pathio module<pre><code>## Import the pathio module. Now, anything in __all__ from pathio is availalbe.\nfrom example.io import pathio\n\n## Use the io.pathio.path_exists() function to test if a path exists\npathio.path_exists(\"example/path\")\n\n## The example_var was not added to __all__, so it is not available\nprint(pathio.example_var)\n</code></pre>","tags":["python","stdlib","imports"]},{"location":"programming/python/package_imports.html#examplestr_ops","title":"example/str_ops/","text":"<p>The same is true for <code>example.str_ops</code>, which imports the <code>_manipulate.py</code>'s functions with <code>__all__</code>.</p> example/str_ops/_manipulate.py<pre><code>__all__ = [\"to_uppercase\", \"to_lowercase\", \"to_titlecase\", \"remove_word_from_str\", \"example_var\"]\n\nexample_var: str = \"I will be exported!\"\n\ndef to_uppercase(_str: str) -&gt; str:\n    \"\"\"Return uppercased string.\"\"\"\n    return _str.upper()\n\ndef to_lowercase(_str: str) -&gt; str:\n    \"\"\"Return a lowercased string.\"\"\"\n    return _str.lower()\n\ndef to_titlecase(_str: str) -&gt; str:\n    \"\"\"Return Title-Cased string.\"\"\"\n    return _str.title()\n\ndef remove_word_from_str(_str: str, remove_word: str) -&gt; str:\n    \"\"\"Remove instances of a substring by replacing with ''.\"\"\"\n    return _str.replace(remove_word, \"\")\n</code></pre> <p>This module exports the <code>example_var</code>, unlike <code>example.io.pathio</code>'s <code>example_var</code>.</p>","tags":["python","stdlib","imports"]},{"location":"programming/python/package_imports.html#importing-the-examplestr_ops-module","title":"Importing the example.str_ops module","text":"<p>The <code>example.str_ops.__init__.py</code> file imports everything from _manipulate.py:</p> example/str_ops/__init__.py<pre><code>from ._manipulate import *\n</code></pre> <p>This makes the <code>to_uppercase</code>, <code>to_lowercase</code>, <code>to_titlecase</code>, <code>remove_word_from_str</code>, functions are available, as well as the <code>example_var</code> variable:</p> Script that imports the example str_ops module<pre><code>## Import the str_ops module. Now, anything in __all__ from str_ops is availalbe.\nfrom example import str_ops\n\n## Use the str_ops.to_uppercase() function to uppercase a string\nuppercased: str = str_ops.to_uppercase(\"uppercased string\") # becomes \"UPPERCASED STRING\"\n\n## The example_var was added to __all__, so it is available\nprint(str_ops.example_var) # Prints \"I will be exported!\"\n</code></pre>","tags":["python","stdlib","imports"]},{"location":"programming/python/package_imports.html#summary","title":"Summary","text":"<p>By adding an <code>__all__</code> list variable to your Python module scripts, you can control what is exported. This way, you can have private variables/functions accessible only from within the <code>.py</code> script file, and public/exported objects that are available when you import the module in another script.</p> <p>Using an <code>__all__</code> list also simplifies your imports in <code>__init__.py</code> files. You can simply use <code>from .script_name import *</code>, which will only import what's listed in that file's <code>__all__</code>.</p>","tags":["python","stdlib","imports"]},{"location":"programming/python/pdm.html","title":"Use PDM to manage your Python projects &amp; dependencies","text":"<p>Toc</p> <ul> <li><code>pdm</code> cheat sheet</li> <li>What is PDM?<ul> <li>What problem does PDM solve?</li> </ul> </li> <li>How to install PDM</li> <li>What is a \"library\" vs an \"application\"</li> </ul>","tags":["python","pdm"]},{"location":"programming/python/pdm.html#pdm-cheat-sheet","title":"PDM cheat sheet","text":"Command Description Notes <code>pdm init</code> Initialize a new project with <code>pdm</code> The <code>pdm</code> tool will ask a series of questions, and build an environment based on your responses <code>pdm add &lt;package-name&gt;</code> Add a Python package to the project's dependencies. <code>pdm</code> will automatically find all required dependencies and add them to the install command, and will ensure the package installed is compatible with your environment. <code>pdm add -d &lt;package-name&gt;</code> Add a Python package to the project's <code>development</code> dependency group. Use this for packages that should not be built with your Python package. Useful for things like formatters (<code>black</code>, <code>ruff</code>, <code>pyflakes</code>, etc), testing suites (<code>pytest</code>, <code>pytest-xidst</code>), automation tools (<code>nox</code>, <code>tox</code>), etc. <code>pdm add -G standard &lt;package-name&gt;</code> Add a Python package to the project's <code>standard</code> group. In this example, <code>standard</code> is a custom dependency group. You can use whatever name you want, and users can install the dependencies in this group with <code>pip install &lt;project-name&gt;[standard]</code> (or with <code>pdm</code>: <code>pdm add &lt;project-name&gt;[standard]</code>) <code>pdm remove &lt;package-name&gt;</code> Remove a Python dependency from the project. Analagous to <code>pip uninstall &lt;package-name&gt;</code> <code>pdm remove -d &lt;package-name&gt;</code> Remove a Python development dependency from the project. <code>pdm remove -G standard &lt;package-name&gt;</code> Remove a Python dependency from the <code>standard</code> (or whatever group name you're targeting) dependency group. <code>standard</code> is an example name. You can use whatever name you want for dependency groups, as long as they adhere to the naming rules. You will be warned if a name is not compatible. <code>pdm update</code> Update all dependencies in the <code>pdm.lock</code> file. <code>pdm update &lt;package-name&gt;</code> Update a specific dependency. <code>pdm update -d &lt;package-name&gt;</code> Update a specific development dependency. <code>pdm update -G standard &lt;package-name&gt;</code> Update a specific dependency in the \"standard\" dependency group. <code>standard</code> is an example name; make sure you're using the right group name. <code>pdm update -d</code> Update all development dependencies. <code>pdm update -G standard</code> Update all dependencies in the dependency group named \"standard\". <code>pdm update -G \"standard,another_group\"</code> Update multiple dependency groups at the same time. <code>pdm lock</code> Lock dependencies to a <code>pdm.lock</code> file. Helps <code>pdm</code> with reproducible builds. <code>pdm run python src/app_name/main.py</code> Run the <code>main.py</code> file using the <code>pdm</code>-controller Python executable. Prepending Python commands with <code>pdm run</code> ensures you are running them with the <code>pdm</code> Python version instead of the system's Python. You can also activate the <code>.venv</code> environment and drop <code>pdm run</code> from the beginning of the command to accomplish the same thing. <code>pdm run custom-script</code> Execute a script defined in <code>pyproject.toml</code> file named <code>custom-script</code>. PDM scripts","tags":["python","pdm"]},{"location":"programming/python/pdm.html#what-is-pdm","title":"What is PDM?","text":"<p>\ud83d\udd17 PDM official site</p> <p>\ud83d\udd17 PDM GitHub</p> <p>PDM (Python Dependency Manager) is a tool that helps manage Python projects &amp; dependencies. It does many things, but a simple way to describe it is that it replaces <code>virtualenv</code> and <code>pip</code>, and ensures when you install dependencies, they will be compatible with the current Python version and other dependencies you add to the project. Some dependencies require other dependencies, and where <code>pip</code> might give you an error about a <code>ModuleNotFound</code>, <code>pdm</code> is smart enough to see that there are extra dependencies and will automatically add them as it installs the dependency you asked for.</p> <p>Note</p> <p>Unlike <code>pip</code>, <code>pdm</code> uses a <code>pyproject.toml</code> file to manage your project and its dependencies. This is a flexible file where you can add metadata to your project, create custom <code>pdm</code> scripts, &amp; more.</p>","tags":["python","pdm"]},{"location":"programming/python/pdm.html#what-problem-does-pdm-solve","title":"What problem does PDM solve?","text":"<p>If you have already used tools like <code>poetry</code> or <code>conda</code>, you are familiar with the problem <code>pdm</code> solves. In a nutshell, dependency management in Python is painful, frustrating, and difficult. <code>pdm</code> (and other similar tools) try to solve this problem using a \"dependency matrix,\" which ensures the dependencies you add to your project remain compatible with each other. This approach helps avoid \"dependency hell,\" keeps your packages isolated to the project they were installed for, and skips the <code>ModuleNotFound</code> error you will see in <code>pip</code> when a dependency requires other dependencies to install.</p> <p><code>pdm</code> is also useful for sharing code, as anyone else who uses <code>pdm</code> can install the exact same set of dependencies that you installed on your machine. This is accomplished using a \"lockfile,\" which is a special <code>pdm.lock</code> file the <code>pdm</code> tool creates any time you install a dependency in your project. Unless you manually update the package, any time you run <code>pdm install</code>, it will install the exact same set of dependencies, down to the minor release number, keeping surprise errors at bay if you update a package that does not \"fit\" in your current environment by ensuring a specific, compatible version of that package is installed.</p> <p><code>pdm</code> can also help you build your projects and publish them to <code>pypi</code>, and can install your development tools (like <code>black</code>, <code>ruff</code>, <code>pytest</code>, etc) outside of your project's environment, separating development and production dependencies. <code>pdm</code> also makes it easy to create dependency \"groups,\" where a user can install your package with syntax like <code>package_name[group]</code>. You may have seen this syntax in the <code>pandas</code> or <code>uvicorn</code> packages, where the documentation will tell you to install like <code>pandas[excel]</code> or <code>uvicorn[standard]</code>. The maintainers of these packages have created different dependency \"groups,\" where if you only need the Excel functionality included in <code>pandas</code>, for example, it will only install the dependencies required for that group, instead of all of the dependencies the <code>pandas</code> package requires.</p> <p>Lastly, <code>pdm</code> can make you more efficient by providing functionality for scripting. With <code>pdm</code>, you can create scripts like <code>start</code> or <code>start-dev</code> to control executing commands you would manually re-type each time you wanted to run them.</p>","tags":["python","pdm"]},{"location":"programming/python/pdm.html#how-to-install-pdm","title":"How to install PDM","text":"<p>Note</p> <p>Please refer to the Official PDM installation instructions for instructions. I personally use the <code>pipx</code> method (<code>pipx install pdm</code>), but there are multiple sets of instructions that are updated occasionally, and it is best to check PDM's official documentation for installation/setup instructions.</p>","tags":["python","pdm"]},{"location":"programming/python/pdm.html#library-vs-application","title":"Library vs Application","text":"<p>When you initialize a new project with <code>pdm init</code>, you will be asked a series of questions that will help <code>pdm</code> determine the type of environment to set up. One of the questions asks if your project is a library or application:</p> PDM library or app question<pre><code>Is the project a library that is installable?\nIf yes, we will need to ask a few more questions to include the project name and build backend [y/n] (n):\n</code></pre> <p>Warning</p> <p>This choice does matter! It affects how Python structures your project, and how the project executes. You will need to answer a couple of extra questions for an \"application,\" but there is a correct way to answer this question when <code>pdm</code> asks it.</p> <p>Continue reading for more information.</p> <p>For a more in-depth description of when to choose a \"library\" vs an \"application\", please read the Official PDM documentation.</p> <p>As a rule of thumb, modules you import into other scripts/apps and do not have a CLI tool (like the <code>pydantic</code> or <code>requests</code> modules) are a \"library.\" A module that includes a CLI and can be executed from the command line, but is built with Python (like <code>black</code>, <code>ruff</code>, <code>pytest</code>, <code>mypy</code>, <code>uvicorn</code>, etc) are an \"application.\" </p>","tags":["python","pdm"]},{"location":"programming/python/project_structure.html","title":"Python project structure","text":"<p>Todo</p> <ul> <li> Write comparison of \"flat\" vs <code>src/</code> layout</li> <li> Document basic examples of a flat &amp; <code>src/</code> project</li> <li> Document monorepos</li> </ul>","tags":["python"]},{"location":"programming/python/pyenv.html","title":"Use pyenv for easier Python version management","text":"<p>Toc</p> <p>Jump right to a section:</p> <ul> <li>Pyenv commands cheat-sheet</li> <li>Install pyenv</li> <li>Windows instructions</li> <li>Linux/WSL instructions</li> <li>Using pyenv</li> </ul>","tags":["python","pyenv","environment"]},{"location":"programming/python/pyenv.html#tldr","title":"TL;DR","text":"<p>If you're just here for a quick reference, use the sections below for quick <code>pyenv</code> setup &amp; usage instructions. Any sections linked here will have a button that returns you to this <code>TL;DR</code> section, so if you're curious about one of the steps and follow a link you can get right back to it when you're done reading.</p> <p>This is what the button will look like: \u2934\ufe0f back to <code>TL;DR</code></p>","tags":["python","pyenv","environment"]},{"location":"programming/python/pyenv.html#tldr-1-install-pyenv","title":"TLDR 1: Install pyenv","text":"<p>See the Install pyenv section for OS-specific installation instructions.</p> <p>Warning</p> <p>If you skip the installation instructions, make sure you add <code>pyenv</code> to your <code>PATH</code>. If you need help setting <code>PATH</code> variables, please read the Install pyenv section.</p> <ul> <li>Linux/WSL:<ul> <li>Edit <code>~/.bashrc</code>, add this to the bottom of the file (if in a terminal environment, use an editor like <code>$ nano ~.bashrc</code>):</li> </ul> </li> </ul> Add pyenv to PATH<pre><code>export PATH=\"$HOME/.pyenv/bin:$PATH\"\neval \"$(pyenv init --path)\"\neval \"$(pyenv virtualenv-init -)\"\n</code></pre> <ul> <li>Windows:<ul> <li>Edit the user's <code>PATH</code> variable, add these 2 paths:</li> </ul> </li> </ul> Add pyenv to the Windows path<pre><code>%USERPROFILE%\\\\.pyenv\\\\pyenv-win\\\\bin\n%USERPROFILE%\\\\.pyenv\\\\pyenv-win\\\\shims\n</code></pre>","tags":["python","pyenv","environment"]},{"location":"programming/python/pyenv.html#tldr-2-choose-from-available-python-versions","title":"TLDR 2: Choose from available Python versions","text":"<p>Show versions available for install. Update the list with <code>$ pyenv update</code> if you don't see the version you want.</p> list &amp; install python versions<pre><code>$ pyenv install -l\n# 3.xx.x\n# 3.xx.x-xxx\n# ...\n\n$ pyenv install 3.12.1\n</code></pre>","tags":["python","pyenv","environment"]},{"location":"programming/python/pyenv.html#tldr-3-set-your-global-local-shell-python-versions","title":"TLDR 3: Set your global, local, &amp; shell Python versions","text":"<p>Warning</p> <p>The <code>3.12.1</code> version string is used as an example throughout the documentation. Make sure you're using a more recent version, if one is available.</p> <p>You can check by running <code>$ pyenv update &amp;&amp; pyenv install -l</code>.</p> <p>Examples commands</p> <ul> <li>Set <code>global</code> version to <code>3.12.1</code>: <code>$ pyenv global 3.12.1</code></li> <li>Set <code>global</code> to multiple versions, <code>3.12.1</code> + <code>3.11.8</code>: <code>$ pyenv global 3.12.1 3.11.8</code><ul> <li> <p>Version-order</p> The order of the versions you specify does matter. For example, in the command above, <code>3.12.1</code> is the \"primary\" Python, but <code>3.11.8</code> will also be available for tools like <code>pytest</code>, which can run tests against multiple versions of Python, provided they are available in the <code>PATH</code>. </li> </ul> </li> <li>Set <code>local</code> version to <code>3.11.8</code>, creating a <code>.python-version</code> file in the process: <code>$ pyenv local 3.11.8</code><ul> <li>Setting a <code>local</code> version will override the <code>global</code> setting</li> </ul> </li> <li>Set <code>local</code> to multiple versions, <code>3.12.1</code> + <code>3.11.8</code>: <code>$ pyenv local 3.12.1 3.11.8</code></li> <li>Set <code>shell</code> version to `3.</li> </ul>","tags":["python","pyenv","environment"]},{"location":"programming/python/pyenv.html#pyenv-cheat-sheet","title":"Pyenv cheat sheet","text":"Command Description Notes <code>pyenv commands</code> List available pyenv commands Useful as a quick reference if you forget a command name. Does not include help/man text. To see available help for a command, run it without any parameters, i.e. <code>pyenv whence</code> <code>pyenv update</code> Refresh pyenv's repository Updates the <code>pyenv</code> utility, fetches new available Python versions, etc. Run this command occasionally to keep everything up to date. <code>pyenv install 3.xx.xx</code> Install a version of Python If you don't see the version you want, run <code>pyenv update</code> <code>pyenv uninstall 3.xx.xx</code> Uninstall a version of Python that was installed with <code>pyenv</code>. You can uninstall multiple versions at a time with <code>pyenv uninstall 3.11.2 3.11.4</code> <code>pyenv global 3.xx.xx</code> Set the global/default Python version to use. You can set multiple versions like <code>pyenv global 3.11.4 3.11.6 3.12.2</code>. Run without any args to print the current global Python interpreter(s). <code>pyenv local 3.xx.xx</code> Set the local Python interpreter. Creates a file in the current directory <code>.python-version</code>, which <code>pyenv</code> will detect and will set your interpreter to the version specified. Like <code>pyenv global</code>, you can set multiple versions with <code>pyenv local 3.11.4 3.11.6 3.12.2</code>. Run without any args to print the current global Python interpreter(s). <code>pyenv shell 3.xx.xx</code> Set the Python interpreter for the current shell session. Resets when session exits. Like <code>pyenv global</code>, you can set multiple versions with <code>pyenv shell 3.11.4 3.11.6 3.12.2</code>. Run without any args to print the current global Python interpreter(s). <code>pyenv versions</code> List versions of Python installed with Pyenv &amp; available for use. Versions will be printed as a list of Python version numbers, and may include interpreter paths. <code>pyenv which &lt;executable&gt;</code> Print the path to a pyenv-installed executable, i.e. <code>pip</code> Helps with troubleshooting unexpected behavior. If you suspect <code>pyenv</code> is using the wrong interpreter, check the path with <code>pyenv which python</code>, for example. <code>pyenv rehash</code> Rehash pyenv shims. Run this after switching Python versions with <code>pyenv</code> if you're getting unexpected outputs.","tags":["python","pyenv","environment"]},{"location":"programming/python/pyenv.html#what-is-pyenv","title":"What is pyenv","text":"<p><code>pyenv</code> is a tool for managing versions of Python. It handles downloading the version archive, extracting &amp; installing, and can isolate different versions of Python into their own individual environments.</p> <p><code>pyenv</code> can also handle running multiple versions of Python at the same time. For example, when using <code>nox</code>, you can declare a list of Python versions the session should run on, like <code>@nox.session(python=[\"3.11.2\", \"3.11.4\", \"3.12.1\"], name=\"session-name\")</code>. As long as one of the <code>pyenv</code> scopes (<code>shell</code>, <code>local</code>, or <code>global</code>) has all of these versions of Python, <code>nox</code> will run the session multiple times, once for each version declared in <code>python=[]</code>.</p> <p>Note</p> <p>For more information on the problem <code>pyenv</code> solves, read the \"Why use pyenv?\" section below.</p> <p>If you just want to see installation &amp; setup instructions, you can skip to the \"install pyenv\" section, or to \"using pyenv\" to see notes on using the tool.</p> <p>For more detailed (and likely more up-to-date) documentation on <code>pyenv</code> installation &amp; usage, check the <code>pyenv gihub's README</code>.</p>","tags":["python","pyenv","environment"]},{"location":"programming/python/pyenv.html#why-use-pyenv","title":"Why use pyenv?","text":"<p>With <code>pyenv</code>, you can separate your user Python installation from the system Python. This is generally a good practice, specifically on Linux machines; the version of Python included in some distributions is very old, and you are not meant to install packages with <code>pip</code> using the version of Python that came installed on your machine (especially without at least using a <code>.venv</code>).</p> <p>The system Python is there for system packages you install that have some form of Python scripting included. Packages built for specific Linux distributions, like Debian, Ubuntu, Fedora, OpenSuSE, etc, are able to target the system version of Python, ensuring a stable and predictable installation. When you add <code>pip</code> packages to your system's Python environment, it not only adds complexity for other packages on your system to work around, you also increase your chance of landing yourself in dependency hell, where 2 <code>pip</code> packages require the same package but different versions of that package.</p> <p>For more reading on why it's a good idea to install a separate version of Python, and leave the version that came installed with your machine untouched, check this RealPython article.</p> <p>You also have the option of installing Python yourself, either compiling it from source or downloading, building, and installing a package from python.org. This option is perfectly fine, but can be difficult for beginners, and involves more steps and room for error when trying to automate.</p>","tags":["python","pyenv","environment"]},{"location":"programming/python/pyenv.html#pyenv-scopes","title":"pyenv scopes","text":"<p>\u2934\ufe0f back to <code>TL;DR</code></p> <p><code>pyenv</code> has 3 \"scopes\":</p> <ul> <li><code>shell</code><ul> <li>Setting a version with <code>pyenv shell x.xx.xx</code> sets the Python interpreter for the current shell</li> <li>When that shell is exited or refreshed (i.e. with <code>exec $SHELL</code> or <code>source ~/.bashrc</code>), this value is also reset</li> </ul> </li> <li><code>local</code><ul> <li>Setting a version with <code>pyenv local x.xx.xx</code> creates a file called <code>.python-version</code> at the current path</li> <li><code>pyenv</code> uses this file to automatically set the version of Python while in this directory</li> </ul> </li> <li><code>global</code><ul> <li>Setting a version with <code>pyenv global x.xx.xx</code> sets the default, global Python version</li> <li>Any Python command (like <code>python -m pip install ...</code>) will use this <code>global</code> version, if no <code>local</code> or <code>shell</code> version is specified</li> <li><code>local</code> and <code>shell</code> override this value</li> <li>You generally don't need to change this value often. You are providing a version of Python you've installed with <code>pyenv</code>, in the event you have not set a <code>pyenv local</code> or <code>pyenv shell</code> version of Python.</li> </ul> </li> </ul> <p>The <code>global</code> scope is essentially the \"default\" scope. When no other version is specified, the <code>global</code> Python version will be used.</p>","tags":["python","pyenv","environment"]},{"location":"programming/python/pyenv.html#scope-precedence","title":"Scope precedence","text":"<p><code>pyenv shell</code> overrides &gt; <code>pyenv local</code> overrides &gt; <code>pyenv global</code></p> <p>Warning</p> <p>Make sure to pay attention to your current <code>pyenv</code> version. If you are getting unexpected results when running Python scripts, check the version with <code>python3 --version</code>. This will help if you are expecting <code>3.11.4</code> to be the version of Python for your session, for example, but have set <code>pyenv shell 3.12.1</code>. Because <code>shell</code> overrides <code>local</code>, the Python version in <code>.python-version</code> will be ignored until the session is exited.</p>","tags":["python","pyenv","environment"]},{"location":"programming/python/pyenv.html#install-pyenv","title":"Install pyenv","text":"<p>Installing <code>pyenv</code> varies between OSes. On Windows, you use the <code>pyenv-win</code> package, for example. Below are installation instructions for Windows and Linux.</p>","tags":["python","pyenv","environment"]},{"location":"programming/python/pyenv.html#install-pyenv-in-linuxwsl","title":"Install pyenv in Linux/WSL","text":"<p>\u2934\ufe0f back to <code>TL;DR</code></p> <ul> <li>Install dependencies</li> </ul> install pyenv dependencies (Debian/Ubuntu)<pre><code>sudo apt-get install -y \\\n  git \\\n  gcc \\\n  make \\\n  openssl \\\n  libssl-dev \\\n  libbz2-dev \\\n  libreadline-dev \\\n  libsqlite3-dev \\\n  zlib1g-dev \\\n  libncursesw5-dev \\\n  libgdbm-dev \\\n  libc6-dev \\\n  zlib1g-dev \\\n  libsqlite3-dev \\\n  tk-dev \\\n  libssl-dev \\\n  openssl \\\n  libffi-dev\n</code></pre> install pyenv dependencies (RedHat/Fedora)<pre><code>sudo dnf install -y \\\n  git \\\n  gcc \\\n  make \\\n  openssl \\\n  openssl-devel \\\n  bzip2-devel \\\n  zlib-devel \\\n  readline-devel \\\n  soci-sqlite3-devel \\\n  ncurses-devel \\\n  gdbm \\\n  glibc-devel \\\n  tk-devel \\\n  libffi-devel\n</code></pre> <ul> <li>Install <code>pyenv</code> with the convenience script</li> </ul> install pyenv with convenience script<pre><code>curl https://pyenv.run | bash\n</code></pre> <ul> <li>Add <code>pyenv</code> variables to your <code>~/.bashrc</code></li> </ul> ~/.bashrc<pre><code>...\n\n## Pyenv\nexport PATH=\"$HOME/.pyenv/bin:$PATH\"\neval \"$(pyenv init -)\"\neval \"$(pyenv virtualenv-init -)\"\n</code></pre>","tags":["python","pyenv","environment"]},{"location":"programming/python/pyenv.html#install-pyenv-in-windows","title":"Install pyenv in Windows","text":"<p>\u2934\ufe0f back to <code>TL;DR</code></p> <ul> <li>Install <code>pyenv-win</code> with Powershell</li> </ul> install pyenv with Powershell convenience script<pre><code>Invoke-WebRequest -UseBasicParsing -Uri \"https://raw.githubusercontent.com/pyenv-win/pyenv-win/master/pyenv-win/install-pyenv-win.ps1\" -OutFile \"./install-pyenv-win.ps1\"; &amp;\"./install-pyenv-win.ps1\"\n</code></pre> <ul> <li>Add the following variables to your <code>PATH</code></li> </ul> pyenv Windows PATH variables<pre><code>%USERPROFILE%\\\\.pyenv\\\\pyenv-win\\\\bin\n%USERPROFILE%\\\\.pyenv\\\\pyenv-win\\\\shims\n</code></pre> <ul> <li>You can do this by opening the Control Panel &gt; User Accounts and clicking \"Change my environment variables\":</li> </ul> Set pyenv-win PATH variables on Windows Set pyenv-win PATH variables on Windows pt. 2 <ul> <li>NOTE: Setting these env variables can also be accomplished with this Powershell script:</li> </ul> set pyenv-win PATH variables<pre><code>[CmdletBinding()]\nparam (\n    [Parameter(Mandatory = $true)]\n    [string] $PythonVersion\n)\n\n$ErrorActionPreference = \"Stop\"\n$ProgressPreference = \"SilentlyContinue\"\nSet-StrictMode -Version Latest\n\nfunction _runCommand {\n    [CmdletBinding()]\n    param (\n        [Parameter(Mandatory = $true, Position = 0)]\n        [string] $Command,\n        [switch] $PassThru\n    )\n\n    try {\n        if ( $PassThru ) {\n            $res = Invoke-Expression $Command\n        }\n        else {\n            Invoke-Expression $Command\n        }\n\n        if ( $LASTEXITCODE -ne 0 ) {\n            $errorMessage = \"'$Command' reported a non-zero status code [$LASTEXITCODE].\"\n            if ($PassThru) {\n                $errorMessage += \"`nOutput:`n$res\"\n            }\n            throw $errorMessage\n        }\n\n        if ( $PassThru ) {\n            return $res\n        }\n    }\n    catch {\n        $PSCmdlet.WriteError( $_ )\n    }\n}\n\nfunction _addToUserPath {\n    [CmdletBinding()]\n    param (\n        [Parameter(Mandatory = $true, Position = 0)]\n        [string] $AppName,\n        [Parameter(Mandatory = $true, Position = 1)]\n        [string[]] $PathsToAdd\n    )\n\n    $pathEntries = [System.Environment]::GetEnvironmentVariable(\"PATH\", [System.EnvironmentVariableTarget]::User) -split \";\"\n\n    $pathUpdated = $false\n    foreach ( $pathToAdd in $PathsToAdd ) {\n        if ( $pathToAdd -NotIn $pathEntries ) {\n            $pathEntries += $pathToAdd\n            $pathUpdated = $true\n        }\n    }\n    if ( $pathUpdated ) {\n        Write-Host \"$($AppName): Updating %PATH%...\" -f Green\n        # Remove any duplicate or blank entries\n        $cleanPaths = $pathEntries | Select-Object -Unique | Where-Object { -Not [string]::IsNullOrEmpty($_) }\n\n        # Update the user-scoped PATH environment variable\n        [System.Environment]::SetEnvironmentVariable(\"PATH\", ($cleanPaths -join \";\").TrimEnd(\";\"), [System.EnvironmentVariableTarget]::User)\n\n        # Reload PATH in the current session, so we don't need to restart the console\n        $env:PATH = [System.Environment]::GetEnvironmentVariable(\"PATH\", [System.EnvironmentVariableTarget]::User)\n    }\n    else {\n        Write-Host \"$($AppName): PATH already setup.\" -f Cyan\n    }\n}\n\n# Install pyenv\nif ( -Not ( Test-Path $HOME/.pyenv ) ) {\n    if ( $IsWindows ) {\n        Write-Host \"pyenv: Installing for Windows...\" -f Green\n        &amp; git clone https://github.com/pyenv-win/pyenv-win.git $HOME/.pyenv\n        if ($LASTEXITCODE -ne 0) {\n            Write-Error \"git reported a non-zero status code [$LASTEXITCODE] - check previous output.\"\n        }\n    }\n    else {\n        Write-Error \"This script currently only supports Windows.\"\n    }\n}\nelse {\n    Write-Host \"pyenv: Already installed.\" -f Cyan\n}\n\n# Add pyenv to PATH\n_addToUserPath \"pyenv\" @(\n    \"$HOME\\.pyenv\\pyenv-win\\bin\"\n    \"$HOME\\.pyenv\\pyenv-win\\shims\"\n)\n\n# Install default pyenv python version\n$pyenvVersions = _runCommand \"pyenv versions\" -PassThru | Select-String $PythonVersion\nif ( -Not ( $pyenvVersions ) ) {\n    Write-Host \"pyenv: Installing python version $PythonVersion...\" -f Green\n    _runCommand \"pyenv install $PythonVersion\"\n}\nelse {\n    Write-Host \"pyenv: Python version $PythonVersion already installed.\" -f Cyan\n}\n\n# Set pyenv global version\n$globalPythonVersion = _runCommand \"pyenv global\" -PassThru\nif ( $globalPythonVersion -ne $PythonVersion ) {\n    Write-Host \"pyenv: Setting global python version: $PythonVersion\" -f Green\n    _runCommand \"pyenv global $PythonVersion\"\n}\nelse {\n    Write-Host \"pyenv: Global python version already set: $globalPythonVersion\" -f Cyan\n}\n\n# Update pip\n_runCommand \"python -m pip install --upgrade pip\"\n\n# Optional, install pipx, pdm, black, cookiecutter\n# _runCommand \"pip install pipx\"\n# _runCommand \"pip install pdm\"\n# _runCommand \"pip install black\"\n# _runCommand \"pip install cookiecutter\"\n</code></pre> <p>\u2934\ufe0f back to <code>TL;DR</code></p>","tags":["python","pyenv","environment"]},{"location":"programming/python/pyenv.html#using-pyenv","title":"Using pyenv","text":"<p>The sections below will detail how to install a version (or multiple versions!) of Python using <code>pyenv</code>, switching between them, and configuring your shell to make multiple versions of Python available to tools like <code>nox</code> and <code>pytest</code>.</p> <p>Note</p> <p>To see a list of the commands you can run, execute <code>$ pyenv</code> without any commands.</p>","tags":["python","pyenv","environment"]},{"location":"programming/python/pyenv.html#update-pyenv","title":"Update pyenv","text":"<p>Updating <code>pyenv</code> is as simple as typing <code>$ pyenv update</code> in your terminal. The <code>pyenv update</code> command will update <code>pyenv</code> itself, as well as the listing of available Python distributions.</p>","tags":["python","pyenv","environment"]},{"location":"programming/python/pyenv.html#list-all-available-python-versions","title":"List all available Python versions","text":"<p>You can ask <code>pyenv</code> to show you a list of all the versions of Python available for you to install with the tool by running: <code>$ pyenv install -l</code> (or <code>$ pyenv install --list</code>). You will see a long list of version numbers, which you can install with <code>pyenv</code>.</p> <p>Note</p> <p>Some releases will indicate a specific CPU architecture, like <code>-win32</code>, <code>-arm64</code>, etc. Make sure you're installing the correct version for your CPU type!</p> <p>To be safe, you can simply use a Python version string, omitting any CPU specification, like <code>3.12.1</code> instead of <code>3.12.1-arm</code>.</p>","tags":["python","pyenv","environment"]},{"location":"programming/python/pyenv.html#install-python-versions-with-pyenv","title":"Install Python version(s) with pyenv","text":"<p>Once you have decided on a version of Python to install (we will use <code>3.12.1</code>, a recent release as of 2/14/2024), install it with: <code>$ pyenv install 3.12.1</code> (or whatever version you want to install).</p> <p>You can see which versions of Python are available to <code>pyenv</code> by running <code>$ pyenv versions</code>. The list will grow as you install more versions of Python with <code>$ pyenv install x.x.x</code>.</p>","tags":["python","pyenv","environment"]},{"location":"programming/python/pyenv.html#set-global-local-shell-python-with-pyenv","title":"Set global, local, &amp; shell Python with pyenv","text":"<p>Note</p> <p>To learn more about the <code>global</code>, <code>local</code>, and <code>shell</code> scopes, check the <code>pyenv scopes</code> section.</p> set python version(s) per-scope<pre><code>## Set global/default Python version to 3.12.1\n$ pyenv global 3.12.1\n\n## Make multiple versions available to tools like pytest, nox, etc\n$ pyenv global 3.11.8 3.11.6 3.12.1\n\n## Create a file `.python-version` in the local directory.\n#  Python commands called from this directory will use\n#  the version(s) specified this way\n$ pyenv local 3.12.1\n\n## Multiple versions in `.python-version`\n$ pyenv local 3.11.8 3.12.1\n\n## Set python version(s) for current shell session.\n#  This value is cleared on logout, exit, or shutdown/restart\n$ pyenv shell 3.12.1\n\n# Set newer version first to prioritize it, make older version available\n$ pyenv shell 3.12.1 3.11.8\n</code></pre>","tags":["python","pyenv","environment"]},{"location":"programming/python/pyenv.html#uninstall-a-version-of-python-installed-with-pyenv","title":"Uninstall a version of Python installed with pyenv","text":"<p>To uninstall a version of Python that you installed with <code>pyenv</code>, use <code>$ pyenv uninstall x.x.x</code>. For example, to uninstall version <code>3.12.1</code>: <code>pyenv uninstall 3.12.1</code>.</p>","tags":["python","pyenv","environment"]},{"location":"programming/python/rich.html","title":"Using rich to enhance your console output","text":"<p>The <code>rich</code> package helps make console/terminal output look nicer. It has colorization, animations, and more.</p>","tags":["python","rich"]},{"location":"programming/python/rich.html#details","title":"Details","text":"<ul> <li><code>rich</code> Github repository</li> <li><code>rich</code> docs</li> <li><code>rich</code> pypi</li> </ul>","tags":["python","rich"]},{"location":"programming/python/rich.html#examples","title":"Examples","text":"","tags":["python","rich"]},{"location":"programming/python/rich.html#simple-cli-spinner-context-manager","title":"Simple CLI spinner context manager","text":"<p>This example uses the <code>rich.console.Console</code> and <code>rich.spinner.Spinner</code> classes. The first context manager, <code>get_console()</code>, yields a <code>rich.Console()</code> object, which can be used with the <code>rich.Spinner()</code> class to display a spinner on the command line.</p> get_console() function<pre><code>from contextlib import contextmanager\nimport typing as t\nfrom rich.console import Console\n\n@contextmanager\ndef get_console() -&gt; t.Generator[Console, t.Any, None]:\n    \"\"\"Yield a `rich.Console`.\n\n    Usage:\n        `with get_console() as console:`\n\n    \"\"\"\n    try:\n        console: Console = Console()\n\n        yield console\n\n    except Exception as exc:\n        msg = Exception(f\"Unhandled exception getting rich Console. Details: {exc}\")\n        log.error(msg)\n\n        raise exc\n</code></pre> <p>The <code>simple_spinner()</code> context manager yields a <code>rich.Spinner()</code> instance. Wrap a function in <code>with simple_spinner(msg=\"Some message\") as spinner:</code> to show a spinner while the function executes.</p> simple_spinner()<pre><code>from contextlib import contextmanager\nfrom rich.spinner import Spinner\n\n@contextmanager\ndef simple_spinner(text: str = \"Processing... \\n\", animation: str = \"dots\"):\n    if not text:\n        text: str = \"Processing... \\n\"\n    assert isinstance(text, str), TypeError(\n        f\"Expected spinner text to be a string. Got type: ({type(text)})\"\n    )\n\n    if not text.endswith(\"\\n\"):\n        text += \" \\n\"\n\n    if not animation:\n        animation: str = \"dots\"\n    assert isinstance(animation, str), TypeError(\n        f\"Expected spinner animation to be a string. Got type: ({type(text)})\"\n    )\n\n    try:\n        _spinner = Spinner(animation, text=text)\n    except Exception as exc:\n        msg = Exception(f\"Unhandled exception getting console spinner. Details: {exc}\")\n        log.error(msg)\n\n        raise exc\n\n    ## Display spinner\n    try:\n        with get_console() as console:\n            with console.status(text, spinner=animation):\n                yield console\n    except Exception as exc:\n        msg = Exception(\n            f\"Unhandled exception yielding spinner. Continuing without animation. Details: {exc}\"\n        )\n        log.error(msg)\n\n        pass\n</code></pre> <p>Putting both of these functions in the same file allows you to import just the <code>simple_spinner</code> method, which calls <code>get_console()</code> for you. You can also get a console using <code>with get_console() as console:</code>, and write custom spinner/<code>rich</code> logic.</p>","tags":["python","rich"]},{"location":"programming/python/rich.html#usage","title":"Usage","text":"Example CLI spinner<pre><code>from some_modules import simple_spinner\n\nwith simple_spinner(text=\"Thinking... \"):\n    ## Some long-running code/function\n    ...\n</code></pre>","tags":["python","rich"]},{"location":"programming/python/virtualenv.html","title":"Use virtualenv to manage dependencies","text":"<p>Toc</p> <p>Jump right to a section:</p> <ul> <li>Virtualenv commands cheat-sheet</li> <li>What is virtualenv?<ul> <li>What problem does virtualenv solve?</li> </ul> </li> <li>Importing/exporting virtualenv pip requirements</li> <li>Common virtualenv troubleshooting</li> <li>Alternatives to virtualenv</li> </ul>","tags":["python","virtualenv"]},{"location":"programming/python/virtualenv.html#virtualenv-cheat-sheet","title":"virtualenv cheat sheet","text":"Command Description Notes <code>virtualenv .venv</code> Create a new virtual environment This command creates a new directory called <code>.venv/</code> at the path where you ran the command. (Windows) <code>.\\.venv\\Scripts\\activate</code> Activate a virtual environment on Windows. Your shell should change to show <code>(.venv)</code>, indicating you are in a virtual environment (Linux/Mac) <code>./venv/bin/activate</code> Activate a virtual environment on Linux. Your shell should change to show <code>(.venv)</code>, indicating you are in a virtual environment <code>deactivate</code> Exit/deactivate a virtual environment. You can also simply close your shell session, which exists the environment. This command only works once a virtual environment is activated; <code>deactivate</code> will give an error saying the command is not found if you do not have an active virtual environment.","tags":["python","virtualenv"]},{"location":"programming/python/virtualenv.html#what-is-virtualenv","title":"What is virtualenv?","text":"<p>RealPython: Virtualenv Primer</p> <p><code>virtualenv</code> is a tool for creating virtual Python environments. The <code>virtualenv</code> tool is sometimes installed with the \"system\" Python, but you may need to install it yourself (see the warning below).</p> <p>These virtual environments are stored in a directory, usually <code>./.venv</code>, and can be \"activated\" when you are developing a Python project. Virtual environments can also be used as Jupyter kernels if you install the <code>ipykernel</code> package.</p> <p>Warning</p> <p>If you ever see see an error saying the <code>virtualenv</code> command cannot be found (or something along those lines), simply install <code>virtualenv</code> with:</p> Install virtaulenv<pre><code>$ pip install virtualenv\n</code></pre>","tags":["python","virtualenv"]},{"location":"programming/python/virtualenv.html#what-problem-does-virtualenv-solve","title":"What problem does virtualenv solve?","text":"<p><code>virtualenv</code> helps developers avoid \"dependency hell\". Without <code>virtualenv</code>, when you insall a package with <code>pip</code>, it is installed \"globally.\" This becomes an issue when 2 different Python projects use the same dependency, but different versions of that dependency. This leads to a \"broken environment,\" where the only fix is to essentially uninstall all dependencies and start from scratch.</p> <p>With <code>virtualenv</code>, you start each project by running <code>$ virtualenv .venv</code>, which will create a directory called <code>.venv/</code> at the path where you ran the command (i.e. inside of a Python project directory).</p> <p>Note</p> <p><code>.venv/</code> is the convention, but you can name this directory whatever you want. If you run <code>virtualenv</code> using a different path, like <code>virtualenv VirtualEnvironment</code> (which would create a directory called <code>VirtualEnvironment/</code> at the local path), make sure you use that name throughout this guide where you see <code>.venv</code>.</p> <p>Once a virtual environment is created, you need to \"activate\" it. Activating a virtual environment will isolate your current shell/session from the global Python, allowing you to install dependencies specific to the current Python project without interfering with the global Python state.</p> <p>Activating a virtual environment is as simple as:</p> <ul> <li>(on Windows): <code>$ ./.venv/Scripts/activate</code></li> <li>(on Linux): <code>$ ./.venv/bin/activate</code></li> </ul> <p>After activating an environment, your shell will change to indicate that you are within a virtual environment. Example:</p> activating virtualenv<pre><code>## Before .venv activation, using the global Python. Depdendency will\n#  be installed to the global Python environment\n$ pip install pandas\n\n## Activate the virtualenv\n$ ./.venv/Scripts/activate\n\n## The shell will change to indicate you're in a venv. The \"(.venv)\" below\n#  indicates a virtual environment has been activated. The pip install command\n#  installs the dependency within the virtual environment\n(.venv) $ pip install pandas\n</code></pre> <p>This method of \"dependency isolation\" ensures a clean environment for each Python project you start, and keeps the \"system\"/global Python version clean of dependency errors.</p>","tags":["python","virtualenv"]},{"location":"programming/python/virtualenv.html#exportingimporting-requirements","title":"Exporting/importing requirements","text":"<p>Once a virtual environment is activated, commands like <code>pip</code> will run much the same as they do without a virtual environment, but the outputs will be contained to the <code>.venv</code> directory in the project you ran the commands from.</p> <p>To export <code>pip</code> dependencies:</p> Export pip requirements<pre><code>## Make sure you've activated your virtual environment first\n$ .\\\\venv\\\\Scripts\\\\activate  # Windows\n$ ./venv/bin/activate  # Linux/Mac\n\n## Export pip requirements\n(.venv) $ pip freeze &gt; requirements.txt\n</code></pre> <p>To import/install <code>pip</code> dependencies from an existing <code>requirements.txt</code> file:</p> Import pip requirements<pre><code>## Make sure you've activated your virtual environment first\n$ .\\\\venv\\\\Scripts\\\\activate  # Windows\n$ ./venv/bin/activate  # Linux/Mac\n\n## Install dependencies from a requirements.txt file\n(.venv) $ pip install -r requirements.txt\n</code></pre>","tags":["python","virtualenv"]},{"location":"programming/python/virtualenv.html#common-virtualenv-troubleshooting","title":"Common virtualenv troubleshooting","text":"","tags":["python","virtualenv"]},{"location":"programming/python/virtualenv.html#recreate-venv","title":"Recreate .venv","text":"<p>Sometimes you find yourself with a <code>.venv</code> that's in disrepair, and you might want to start from scratch and recreate it. This is as simple as removing the <code>.venv</code> directory with <code>rm -r .venv</code> (you may get warnings about write-protected files, in which case you will need to use <code>sudo</code> to remove the <code>.venv</code>).</p> <p>After removing the <code>.venv</code>, you can recreate it with <code>virtualenv .venv</code>. Make sure to activate the new virtualenv and reinstall your dependencies with <code>pip install -r requirements.txt</code></p>","tags":["python","virtualenv"]},{"location":"programming/python/virtualenv.html#fix-virtual-environment-not-showing-in-jupyter-notebooks","title":"Fix virtual environment not showing in Jupyter notebooks","text":"<p>This is almost always because your virtualenv is missing the <code>ipykernel</code> package. This package is required for a virtualenv to be detectable by Jupyter notebooks.</p> <p>Simply add the package: <code>pip install ipykernel</code>.</p>","tags":["python","virtualenv"]},{"location":"programming/python/virtualenv.html#alternatives-to-virtualenv","title":"Alternatives to virtualenv","text":"<p>There are other ways to manage a virtual environment for Python. Most people start out using <code>virtualenv</code>, and some people never find a need to replace <code>virtualenv</code> with another tool.</p> <p>If you want to separate your development dependencies from production, or \"manage\" your Python project including building &amp; publishing your code, running scripts/tasks, and resolving dependencies more quickly/reliably than with <code>pip</code>, you can use a Python project manager.</p> <p>Python's package management ecosystem is much older than other languages like Node's <code>npm</code>, and as a result package management came later and was not \"figured out\" by the time Python developed <code>pip</code>. The <code>pip</code> utility has served a very important role for many years, and there are those who would argue you never need anything more than <code>pip + virtualenv</code>. It is true that you can be just as effective with <code>pip + virtualenv</code> as any of the project managers below, but there are some conveniences to using a project manager, such as installing non-Python dependencies with <code>conda</code> for scientific/ML/AI development.</p> <p>This section will not go into much detail on each package manager. Other sections of this KB site may expand more on how to use each of these. This page serves to simply list alternatives to <code>virtualenv</code>.</p> <p>Alternatives to <code>pip/virtualenv</code>:</p> tool description <code>pdm</code> The \"Python Dependency Manager\" is one of the best all-around tools on this list. It can handle complex dependency resolution (i.e. <code>pytorch</code> installation), project scripts (something <code>poetry</code> needs a plugin for, and that is completely missing from <code>virtualenv</code>), and can build &amp; publish your code to <code>pypi</code>. <code>uv</code> A new(ew) Python project manage, <code>uv</code> is built with Rust and boasts the fastest dependency resolution of any package manager I've tried. What might take 30 seconds to install with <code>pdm</code> (a perfectly acceptable dependency resolution/installation time) may take only 2-3 seconds in <code>uv</code>. Support for monorepos, building the project, scripts (that function differently from <code>pdm</code>, but are still useful), installing Python (you don't even need Python installed to run <code>uv</code>!), <code>venv</code> management, and more. I am slowly converting most/all of my projects to <code>uv</code>, and using it for new projects. It's great! <code>poetry</code> A favorite of many in the Python community, the <code>poetry</code> tool is another fast, capable project manager. The project does not follow PEP standards, which puts it at odds with some other tools. For example, <code>poetry</code> makes heavy modifications to the standard <code>pyproject.toml</code> file, and has not expressed interest in following a number of PEPs, which could lead to complicated divergence over time. Scripting is not a first-class feature of <code>poetry</code> either, requiring a plugin called <code>poe-the-poet</code>. <code>conda</code> The package manager for the Anaconda Python distribution, aimed at scientific Python and machine learning/AI development. Conda filled a desperately needed role in the mid-2010s, when it was very difficult to install packages required for machine learning development. The tool continues to see use, but if you are interested in using <code>conda</code> for development, you would be better served using something like <code>mamba</code>/<code>micromamba</code>, or the newer <code>pixi</code> tool that uses Conda's repositories but is written in Rust and has many more convenient features. <code>mamba</code>/<code>micromamba</code> <code>mamba</code> is a resolver for <code>conda</code> packages that significantly speeds up environment resolution time. <code>conda</code> is great, but is very, very slow. The <code>mamba</code> solver makes <code>conda</code> far more useable. <code>micromamba</code> is a statically linked, self-contained tool. It supports all of the major features of <code>mamba</code>, but instead of installing a whole package, you simply place the <code>micromamba</code> binary somewhere in your <code>PATH</code>. <code>micromamba</code> is faster than <code>conda</code> and <code>mamba</code>, and will serve most use cases. <code>pixi</code> A newer project manager, <code>pixi</code> uses the <code>conda</code> package sources (meaning any package you can install with <code>conda</code>/<code>mamba</code> can be installed with <code>pixi</code>). The tool is written in Rust and is extremely fast. It also has a number of very useful features that put it more in line with <code>pdm</code> or <code>uv</code> than <code>conda</code>. If you are interested in using <code>conda</code> for your development, I highly recommend trying <code>pixi</code> over the other options.","tags":["python","virtualenv"]},{"location":"programming/python/Argument%20Parsing/index.html","title":"Argument Parsing","text":"<p>...</p>","tags":["python","programming","args","cli"]},{"location":"programming/python/Argument%20Parsing/argparse/index.html","title":"Parse arguments with argparse","text":"","tags":["python","python-stdlib","programming","cli","args"]},{"location":"programming/python/Argument%20Parsing/cyclopts/index.html","title":"Parse arguments with Cyclopts","text":"","tags":["python","programming","args","cli","utilities","cyclopts"]},{"location":"programming/python/Argument%20Parsing/cyclopts/index.html#overview","title":"Overview","text":"<p><code>cyclopts</code> is a Python library that helps with building CLI/TUI applications. It is inspired by and comparable to libraries like <code>click</code>, <code>fire</code>, and <code>Typer</code>, and in fact directly compares itself to <code>Typer</code> in a number of instances.</p> <p><code>cyclopts</code> helps to reduce boilerplate code, and allows for flexible structuring of your code in files. It makes adding commands and <code>--args</code> simple, and can handle some fairly complex configurations, or very simple argument parsing.</p> <p>Use the sections on the left to read more about using <code>cyclopts</code> to build CLIs for your apps.</p>","tags":["python","programming","args","cli","utilities","cyclopts"]},{"location":"programming/python/Argument%20Parsing/cyclopts/building_a_cli.html","title":"Building a CLI app with cyclopts","text":"<p>Note</p> <p>This guide uses <code>my_pymodule</code> as the name of the example package/app that a <code>cyclopts</code> CLI will be added to. Wherever you see <code>my_pymodule</code> in this guide, replace with the name of the Python app you are adding a CLI to.</p> <p>As with every other guide on this site, I am writing about the way I personally use <code>cyclopts</code>. It may not suit your needs, but I hope it helps you learn the concepts.</p> <p>When I'm adding a CLI to one of my apps, I like to create a directory inside my app's source named <code>cli/</code>, and a file beneath that <code>main.py</code>, which represents my CLI's entrypoint. In my app's <code>__main__.py</code>, I call the <code>cyclopts</code> app, parsing arguments when the app is called.</p> <p>For example, if I'm working on a Python app named <code>my_pymodule</code>, I would create the following file structure:</p> Example CLI app location in a Python app<pre><code>pyproject.toml  # Project root with a pyproject.toml or requirements.txt file\nsrc/  # Code exists in a src directory\n  my_pymodule/\n    cli/  # The cyclopts app\n      __init__.py\n      main.py  # The CLI's entrypoint\n      commands.py  # Import commands from a separate file for easier development\n    __init__.py\n    __main__.py  # Import the cli app here\n    main.py  # Regular app entrypoint\n</code></pre> <p>In my <code>src/my_pymodule/cli/main.py</code>, I declare a <code>cyclopts</code> app and add a command to accept a user's name and print \"hello, {name}\"</p> Create cyclopts app<pre><code>from cyclopts import App\n\n## Initialize the app. Give the app a name, and a help message for --help\napp = App(name=\"demo\", help=\"Cyclopts demo app.\")\n\n\n@app.command(name=\"hello\")\ndef say_hello(name: str = \"world\"):\n    \"\"\"Say hello to a user.\n\n    Description:\n        Says hello to a user, or 'world' if no user is given. This docstring becomes the command's help messsage!\n    Params:\n        name (str): Name of user to say hello to.\n\n    \"\"\"\n    print(f\"Hello, {name}!\")\n\n\nif __name__ == \"__main__\":\n    ## When this script is called directly, run the app\n    app()\n</code></pre> <p>This file become's the main entrypoint to the <code>cyclopts</code> app. As you add commands and functionality to the CLI app, you will import code from other modules and \"mount\" them in the main app.</p>","tags":["python","utilities","cyclopts"]},{"location":"programming/python/Argument%20Parsing/cyclopts/building_a_cli.html#cyclopts-meta-app","title":"Cyclopts Meta app","text":"<p>Running the <code>cyclopts</code> app with <code>app()</code> calls the app directly, but you can also call a <code>cyltops</code> \"Meta app\". Calling the app this way gives you more control over <code>cyclopts</code>'s launch process. This allows you to do things like setup logging (or add a <code>--debug</code> flag that sets logging level to <code>DEBUG</code> when present), run functions before startup, and control tokens (inputs) that are passed to the app.</p> <p>The setup for using a meta app is simple; after adding the code below, anywhere you would normally call <code>app()</code>, call <code>app.meta()</code> instead. The code imports <code>Parameter</code> from <code>cyclopts</code>, and the stdlib <code>typing</code> library for accessing the <code>Annotated</code> class. The new meta app is created in <code>@app.meta.default</code>, and the function <code>cli_app_launcher</code> now calls the app. You can name this function anything you want.</p> <p>When adding function parameters to <code>cli_app_launcher</code>, they become CLI args.</p> cyclopts meta app<pre><code>from cyclopts import App, Parameter\nimport typing as t\n\n## Initialize the app. Give the app a name, and a help message for --help\napp = App(name=\"demo\", help=\"Cyclopts demo app.\")\n## Allow global flags like --debug\napp.meta.group_parameters = Group(\"Session Parameters\", sort_key=0)\n\n\n@app.command(name=\"hello\")\ndef say_hello(name: str = \"world\"):\n    \"\"\"Say hello to a user.\n\n    Description:\n        Says hello to a user, or 'world' if no user is given. This docstring becomes the command's help messsage!\n    Params:\n        name (str): Name of user to say hello to.\n\n    \"\"\"\n    print(f\"Hello, {name}!\")\n\n\n## Add a meta launcher to the app. Call with app.meta()\n@app.meta.default\ndef cli_app_launcher(\n    *tokens: t.Annotated[str, Parameter(show=False, allow_leading_hyphen=True)],\n):\n    ## Uncomment to print tokens for debugging\n    # print(f\"Tokens: {tokens}\")\n\n    ## Call the app, passing tokens as arguments\n    app(tokens)\n\n\nif __name__ == \"__main__\":\n    ## Call the app's meta launcher\n    app.meta()\n</code></pre>","tags":["python","utilities","cyclopts"]},{"location":"programming/python/Argument%20Parsing/cyclopts/building_a_cli.html#setup-logging-with-meta-app","title":"Setup logging with meta app","text":"<p>Launching the <code>cyclopts</code> app using a \"meta\" launcher provides control over the CLI app's startup. As an example, the <code>cli_app_launcher</code> command below configures logging based on the presence of a <code>-d/--debug</code> flag. If present, the logging level for the <code>loguru</code> library is set to <code>DEBUG</code>.</p> cyclopts meta logging config<pre><code>from cyclopts import App, Parameter\nfrom loguru import logger as log\nimport typing as t\nimport sys\n\napp = App(name=\"demo\", help=\"Cyclopts demo app.\")\napp.meta.group_parameters = Group(\"Session Parameters\", sort_key=0)\n\n\n@app.meta.default\ndef cli_launcher(\n    *tokens: t.Annotated[str, Parameter(show=False, allow_leading_hyphen=True)],\n    debug: t.Annotated[\n        bool,\n        Parameter(\n            name=[\"-d\", \"--debug\"], show_default=True, help=\"Enable debug logging.\"\n        ),\n    ] = False,\n    file_log: t.Annotated[\n        bool,\n        Parameter(\n            name=[\"-l\", \"--log-file\"], show_default=True, help=\"Enable logging to file\"\n        ),\n    ] = False,\n):\n    ## Initialize Loguru logger\n    log.remove(0)\n\n    ## If --debug flag was passed, set logging to `DEBUG` and more verbose log format\n    if debug:\n        log.add(\n            sys.stderr,\n            format=\"{time:YYYY-MM-DD HH:mm:ss} | [{level}] | {name}.{function}:{line} | &gt; {message}\",\n            level=\"DEBUG\",\n        )\n\n    ## If --debug flag not passed, set logging to `INFO` and a shorter log message format\n    else:\n\n        log.add(\n            sys.stderr,\n            format=\"{time:YYYY-MM-DD HH:mm:ss} [{level}] : {message}\",\n            level=\"INFO\",\n        )\n\n    ## Call the cyclopts app with user's inputs\n    app(tokens)\n</code></pre>","tags":["python","utilities","cyclopts"]},{"location":"programming/python/Argument%20Parsing/cyclopts/building_a_cli.html#adding-commands-and-sub-apps","title":"Adding commands and sub-apps","text":"<p>So far, we have added a <code>say_hello()</code> function to the app. When <code>my_pymodule hello &lt;name&gt;</code> is called, the CLI will say hello to the user. The <code>hello</code> arg here is a \"command.\"</p> <p>You can make your CLI modular by putting commands into other <code>.py</code> files. You can also create multiple <code>cyclopts.App</code> instances, and mount them in the main app. This allows for creating complex but maintainable commands.</p>","tags":["python","utilities","cyclopts"]},{"location":"programming/python/Argument%20Parsing/cyclopts/building_a_cli.html#create-sub-appcommand","title":"Create sub-app/command","text":"<p>To start, let's add some more \"top level\" commands like the <code>hello</code> command. Let's create one that adds 2 numbers together. We will put this code into <code>cli/commands.py</code>, and import into <code>cli/main.py</code>. We will create a new <code>cyclopts</code> app for this called <code>math</code>, and create a command beneath it for adding numbers. We will import this sub-app into the <code>cli/main.py</code> file and mount it in our <code>cyclopts</code> app.</p> <p>I will also add <code>DEBUG</code> logging messages so you can see how the <code>--debug</code> param works.</p> cli/commands.py<pre><code>from cyclopts import App, Parameter\nimport typing as t\nfrom loguru import logger as log\n\n\nmath_subapp = App(name=\"math\", help=\"Math commands.\")\n\n\n@math_subapp.command(name=\"add\")\ndef add_nums(a: int = 0, b: int = 0):\n    \"\"\"Add 2 numbers and print the sum.\n\n    Params:\n        a (int): First number to add.\n        b (int): Second number to add.\n    \"\"\"\n    log.debug(f\"a={a}, b={b}\")\n\n    ## Add the numbers\n    sum: int = a + b\n\n    if sum == 0:\n        print(\"Nothing to add!\")\n    else:\n        print(f\"{a} + {b} = {sum}\")\n</code></pre> <p>Then, in the <code>cli/main.py</code> file, I will import the <code>math_subapp</code> command, which will make <code>my_pymodule math add &lt;a&gt; &lt;b&gt;</code> available.</p> cli/main.py<pre><code>from cyclopts import App, Parameter, Group\nimport typing as t\nfrom loguru import logger as log\n\n## Import the math subapp\nfrom my_pymodule.commands import math_subapp\n\n\n## Initialize the app. Give the app a name, and a help message for --help\napp = App(name=\"demo\", help=\"Cyclopts demo app.\")\n## Allow global flags like --debug\napp.meta.group_parameters = Group(\"Session Parameters\", sort_key=0)\n\n## Mount the math subcommand\napp.command(math_subapp)\n\n\n@app.command(name=\"hello\")\ndef say_hello(name: str = \"world\"):\n    \"\"\"Say hello to a user.\n\n    Description:\n        Says hello to a user, or 'world' if no user is given. This docstring becomes the command's help messsage!\n    Params:\n        name (str): Name of user to say hello to.\n\n    \"\"\"\n    print(f\"Hello, {name}!\")\n\n\n@app.meta.default\ndef cli_launcher(\n    *tokens: t.Annotated[str, Parameter(show=False, allow_leading_hyphen=True)],\n    debug: t.Annotated[\n        bool,\n        Parameter(\n            name=[\"-d\", \"--debug\"], show_default=True, help=\"Enable debug logging.\"\n        ),\n    ] = False,\n    file_log: t.Annotated[\n        bool,\n        Parameter(\n            name=[\"-l\", \"--log-file\"], show_default=True, help=\"Enable logging to file\"\n        ),\n    ] = False,\n):\n    ## Initialize Loguru logger\n    log.remove(0)\n\n    ## If --debug flag was passed, set logging to `DEBUG` and more verbose log format\n    if debug:\n        log.add(\n            sys.stderr,\n            format=\"{time:YYYY-MM-DD HH:mm:ss} | [{level}] | {name}.{function}:{line} | &gt; {message}\",\n            level=\"DEBUG\",\n        )\n\n    ## If --debug flag not passed, set logging to `INFO` and a shorter log message format\n    else:\n        log.add(\n            sys.stderr,\n            format=\"{time:YYYY-MM-DD HH:mm:ss} [{level}] : {message}\",\n            level=\"INFO\",\n        )\n\n    ## Call the cyclopts app with user's inputs\n    app(tokens)\n\n\nif __name__ == \"__main__\":\n    ## Call the app's meta launcher\n    app.meta()\n</code></pre> <p>Now, run <code>my_pymodule math --help</code> to see the <code>add</code> command, and <code>my_pymodule math add --help</code> to see the help message for <code>add</code>. Call the command with <code>my_pymodule math add 1 2</code> and <code>my_pymodule math add 1 2 --debug</code>.</p> <p>You can nest commands/sub-apps like this to add functionality to your CLI. These are the basics of <code>cyclopts</code>. Check the <code>cyclopts</code> documentation for more information on building CLIs and TUIs using the package.</p>","tags":["python","utilities","cyclopts"]},{"location":"programming/python/Argument%20Parsing/cyclopts/building_a_cli.html#adding-as-package-entrypoint","title":"Adding as package entrypoint","text":"<p>When you call your package as a module, i.e. <code>python -m my_pymodule</code>, you want to be able to pass args like <code>math add 1 2</code>. To do this, import your <code>cyclopts</code> app into <code>src/my_pymodule/cli/__init__.py</code>, then add the app to <code>src/my_pymodule/__main__.py</code>.</p> src/my_pymodule/cli/__init__.py<pre><code>from .main import app\n</code></pre> <p>Then set it as your package's entrypoint in <code>__main__.py</code>:</p> src/my_pymodule/__main__.py<pre><code>from my_pymodule import cli\n\nif __name__ == \"__main__\":\n    ## When my_pymodule is called from the CLI, launch the cyclopts app\n    cli.app.meta()\n</code></pre>","tags":["python","utilities","cyclopts"]},{"location":"programming/python/Argument%20Parsing/cyclopts/script_entrypoint.html","title":"Call a cyclopts CLI from another Python script","text":"<p>It is possible to call your <code>cyclopts</code> app from other Python scripts. This can be useful for calling a specific set of arguments in a repeatable way. As an example, we'll create a Python script that prompts a user for their name, then calls the <code>say_hello()</code> command of the <code>cyclopts</code> app we built above.</p> <p>In the root of the project folder, i.e. where your <code>pyproject.toml</code> or <code>requirements.txt</code> are, above the <code>src/</code> path, create a file <code>say_hi.py</code>. This script will import the <code>my_pymodule.cli</code> module, prompt the user for a name, then call the <code>say_hello()</code> command of the CLI app, all within the Python script file.</p> say_hi.py<pre><code>from my_pymodule import cli\n\nif __name__ == \"__main__\":\n    name: str = input(\"What is your name? \")\n    command: list[str] = [\"hello\", name]\n\n    cli.app.meta(tokens=command)\n</code></pre> <p>Now when you run <code>python say_hi.py</code>, the script will create a list of commands to pass as tokens to the meta app. If you want to enable debugging, you can just add <code>--debug</code> to the commands list:</p> Add debugging to command list<pre><code>...\n\ncommand: list[str] = [..., \"--debug\"]\n\n...\n</code></pre>","tags":["python","utilities","cyclopts"]},{"location":"programming/python/Context%20Managers/index.html","title":"Context Managers","text":"<p>Todo</p> <ul> <li>ELI5: Context managers</li> <li>Examples you may have encountered<ul> <li><code>with open() as f:</code></li> <li><code>with sqlalchemy.Session() as sess:</code></li> </ul> </li> <li>Writing a context manager function with <code>@contextmanager</code></li> <li>Writing a context manager class<ul> <li>The <code>__enter__()</code> and <code>__exit__()</code> methods</li> <li>Handling exceptions</li> </ul> </li> </ul>","tags":["python","context-managers"]},{"location":"programming/python/Context%20Managers/index.html#extra-reading","title":"Extra reading","text":"<ul> <li>Python docs: contextlib</li> <li>RealPython: Context Managers and Python's with Statement</li> <li>PythonTutorial: Python context managers</li> <li>LearnDataSci.com: Context managers in Python using the \"with\" statement</li> <li>FreeCodeCamp: Context Managers in Python</li> <li>Medium: Demystifying Python Context Managers: A Comprehensive Guide</li> </ul>","tags":["python","context-managers"]},{"location":"programming/python/SQLAlchemy/index.html","title":"SQLAlchemy","text":"<p>Introduction</p> <p>These docs written for <code>sqlalchemy == 2.0</code></p> <ul> <li>\ud83c\udfe0 SQLAlchemy Home</li> <li>\ud83d\udcd6 SQLAlchemy Docs Index<ul> <li>\ud83d\udcc4 SQLAlchemy ORM Quick Start<ul> <li>Learn the new 2.0 syntax with a guided tutorial/quickstart.</li> </ul> </li> <li>\ud83d\udcc4 SQLAlchemy Unified Tutorial<ul> <li>The 2.0 release of SQLAlchemy introduced a new ORM syntax. It is different enough from versions prior to 2.0 that a tutorial demonstrating the \"old\" and \"new\" ways of doing things was needed.</li> <li>The newer 2.0 syntax is simpler and more Pythonic, and feels more flexible (subjective opinions).</li> </ul> </li> </ul> </li> </ul>","tags":["python","sqlalchemy"]},{"location":"programming/python/SQLAlchemy/index.html#sample-code","title":"Sample Code","text":"<p>Check the pages in this section for sample code &amp; explanations for using SQLAlchemy in your app.</p> <p>Note</p> <p>Check my <code>red-utils</code> package's <code>.ext.sqlalchemy_utils</code> module for an example <code>database</code> module. You can essentially copy/paste the code into a directory in your project like <code>src/app/database/</code>.</p>","tags":["python","sqlalchemy"]},{"location":"programming/python/SQLAlchemy/index.html#full-sample-from-initializing-dbsettings-to-adding-an-entity","title":"Full sample: From initializing DBSettings to adding an entity","text":"<p>When setting SQLAlchemy up in a new project, you must lay some groundwork first. You need to create a <code>Base</code> class that your models will inherit from, and you need to configure an engine &amp; session pool. Repository classes are optional, but highly recommended to control reading from/writing to the database.</p> <p>The guide below assumes you have created a directory in your project called <code>db/</code> to store all the SQLAlchemy code.</p>","tags":["python","sqlalchemy"]},{"location":"programming/python/SQLAlchemy/index.html#setup-sqlalchemy-configuration","title":"Setup SQLAlchemy configuration","text":"<p>Todo</p> <ul> <li> Section for loading from <code>dynaconf</code></li> <li> Section for loading from env with <code>os.getenv()</code></li> <li> Creating a <code>DBSettings</code> class</li> </ul>","tags":["python","sqlalchemy"]},{"location":"programming/python/SQLAlchemy/index.html#create-engine-session-pool","title":"Create engine &amp; session pool","text":"<p>Todo</p> <ul> <li> Creating an engine</li> <li> Creating a session pool</li> </ul>","tags":["python","sqlalchemy"]},{"location":"programming/python/SQLAlchemy/index.html#create-your-sqlalchemy-and-repository-base-classes","title":"Create your SQLAlchemy and repository Base classes","text":"<p>In your <code>db/</code> directory, create a file called <code>base.py</code>. This file is where you will configure your SQLAlchemy <code>Base</code> class, as well as any other base classes for the app like <code>BaseRepository</code>:</p> db/base.py<pre><code>from __future__ import annotations\n\nimport typing as t\n\nimport sqlalchemy as sa\nimport sqlalchemy.exc as sa_exc\nimport sqlalchemy.orm as so\n\n## Generic type representing an instance of a class\nT = t.TypeVar(\"T\")\n\n\nclass Base(so.DeclarativeBase):\n    pass\n\n\nclass BaseRepository(t.Generic[T]):\n    \"\"\"Base class for a SQLAlchemy database repository.\n\n    Usage:\n        When creating a new repository class, inherit from this BaseRepository.\n        The new class will have sessions for create(), get(), update(), delete(), and list().\n    \"\"\"\n\n    def __init__(self, session: so.Session, model: t.Type[T]):\n        self.session = session\n        self.model = model\n\n    def create(self, obj: T) -&gt; T:\n        self.session.add(obj)\n\n        self.session.commit()\n        self.session.refresh(obj)\n\n        return obj\n\n    def get(self, id: int) -&gt; t.Optional[T]:\n        return self.session.get(self.model, id)\n\n    def update(self, obj: T, data: dict) -&gt; T:\n        for key, value in data.items():\n            setattr(obj, key, value)\n\n        self.session.commit()\n\n        return obj\n\n    def delete(self, obj: T) -&gt; None:\n        self.session.delete(obj)\n\n        self.session.commit()\n\n    def list(self) -&gt; list[T]:\n        return self.session.execute(sa.select(self.model)).scalars().all()\n\n    def count(self) -&gt; int:\n        \"\"\"Return the count of entities in the table.\"\"\"\n        return self.session.query(self.model).count()\n</code></pre> <p>When creating new database model classes, you will inherit from this <code>Base</code> class. For example, to create a <code>User</code> model in a file called <code>models.py</code>:</p> Example models.py<pre><code>## Import the Base class from the db/ directory\nfrom db import Base\n\nimport sqlalchemy as sa\nimport sqlalchemy.orm as so\n\n\n## Create a User class that inherits from the SQLAlchemy Base class\nclass User(Base):\n    __tablename__ = \"users\"\n\n    id: so.Mapped[int] = so.mapped_column(sa.INTEGER, primary_key=True)\n\n    name: so.Mapped[str] = so.mapped_column(sa.TEXT, nullable=False, default=None)\n    age: so.Mapped[int] = so.mapped_column(sa.INTEGER, nullable=False, default=0)\n</code></pre> <p>Similarly, the <code>BaseRepository</code> class can be inherited from when creating a <code>UserRepository</code> class to handle CRUD operations on the <code>User</code> model:</p> Example UserRepository() class<pre><code>## Import the BaseRepository class from db/\nfrom db import BaseRepository\n## Import the User model from models.py\nfrom .models import User\n\nimport sqlalchemy as sa\nimport sqlalchemy.orm as so\nimport sqlalchemy.exc as sa_exc\n\n\n## Inherit from the BaseRepository class, granting access to all functions/parameters\nclass UserRepository(BaseRepository):\n    def __init__(self, session: so.Session):\n        ## The super() function here calls the base class's __init__. This is required for inheritance\n        super().__init__(session, User)\n\n    ## This class now has access to the BaseRepository's functions, like create(), get(), update(), etc\n    #  Any functions/parameters defined on this class are exclusive to this class and do not get propagated\n    #  \"down\" to the BaseRepository\n    def get_by_name(self, name: str) -&gt; User:\n        return self.session.query(User).filter(User.name == name).one_or_none()\n\n    ...\n</code></pre>","tags":["python","sqlalchemy"]},{"location":"programming/python/SQLAlchemy/index.html#committing-a-user-to-the-database","title":"Committing a User to the database","text":"<p>Todo</p> <ul> <li> Example converting a dict to a <code>User</code> class</li> <li> Example of creating a <code>session_pool</code></li> <li> Example of using a repository to commit a <code>User</code> to the database</li> </ul>","tags":["python","sqlalchemy"]},{"location":"programming/python/SQLAlchemy/DBConfig_settings.html","title":"DBConfig settings class","text":"<p>I use a Python <code>dataclass</code> to store my database settings. The <code>DBSettings</code> class below accepts SQLAlchemy parameters for a database connection, and handles getting session pools, engines, and more using class methods.</p> DBConfig settings<pre><code>from __future__ import annotations\n\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nimport typing as t\n\nimport sqlalchemy as sa\nimport sqlalchemy.orm as so\n\n@dataclass\nclass DBSettings:\n    \"\"\"Store configuration for a database.\n\n    Params:\n        drivername (str): The `sqlalchemy` driver name, i.e. `'sqlite+pysqlite'`.\n        username (str|None): The database user's username.\n        password (str|None): The database user's password.\n        host (str|None): The database host address.\n        port (str|int|None): The database host's port.\n        database (str): The name of the database to connect to. For SQLite, use the path to the file,\n            i.e. `db/app.sqlite`.\n        echo (bool): If `True`, the SQLAlchemy `Engine` will echo SQL queries to the CLI, and will create tables\n            that do not exist (if possible).\n\n    \"\"\"\n\n    drivername: str = field(default=\"sqlite+pysqlite\")\n    username: str | None = field(default=None)\n    password: str | None = field(default=None, repr=False)\n    host: str | None = field(default=None)\n    port: str | None = field(default=None)\n    database: str = field(default=\"app.sqlite\")\n    echo: bool = field(default=False)\n\n    def __post_init__(self):  # noqa: D105\n        assert self.drivername is not None, ValueError(\"drivername cannot be None\")\n        assert isinstance(self.drivername, str), TypeError(\n            f\"drivername must be of type str. Got type: ({type(self.drivername)})\"\n        )\n        assert isinstance(self.echo, bool), TypeError(\n            f\"echo must be a bool. Got type: ({type(self.echo)})\"\n        )\n        if self.username:\n            if self.username == \"\":\n                self.username = None\n            else:\n                assert isinstance(self.username, str), TypeError(\n                    f\"user must be of type str. Got type: ({type(self.username)})\"\n                )\n        if self.password:\n            if self.password == \"\":\n                self.password = None\n            else:\n                assert isinstance(self.password, str), TypeError(\n                    f\"password must be of type str. Got type: ({type(self.password)})\"\n                )\n        if self.host:\n            if self.host == \"\":\n                self.host = None\n            else:\n                assert isinstance(self.host, str), TypeError(\n                    f\"host must be of type str. Got type: ({type(self.host)})\"\n                )\n        if self.port:\n            if self.port == \"\":\n                self.port = None\n            else:\n                assert isinstance(self.port, int), TypeError(\n                    f\"port must be of type int. Got type: ({type(self.port)})\"\n                )\n                assert self.port &gt; 0 and self.port &lt;= 65535, ValueError(\n                    f\"port must be an integer between 1 and 65535\"\n                )\n        if self.database:\n            assert isinstance(self.database, Path) or isinstance(\n                self.database, str\n            ), TypeError(\n                f\"database must be of type str or Path. Got type: ({type(self.database)})\"\n            )\n            if isinstance(self.database, Path):\n                self.database: str = f\"{self.database}\"\n\n    def get_db_uri(self) -&gt; sa.URL:\n        \"\"\"Construct a SQLAlchemy `URL` from class params.\n\n        Returns:\n            (sqlalchemy.URL): An initialized database connection URL.\n\n        \"\"\"\n        try:\n            _uri: sa.URL = sa.URL.create(\n                drivername=self.drivername,\n                username=self.username,\n                password=self.password,\n                host=self.host,\n                port=self.port,\n                database=self.database,\n            )\n\n            return _uri\n\n        except Exception as exc:\n            msg = Exception(\n                f\"Unhandled exception getting SQLAlchemy database URL. Details: {exc}\"\n            )\n            raise msg\n\n    def get_engine(self, echo_override: bool | None = None) -&gt; sa.Engine:\n        \"\"\"Build &amp; return a SQLAlchemy `Engine`.\n\n        Returns:\n            `sqlalchemy.Engine`: A SQLAlchemy `Engine` instance.\n\n        \"\"\"\n        assert self.get_db_uri() is not None, ValueError(\"db_uri is not None\")\n        assert isinstance(self.get_db_uri(), sa.URL), TypeError(\n            f\"db_uri must be of type sqlalchemy.URL. Got type: ({type(self.get_db_uri)})\"\n        )\n\n        if echo_override is not None:\n            _echo: bool = echo_override\n        else:\n            _echo: bool = self.echo\n\n        try:\n            engine: sa.Engine = sa.create_engine(\n                url=self.get_db_uri().render_as_string(hide_password=False),\n                echo=_echo,\n            )\n\n            return engine\n        except Exception as exc:\n            msg = Exception(\n                f\"Unhandled exception getting database engine. Details: {exc}\"\n            )\n\n            raise msg\n\n    def get_session_pool(self) -&gt; so.sessionmaker[so.Session]:\n        \"\"\"Configure a session pool using class's SQLAlchemy `Engine`.\n\n        Returns:\n            (sqlalchemy.orm.sessionmaker): A SQLAlchemy `Session` pool for database connections.\n\n        \"\"\"\n        engine: sa.Engine = self.get_engine()\n        assert engine is not None, ValueError(\"engine cannot be None\")\n        assert isinstance(engine, sa.Engine), TypeError(\n            f\"engine must be of type sqlalchemy.Engine. Got type: ({type(engine)})\"\n        )\n\n        session_pool: so.sessionmaker[so.Session] = so.sessionmaker(bind=engine)\n\n        return session_pool\n\n    @contextmanager\n    def get_db(self) -&gt; t.Generator[so.Session, t.Any, None]:\n        \"\"\"Context manager class to handle a SQLAlchemy Session pool.\n\n        Usage:\n\n        ```py title=\"get_db() dependency usage\" linenums=\"1\"\n\n        ## Assumes `db_settings` is an initialized instance of `DBSettings`.\n        with db_settings.get_db() as session:\n            repo = someRepoClass(session)\n\n            all = repo.get_all()\n        ```\n        \"\"\"\n        SESSION_POOL: so.sessionmaker[so.Session] = self.get_session_pool()\n        db: so.Session = SESSION_POOL()\n\n        try:\n            yield db\n        except Exception as exc:\n            msg = Exception(\n                f\"Unhandled exception yielding database session. Details: {exc}\"\n            )\n\n            raise msg\n        finally:\n            db.close()\n</code></pre>","tags":["python","sqlalchemy"]},{"location":"programming/python/SQLAlchemy/DBConfig_settings.html#dbconfig-usage","title":"DBConfig - Usage","text":"How to use DBSettings class<pre><code>db_settings: DBSettings = DBSettings(\n    drivername=\"postgresql+psycopg2\",\n    host=\"127.0.0.1\",\n    port=5432,\n    username=\"postgres\",\n    password=\"postgres\",\n    database=\"example\"\n)\n</code></pre>","tags":["python","sqlalchemy"]},{"location":"programming/python/SQLAlchemy/Table_Mixins.html","title":"Table Mixin Classes","text":"<p>SQLAlchemy supports \"mixins,\" which are partial classes that define functionality when multi-inheritance is used to declare a table class. An example is the <code>TablenameMixin</code> class, which automatically creates a table name based on the class's name (i.e. a class named <code>UserComment</code> would be named <code>usercomments</code>).</p>","tags":["python","sqlalchemy"]},{"location":"programming/python/SQLAlchemy/Table_Mixins.html#multi-inheritance-example","title":"Multi-inheritance example","text":"SQLAlchemy table class multi-inheritance example<pre><code>import sqlalchemy as sa\nimport sqlalchemy.orm as so\n\nfrom your_database_module.base import Base\n\n\nclass TableNameMixin:\n    \"\"\"Mixin to automatically name tables based on class name.\n\n    Generates a `__tablename__` for classes inheriting from this mixin.\n    \"\"\"\n\n    @so.declared_attr.directive\n    def __tablename__(cls) -&gt; str:\n        return cls.__name__.lower() + \"s\"\n\n\nclass UserComment(Base, TabeNameMixin):\n    ## No need to declare a table name, the table will be named 'usercomments'\n    #   __table_name__ = \"user_comments_tbl\"\n\n    ...\n</code></pre>","tags":["python","sqlalchemy"]},{"location":"programming/python/SQLAlchemy/Table_Mixins.html#example-mixin-classes","title":"Example mixin classes","text":"","tags":["python","sqlalchemy"]},{"location":"programming/python/SQLAlchemy/Table_Mixins.html#timestampmixin-class","title":"TimestampMixin class","text":"TimestampMixin<pre><code>from datetime import datetime\n\nimport sqlalchemy as sa\nimport sqlalchemy.orm as so\n\n\nclass TimestampMixin:\n    \"\"\"Add a created_at &amp; updated_at column to records.\n\n    Add to class declaration to automatically create these columns on\n    records.\n\n    Usage:\n\n    py linenums=1\n    class Record(Base, TimestampMixin):\n        __tablename__ = ...\n\n        ...\n\n    \"\"\"\n\n    created_at: so.Mapped[datetime] = so.mapped_column(\n        sa.TIMESTAMP, server_default=sa.func.now()\n    )\n    updated_at: so.Mapped[datetime] = so.mapped_column(\n        sa.TIMESTAMP, server_default=sa.func.now(), onupdate=sa.func.now()\n    )\n</code></pre>","tags":["python","sqlalchemy"]},{"location":"programming/python/SQLAlchemy/Table_Mixins.html#tablenamemixin-class","title":"TableNameMixin class","text":"TimestampMixin<pre><code>class TableNameMixin:\n    \"\"\"Mixin to automatically name tables based on class name.\n\n    Generates a `__tablename__` for classes inheriting from this mixin.\n    \"\"\"\n\n    @so.declared_attr.directive\n    def __tablename__(cls) -&gt; str:  # noqa: D105\n        return cls.__name__.lower() + \"s\"\n</code></pre>","tags":["python","sqlalchemy"]},{"location":"programming/python/SQLAlchemy/annotated_columns.html","title":"Annotated columns &amp; custom types","text":"<ul> <li>\ud83d\udcc4 SQLAlchemy docs: Mapping whole column declarations to Python types</li> </ul>","tags":["python","sqlalchemy"]},{"location":"programming/python/SQLAlchemy/annotated_columns.html#annotated-columns","title":"Annotated columns","text":"","tags":["python","sqlalchemy"]},{"location":"programming/python/SQLAlchemy/annotated_columns.html#example-annotated-columns","title":"Example annotated columns","text":"annotated_columns.py<pre><code>\"\"\"Define `Annotated` columns for SQLAlchemy models.\n\nExamples:\n    * `INT_PK`: An auto-incrementing, primary key integer value.\n    * `STR_10`: A `VARCHAR(10)` column.\n    * `STR_255`: A `VARCHAR(255)` column.\n\n\"\"\"\n\nfrom __future__ import annotations\n\nimport sqlalchemy as sa\nimport sqlalchemy.orm as so\nfrom typing_extensions import Annotated\n\n## Annotated auto-incrementing integer primary key column\nINT_PK = Annotated[\n    int, so.mapped_column(sa.INTEGER, primary_key=True, autoincrement=True, unique=True)\n]\n\n## SQLAlchemy VARCHAR(10)\nSTR_10 = Annotated[str, so.mapped_column(sa.VARCHAR(10))]\n## SQLAlchemy VARCHAR(255)\nSTR_255 = Annotated[str, so.mapped_column(sa.VARCHAR(255))]\n</code></pre>","tags":["python","sqlalchemy"]},{"location":"programming/python/SQLAlchemy/annotated_columns.html#custom-column-types","title":"Custom Column Types","text":"","tags":["python","sqlalchemy"]},{"location":"programming/python/SQLAlchemy/annotated_columns.html#example-customjson-column-type","title":"Example CustomJson column type","text":"<p>Column type for converting objects to JSON for storage in SQLAlchemy. Inputs can be a list of types, like <code>list[int]</code>, <code>list[str]</code>, a Python <code>dict</code>, or any other JSON-serializable object.</p> CustomJson SQLAlchemy column type<pre><code>import json\n\nimport sqlalchemy as sa\n\nclass CustomJson(sa.TypeDecorator):\n    impl = sa.String\n\n    def process_bind_param(self, value, dialect):\n        return json.dumps(value)\n\n    def process_result_value(self, value, dialect):\n        return json.loads(value)\n</code></pre>","tags":["python","sqlalchemy"]},{"location":"programming/python/SQLAlchemy/dependencies.html","title":"SQLAlchemy dependency methods","text":"<p>Dependencies for your database code. These can be context manager classes/functions, initialized objects, and other things to assist you interacting with the database.</p> _depends.py<pre><code>\"\"\"Dependencies for database.\n\nIncludes functions like `get_db()`, which is a context manager that yields a database session.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\n\nfrom contextlib import contextmanager\nimport typing as t\n\nlog = logging.getLogger(__name__)\n\nfrom .db_config import DBSettings\n\nimport sqlalchemy as sa\nimport sqlalchemy.orm as so\n\n\ndef get_db_uri(\n    drivername: str = None,\n    username: str = None,\n    password: str = None,\n    host: str = None,\n    port: t.Union[int, str] = None,\n    database: str = None,\n) -&gt; sa.URL:\n    \"\"\"Construct a SQLAlchemy `URL` from params.\n\n    Returns:\n        (sqlalchemy.URL): An initialized database connection URL.\n\n    \"\"\"\n    try:\n        _uri: sa.URL = sa.URL.create(\n            drivername=drivername,\n            username=username,\n            password=password,\n            host=host,\n            port=port,\n            database=database,\n        )\n\n        return _uri\n\n    except Exception as exc:\n        msg = Exception(\n            f\"Unhandled exception getting SQLAlchemy database URL. Details: {exc}\"\n        )\n        log.error(msg)\n\n        raise exc\n\n\ndef get_engine(db_uri: sa.URL = None, echo: bool = False) -&gt; sa.Engine:\n    \"\"\"Build &amp; return a SQLAlchemy `Engine`.\n\n    Returns:\n        `sqlalchemy.Engine`: A SQLAlchemy `Engine` instance.\n\n    \"\"\"\n    if db_uri is None:\n        raise ValueError(\"db_uri is not None\")\n    if not isinstance(db_uri, sa.URL):\n        raise TypeError(\n            f\"db_uri must be of type sqlalchemy.URL. Got type: ({type(db_uri)})\"\n        )\n\n    try:\n        engine: sa.Engine = sa.create_engine(\n            url=db_uri.render_as_string(hide_password=False),\n            echo=echo,\n        )\n\n        return engine\n    except Exception as exc:\n        msg = Exception(f\"Unhandled exception getting database engine. Details: {exc}\")\n        log.error(msg)\n\n        raise exc\n\n\ndef get_session_pool(\n    engine: sa.Engine = None, autoflush: bool = False, expire_on_commit: bool = False\n) -&gt; so.sessionmaker[so.Session]:\n    \"\"\"Configure a session pool using class's SQLAlchemy `Engine`.\n\n    Returns:\n        (sqlalchemy.orm.sessionmaker): A SQLAlchemy `Session` pool for database connections.\n\n    \"\"\"\n    if engine is None:\n        raise ValueError(\"engine cannot be None\")\n    if not isinstance(engine, sa.Engine):\n        raise TypeError(\n            f\"engine must be of type sqlalchemy.Engine. Got type: ({type(engine)})\"\n        )\n\n    session_pool: so.sessionmaker[so.Session] = so.sessionmaker(\n        bind=engine, autoflush=autoflush, expire_on_commit=expire_on_commit\n    )\n\n    return session_pool\n\n\n@contextmanager\ndef get_db(\n    db_uri: t.Union[sa.URL, str] = None,\n    echo: bool = False,\n    autoflush: bool = False,\n    expire_on_commit: bool = False,\n) -&gt; t.Generator[so.Session, t.Any, None]:\n    \"\"\"Dependency to yield a SQLAlchemy Session pool.\n\n    Usage:\n\n    from core.dependencies import get_db\n\n    with get_db() as session:\n        repo = someRepoClass(session)\n\n        all = repo.get_all()\n\n    \"\"\"\n    if db_uri is None:\n        raise ValueError(\"Missing a SQLAlchemy URL object.\")\n    if not isinstance(db_uri, sa.URL):\n        raise TypeError(\n            f\"Invalid type for db_uri: ({type(db_uri)}). Must be a SQLAlchemy URL object.\"\n        )\n\n    engine = get_engine(db_uri=db_uri, echo=echo)\n\n    SESSION_POOL: so.sessionmaker[so.Session] = get_session_pool(\n        engine=engine, autoflush=autoflush, expire_on_commit=expire_on_commit\n    )\n\n    db: so.Session = SESSION_POOL()\n\n    try:\n        yield db\n    except Exception as exc:\n        msg = Exception(\n            f\"Unhandled exception yielding database session. Details: {exc}\"\n        )\n\n        raise msg\n    finally:\n        db.close()\n</code></pre>","tags":["python","sqlalchemy"]},{"location":"programming/python/SQLAlchemy/repositories.html","title":"Repositories","text":"<p>A database repository standardizes operations for a given entity. The repository is a class that contains methods like <code>.add()</code>, <code>.remove()</code>, <code>.get(id: int)</code>, etc. Each repository is meant to control a single SQLAlchemy table class (a model class that inherits from SQLAlchemy's <code>Base</code> object).</p> <p>For example, if you have a class <code>User(Base)</code> and <code>Comment(Base)</code>, you would create 2 repositories, <code>UserRepository</code> and <code>CommentRepository</code>. Each repository class would have its own <code>.add()</code>, <code>.remove()</code>, <code>.get()</code>, etc.</p> <p>Note</p> <p>Example app using repository class</p> <p>To see an example of one of my apps that uses this pattern, check <code>auto-xkcd</code>.</p> <ul> <li><code>auto_xkcd.domain.xkcd.comic</code> for models, schemas, and repositories.</li> <li><code>auto_xkcd.core.database</code> for SQLAlchemy code.</li> <li>To see the repository in action, check the <code>save_comic_to_db()</code> method in the <code>auto_xkcd.modules.xkcd_modd.methods</code> module.</li> </ul>","tags":["python","sqlalchemy"]},{"location":"programming/python/SQLAlchemy/repositories.html#inheriting-from-a-repositorybase-class","title":"Inheriting from a RepositoryBase class","text":"<p>You can define an abstract base class (<code>abc.ABCMeta</code>) repository, which your other repository classes can inherit from. On the base class, you can define attributes and methods that must be defined on the child class. This is helpful for requiring methods like <code>.add()</code>, <code>.remove()</code>, etc on all child classes.</p> Example RepositoryBase class<pre><code>import abc\nimport typing as t\n\nT = t.TypeVar(\"T\")\n\nimport sqlalchemy as sa\nimport sqlalchemy as so\n\nclass BaseRepository(t.Generic[T]):\n    \"\"\"Base class for a SQLAlchemy database repository.\n\n    Usage:\n        When creating a new repository class, inherit from this BaseRepository.\n        The new class will have sessions for create(), get(), update(), delete(), and list().\n    \"\"\"\n\n    def __init__(self, session: so.Session, model: t.Type[T]):\n        self.session = session\n        self.model = model\n\n    def create(self, obj: T) -&gt; T:\n        self.session.add(obj)\n\n        self.session.commit()\n        self.session.refresh(obj)\n\n        return obj\n\n    def get(self, id: int) -&gt; t.Optional[T]:\n        return self.session.get(self.model, id)\n\n    def update(self, obj: T, data: dict) -&gt; T:\n        for key, value in data.items():\n            setattr(obj, key, value)\n\n        self.session.commit()\n\n        return obj\n\n    def delete(self, obj: T) -&gt; None:\n        self.session.delete(obj)\n\n        self.session.commit()\n\n    def list(self) -&gt; list[T]:\n        return self.session.execute(sa.select(self.model)).scalars().all()\n\n    def count(self) -&gt; int:\n        \"\"\"Return the count of entities in the table.\"\"\"\n        return self.session.query(self.model).count()\n</code></pre>","tags":["python","sqlalchemy"]},{"location":"programming/python/SQLAlchemy/repositories.html#example-userrepository-inherits-from-repositorybase","title":"Example: UserRepository (inherits from RepositoryBase)","text":"<p>You can use this <code>RepositoryBase</code> class to create child repeositories, for example <code>UserRepository</code>:</p> Example UserRepository class<pre><code>class UserRepository(RepositoryBase):\n    ## Required by RepositoryBase\n    def add(self, entity: UserModel):\n        ...\n\n    ## Required by RepositoryBase\n    def remove(self, entity: UserModel):\n        ...\n\n    ## Required by RepositoryBase\n    def get_by_id(self, entity_id: int) -&gt; UserModel:\n        ...\n\n    ## Not required by RepositoryBase, and only available to UserRepository instances, or children fo this class\n    def count(self) -&gt; int:\n        ...\n</code></pre>","tags":["python","sqlalchemy"]},{"location":"programming/python/SQLAlchemy/sqlalchemy_base.html","title":"SQLAlchemy Base class","text":"","tags":["python","sqlalchemy"]},{"location":"programming/python/SQLAlchemy/sqlalchemy_base.html#sqlalchemy-base","title":"SQLAlchemy Base","text":"<p>The SQLAlchemy <code>DeclarativeBase</code> class is the \"base\" class all of your SQLalchemy table model classes should inherit from. When describing a new entity, you intialize the class like:</p> Initialize table class<pre><code>...\n\nclass SomeTableModel(Base):\n    ...\n</code></pre> <p>When you use SQLAlchemy to create the table metadata, the <code>Base</code> class gathers all models that inherit from the same <code>Base</code> instance and creates table structures in your database.</p>","tags":["python","sqlalchemy"]},{"location":"programming/python/SQLAlchemy/sqlalchemy_base.html#example-basepy","title":"Example base.py","text":"<p>In your project, you can put your SQLAlchemy <code>DeclarativeBase</code> code in a file, i.e. <code>app_name/database/base.py</code>, so it can be imported throughout the rest of your app.</p> Declare SQLAlchemy Base (example base.py)<pre><code>\"\"\"SQLAlchemy `DeclarativeBase`, `MetaData`, and `registry` objects.\n\nImport this `Base` into SQLAlchemy model files and let classes inherit from\nthe `DeclarativeBase` declared here.\n\nThe `registry()` function sets the global SQLAlchemy `registry` for the `DeclarativeBase` object.\n\n!!! note\n\n    Docs for `DeclarativeBase` and `registry()`\n\n    - [Using a DelcarativeBase base class](https://docs.sqlalchemy.org/en/20/orm/declarative_styles.html#using-a-declarative-base-class)\n\n    Docs for MetaData object\n\n    - [Unified tutorial](https://docs.sqlalchemy.org/en/20/tutorial/metadata.html#tutorial-working-with-metadata)\n    - [MetaData Docs](https://docs.sqlalchemy.org/en/20/core/metadata.html)\n    - [Impose a table naming scheme with MetaData object](https://docs.sqlalchemy.org/en/20/core/metadata.html#specifying-a-default-schema-name-with-metadata)\n\n\"\"\"\n\nfrom __future__ import annotations\n\nimport sqlalchemy as sa\nimport sqlalchemy.orm as so\n\n## Global SQLAlchemy MetaData object\nmetadata: sa.MetaData = sa.MetaData()\n\n## Registry object stores mappings &amp; config hooks\nreg = so.registry()\n\n\n## SQLAlchemy DeclarativeBase is the parent class object table classes will inherit from\nclass Base(so.DeclarativeBase):\n    \"\"\"Default/Base class for SQLAlchemy models.\n\n    Description:\n\n    Child classes inheriting from this Base object will be treated as SQLAlchemy\n    models. Set child class tables with `__tablename__ = ....`\n\n    Global defaults can be set on this object (i.e. a SQLAlchemy registry), and will\n    be inherited/accessible by all child classes.\n\n    !!! note\n\n        When this class is instantiated, it will not be of type sqlalchemy.orm.DeclarativeBase;\n            Because of the way this class is intialized, its type will be\n            sqlalchemy.orm.decl_api.DeclarativeAttributeIntercept\n\n    Params:\n        registry (sqlalchemy.Registry): A `registry` object for the `Base` class\n        metadata (sqlalchemy.MetaData): A `MetaData` object, with data about the `Base` class\n    \"\"\"\n\n    registry: so.registry = reg\n    metadata: sa.MetaData = metadata\n</code></pre>","tags":["python","sqlalchemy"]},{"location":"programming/python/SQLAlchemy/validators.html","title":"Validators","text":"","tags":["python","sqlalchemy"]},{"location":"programming/python/SQLAlchemy/validators.html#valid-db-types","title":"Valid DB types","text":"<p>It's useful to have a <code>constants.py</code> or some other place to store validators; to limit the databases your project supports, you could create a <code>valid_db_types</code> variable, like:</p> constants.py - valid_db_types<pre><code>\"\"\"Validators for custom SQLAlchemy utilities.\n\n`valid_db_types`: List of strings of supported database types.\n\nSupported: `[\"sqlite\", \"postgres\", \"mssql\"]`\n\"\"\"\n\n## List of valid/supported databases\nfrom __future__ import annotations\n\nvalid_db_types: list[str] = [\"sqlite\", \"postgres\", \"mssql\"]\n\n## Usage: assert 'some_db_name' not in valid_db_types, ValueError(f\"Database 'some_db_name' is not supported. Supported databases: {valid_db_types}\")\n</code></pre>","tags":["python","sqlalchemy"]},{"location":"programming/python/alembic/index.html","title":"Alembic","text":"<p>Alembic is a database migration tool, meant to be used with the SQLAlchemy package. The tool has a bit of a learning curve, but making changes to your database models using a migration tool is highly recommended, especially for long-lived projects.</p> <p>Question</p> <p>What is a database migration?</p> <p>Migrations offer version control of your database schema by scripting your changes in a repeatable way, incrementally changing your database models over time and offering a path backward if a migration goes south.</p> <p>It's kind of like git, but for your database.</p>","tags":["alembic","python","programming","database"]},{"location":"programming/python/alembic/index.html#using-alembic-in-python","title":"Using Alembic in Python","text":"<p>This is a quick guide on how I set up Alembic for my projects. I am still learning the tool and I'm sure there are more efficient ways of doing some/all of this. I will update this page over time.</p>","tags":["alembic","python","programming","database"]},{"location":"programming/python/alembic/index.html#install-alembic","title":"Install Alembic","text":"<p>First install the <code>alembic</code> package, i.e. <code>pip install alembic</code>. Alembic is normally installed as a \"dev\" dependency, and not included in your production app/package. If your application does a migration as it is coming online, you would want to include <code>alembic</code> in your regular dependencies; otherwise, you should add it as a dev dependency:</p> Installing alembic<pre><code>## with pip\npip install alembic --extra dev\n\n## with uv\nuv add --dev alembic\n\n## with poetry\npoetry add --group dev alembic\n</code></pre>","tags":["alembic","python","programming","database"]},{"location":"programming/python/alembic/index.html#create-alembicini-config-file","title":"Create alembic.ini config file","text":"<p>Next, create an <code>alembic.ini</code> file at the root of your repository. Check the example <code>alembic.ini</code> file I copy/paste into all my projects. You can make changes to the configuration if you want, but don't hardcode any real connection details in this file! You will learn how to connect Alembic to your database below.</p>","tags":["alembic","python","programming","database"]},{"location":"programming/python/alembic/index.html#initialize-migrations-directory","title":"Initialize migrations directory","text":"<p>After creating an <code>alembic.ini</code> file, initialize your Alembic directory by running the command below. The convention for <code>&lt;migrations-dir&gt;</code> below is \"migrations.\" After initializing Alembic, you will have a directory named <code>migrations/</code> (or whatever value you used for <code>&lt;migrations-dir&gt;</code>) in the path where you ran the init command; you can use whatever name you want for this directory, i.e. <code>alembic</code> or <code>project</code>. This guide assumes you are using the conventional \"migrations\":</p> Initialize alembic<pre><code>alembic init &lt;migrations-dir&gt;\n</code></pre> <p>The <code>migrations/</code> directory created after running the init command is where your Alembic code lives. As you create migrations, you will see Python files in the <code>migrations/versions/</code> directory. This is where Alembic keeps a history of previous migrations, allowing you to travel backwards or forwards at will.</p>","tags":["alembic","python","programming","database"]},{"location":"programming/python/alembic/index.html#edit-alembics-envpy-file","title":"Edit Alembic's env.py file","text":"<p>The Alembic initialization also created a file <code>migrations/env.py</code>. This is the file Alembic uses to do your migrations. You need to make a couple of edits to this file before you can create a migration.</p> <ul> <li>Import your database configuration so you can pass a database URI to Alembic.</li> <li>Import your database model classes.<ul> <li>Your SQLAlchemy models (i.e. <code>class ModelName(Base):</code>) must be imported, otherwise Alembic will not be aware of them and won't be able to track changes.</li> <li>If the files are sourced in an <code>__init__.py</code>, it is sufficient to import that entire directory.<ul> <li>i.e. if you have models in <code>domain/models.py</code>, and you import your model classes in <code>domain/__init__.py</code>, it is sufficient to simply import the whole <code>domain</code> module.</li> </ul> </li> </ul> </li> <li>Set your <code>sqlalchemy.url</code> value in the alembic <code>config</code> object.</li> <li>Set your <code>target_metadata</code> to your SQLAlchemy <code>Base.metadata</code></li> </ul> <p>Below is an example of an <code>env.py</code> file I might use in one of my projects.</p> <p>Warning</p> <p>The contents below are only the parts of Alembic's <code>env.py</code> that I change. Don't copy/paste the whole file, there are only a few values to change. Look for a comment like <code># changed by me</code> to spot lines where you need to make an edit.</p> alembic env.py<pre><code>from logging.config import fileConfig\n\nfrom alembic import context\n\nfrom database.config import DB_URI # changed by me, import the database URI string you use to connect with SQLAlchemy.\nfrom database.base import Base # changed by me, import the SQLAlchemy Base object.\nfrom domain.example.models import ModelClass1, ModelClass2 # changed by me, import the database models.\n\nfrom sqlalchemy import engine_from_config, pool\n\n# this is the Alembic Config object, which provides\n# access to the values within the .ini file in use.\nconfig = context.config\nconfig.set_main_option( # changed by me, set the Alembic config object's \"sqlalchemy.url\" value to your database URI.\n    \"sqlalchemy.url\", DB_URI # changed by me, note you could also pass a SQLAlchemy Url object using db_uri.render_as_string(hide_password=False).\n)\n\n# Interpret the config file for Python logging.\n# This line sets up loggers basically.\nif config.config_file_name is not None:\n    fileConfig(config.config_file_name)\n\n# add your model's MetaData object here\n# for 'autogenerate' support\n# from myapp import mymodel\n# target_metadata = mymodel.Base.metadata\ntarget_metadata = Base.metadata # changed by me, set Alembic's target_metadata to the SQLAlchemy Base object's .metadata property\n\ndef run_migrations_offline() -&gt; None:\n  ...\n\n## the rest of the alembic code\n</code></pre>","tags":["alembic","python","programming","database"]},{"location":"programming/python/alembic/index.html#create-the-first-migration","title":"Create the first migration","text":"<p>Use the command below to set your migrations starting point. Each migration after this will be an ancestor in a chain of migrations detailing upgrade paths for the database as your schemas change.</p> Create first migration<pre><code>alembic revision --autogenerate -m \"Initial migration\"\n</code></pre> <p>Your migration is created, and can be found in the <code>migrations/versions/</code> directory. The file will be named <code>&lt;version_hash&gt;_autogenerated_migration.py</code>, where <code>&lt;version_hash&gt;</code> is a string like <code>8d185c0473ce</code>. You can edit this migration script if necessary, and sometimes you will need to if Alembic's <code>--autogenerate</code> creates a migration script that fails.</p> <p>Now you need to apply it with an Alembic \"upgrade.\" This makes changes live in your database to bring the table's schemas inline with the changes described in your migration. You need to do this after each migration.</p> Upgrade database using Alembic migration script.<pre><code>alembic upgrade head\n</code></pre> <p>The <code>head</code> in the above command indicates the most recent migration.</p>","tags":["alembic","python","programming","database"]},{"location":"programming/python/alembic/index.html#switch-between-alembic-versions-with-downgradeupgrade","title":"Switch between Alembic versions with downgrade/upgrade","text":"<p><code>alembic downgrade &lt;revision&gt;</code> and <code>alembic upgrade &lt;revision&gt;</code> are the commands you use to move backwards/forwards through the Alembic migration versions.</p>","tags":["alembic","python","programming","database"]},{"location":"programming/python/alembic/index.html#alembic-downgrade","title":"alembic downgrade","text":"<p>When using the <code>downgrade</code> command, you can use a numeric value like <code>-1</code> to move back 1 revision, or <code>-5</code> to move back 5, etc (any number works as long as there's a version that far back from <code>head</code>). You can also give the command a specific reversion commit hash (i.e. <code>alembic downgrade 8d185c0473ce</code>).</p>","tags":["alembic","python","programming","database"]},{"location":"programming/python/alembic/index.html#alembic-upgrade","title":"alembic upgrade","text":"<p>The <code>upgrade</code> command functions similarly to <code>downgrade</code>, except the numeric values are <code>+#</code> (i.e. <code>+3</code>), a specific hash like <code>8d185c0473ce</code> (must be ahead of the current migration), or <code>head</code> which is the most recent version.</p>","tags":["alembic","python","programming","database"]},{"location":"programming/python/alembic/index.html#example-alembicini-config-file","title":"Example alembic.ini config file","text":"<p>The following is an <code>alembic.ini</code> file I copy/paste into most of my projects. I will occasionally make project-specific tweaks to the file, but the contents below have served me well as a generic Alembic configuration (handling things like the database connection details in <code>migrations/env.py</code>).</p> alembic.ini<pre><code># A generic, single database configuration.\n\n[alembic]\n# path to migration scripts\n# Use forward slashes (/) also on windows to provide an os agnostic path\nscript_location = alembic\n\n# template used to generate migration file names; The default value is %%(rev)s_%%(slug)s\n# Uncomment the line below if you want the files to be prepended with date and time\n# see https://alembic.sqlalchemy.org/en/latest/tutorial.html#editing-the-ini-file\n# for all available tokens\n# file_template = %%(year)d_%%(month).2d_%%(day).2d_%%(hour).2d%%(minute).2d-%%(rev)s_%%(slug)s\n\n# sys.path path, will be prepended to sys.path if present.\n# defaults to the current working directory.\nprepend_sys_path = .\n\n# timezone to use when rendering the date within the migration file\n# as well as the filename.\n# If specified, requires the python&gt;=3.9 or backports.zoneinfo library.\n# Any required deps can installed by adding `alembic[tz]` to the pip requirements\n# string value is passed to ZoneInfo()\n# leave blank for localtime\n# timezone =\n\n# max length of characters to apply to the \"slug\" field\n# truncate_slug_length = 40\n\n# set to 'true' to run the environment during\n# the 'revision' command, regardless of autogenerate\n# revision_environment = false\n\n# set to 'true' to allow .pyc and .pyo files without\n# a source .py file to be detected as revisions in the\n# versions/ directory\n# sourceless = false\n\n# version location specification; This defaults\n# to migrations/versions.  When using multiple version\n# directories, initial revisions must be specified with --version-path.\n# The path separator used here should be the separator specified by \"version_path_separator\" below.\n# version_locations = %(here)s/bar:%(here)s/bat:migrations/versions\n\n# version path separator; As mentioned above, this is the character used to split\n# version_locations. The default within new alembic.ini files is \"os\", which uses os.pathsep.\n# If this key is omitted entirely, it falls back to the legacy behavior of splitting on spaces and/or commas.\n# Valid values for version_path_separator are:\n#\n# version_path_separator = :\n# version_path_separator = ;\n# version_path_separator = space\nversion_path_separator = os  # Use os.pathsep. Default configuration used for new projects.\n\n# set to 'true' to search source files recursively\n# in each \"version_locations\" directory\n# new in Alembic version 1.10\n# recursive_version_locations = false\n\n# the output encoding used when revision files\n# are written from script.py.mako\n# output_encoding = utf-8\n\nsqlalchemy.url = driver://user:pass@localhost/dbname\n\n\n[post_write_hooks]\n# post_write_hooks defines scripts or Python functions that are run\n# on newly generated revision scripts.  See the documentation for further\n# detail and examples\n\n# format using \"black\" - use the console_scripts runner, against the \"black\" entrypoint\n# hooks = black\n# black.type = console_scripts\n# black.entrypoint = black\n# black.options = -l 79 REVISION_SCRIPT_FILENAME\n\n# lint with attempts to fix using \"ruff\" - use the exec runner, execute a binary\n# hooks = ruff\n# ruff.type = exec\n# ruff.executable = %(here)s/.venv/bin/ruff\n# ruff.options = --fix REVISION_SCRIPT_FILENAME\n\n# Logging configuration\n[loggers]\nkeys = root,sqlalchemy,alembic\n\n[handlers]\nkeys = console\n\n[formatters]\nkeys = generic\n\n[logger_root]\nlevel = WARN\nhandlers = console\nqualname =\n\n[logger_sqlalchemy]\nlevel = WARN\nhandlers =\nqualname = sqlalchemy.engine\n\n[logger_alembic]\nlevel = INFO\nhandlers =\nqualname = alembic\n\n[handler_console]\nclass = StreamHandler\nargs = (sys.stderr,)\nlevel = NOTSET\nformatter = generic\n\n[formatter_generic]\nformat = %(levelname)-5.5s [%(name)s] %(message)s\ndatefmt = %H:%M:%S\n</code></pre>","tags":["alembic","python","programming","database"]},{"location":"programming/python/alembic/index.html#my-alembiccontroller-context-manager-class","title":"My AlembicController context manager class","text":"<p>Alembic can be imported into a Python file so you can script Alembic functions. You can put the contents below into a file like <code>alembic_ctl.py</code> to create a custom Alembic CLI.</p> <p>Tip</p> <p>Run the file without any args, or with <code>-h/--help</code>, to see the CLI's help menu. Example:</p> <pre><code>python alembic_ctl.py -h\n</code></pre> alembic_ctl.py<pre><code>\"\"\"Wrapper class for the Alembic CLI.\n\nImports from the alembic Python package so Alembic operations can be scripted. Implements a CLI\nwith argparse and logging.\n\nUsage:\n    python -m scripts.db.alembic_ctl [options]\n\nExample:\n    python -m scripts.db.alembic_ctl -m \"your revision message\" --dry-run # Show the 'what if' for a migration\n    python -m scripts.db.alembic_ctl -m -U # Create migration using default 'autogenerated migration' message and upgrade database\n    python -m scripts.db.alembic_ctl -d -1 # downgrade database by 1 revision\n\n\"\"\"\n\nfrom pathlib import Path\nimport typing as t\nimport argparse\nimport logging\n\nfrom contextlib import AbstractContextManager\n\nfrom alembic import command\nfrom alembic.config import Config\nfrom alembic.script import ScriptDirectory\n\nlog = logging.getLogger(__name__)\n\n\ndef setup_logging(\n    log_level: str = \"INFO\",\n    log_fmt: str = \"%(asctime)s - %(levelname)s - %(message)s\",\n    date_fmt: str = \"%Y-%m-%d %H:%M:%S\",\n) -&gt; None:\n    \"\"\"Initialize script logging.\n\n    Params:\n        log_level (str, optional): Log level. Defaults to \"INFO\".\n        log_fmt (str, optional): Log format. Defaults to \"%(asctime)s - %(levelname)s - %(message)s\".\n        date_fmt (str, optional): Date format. Defaults to \"%Y-%m-%d %H:%M:%S\".\n    \"\"\"\n    logging.basicConfig(\n        level=log_level or \"INFO\",\n        format=log_fmt,\n        datefmt=date_fmt,\n    )\n\n\nclass AlembicController(AbstractContextManager):\n    \"\"\"Context manager for Alembic operations.\n\n    Params:\n        alembic_ini_path (str | Path, optional): Path to the Alembic configuration file. Defaults to \"alembic.ini\".\n    Attributes:\n        cfg_file (str): Path to the Alembic configuration file.\n        config (Config | None): Alembic configuration object.\n        log (logging.Logger | None): Logger for the class.\n\n    Example:\n        with AlembicController() as ac:\n            ac.upgrade()\n            ac.create_migration()\n\n    Raises:\n        Exception: If an error occurs during Alembic operations.\n    \"\"\"\n\n    def __init__(\n        self,\n        alembic_ini_path: t.Union[str, Path] = \"alembic.ini\",\n        dry_run: bool = False,\n        do_upgrade: bool = False,\n    ):\n        ## Path to the Alembic configuration file\n        self.cfg_file: str = str(alembic_ini_path)\n\n        ## Dry run flag\n        self.dry_run: bool = dry_run\n\n        ## Do upgrade flag\n        self.do_upgrade: bool = do_upgrade\n\n        ## Alembic configuration object\n        self.config: Config | None = None\n\n        ## Logger for the class\n        self.log: logging.Logger | None = None\n\n    def __enter__(self) -&gt; t.Self:\n        ## Initialize class logger\n        self.log = log.getChild(\"AlembicController\")\n        ## Initialize Alembic configuration object\n        self.config = Config(self.cfg_file)\n\n        return self\n\n    def __exit__(self, exc_type, exc_val, traceback) -&gt; t.Literal[False] | None:\n        if exc_val:\n            self.log.error(f\"({exc_type}) {exc_val}\")\n\n            if traceback:\n                self.log.error(f\"Traceback: {traceback}\")\n\n            return False\n\n        return True\n\n    def __repr__(self) -&gt; str:\n        return f\"AlembicController(alembic_ini_path={self.cfg_file}, dry_run={self.dry_run}, do_upgrade={self.do_upgrade})\"\n\n    def upgrade(self, revision: str = \"head\") -&gt; None:\n        \"\"\"Upgrade the database, or simulate with --dry-run.\n\n        Args:\n            revision (str, optional): Revision to upgrade to. Defaults to \"head\".\n            dry_run (bool, optional): Simulate upgrade. Defaults to False.\n\n        Raises:\n            Exception: If an error occurs during upgrade.\n        \"\"\"\n        log.info(\n            f\"{'Simulating' if self.dry_run else 'Upgrading'} to revision: {revision}\"\n        )\n        try:\n            command.upgrade(self.config, revision, sql=self.dry_run)\n            self.log.info(\n                f\"Database {('would be ' if self.dry_run else '')}upgraded to: {revision}\"\n            )\n        except Exception as exc:\n            self.log.error(f\"({type(exc)}) Error during upgrade: {exc}\")\n            raise exc\n\n    def downgrade(self, revision: str = \"-1\") -&gt; None:\n        \"\"\"Downgrade the database, or simulate with --dry-run.\n\n        Args:\n            revision (str, optional): Revision to downgrade to. Defaults to \"-1\".\n\n        Raises:\n            Exception: If an error occurs during downgrade.\n        \"\"\"\n        log.info(\n            f\"{'Simulating' if self.dry_run else 'Downgrading'} to revision: {revision}\"\n        )\n\n        try:\n            if self.dry_run:\n                ## Get the current revision before proceeding\n                script = self.config.get_main_option(\"script_location\")\n\n                ## Create ScriptDirectory object\n                script_directory = ScriptDirectory.from_config(self.config)\n                ## Get current head\n                current_head = script_directory.get_current_head()\n\n                if not current_head:\n                    raise ValueError(\n                        \"Cannot determine the current revision for --sql mode.\"\n                    )\n\n                ## Target revision to downgrade to\n                downgrade_target = f\"{current_head}:{revision}\"\n            else:\n                downgrade_target = revision\n\n            if not self.dry_run:\n                ## Downgrade the database\n                command.downgrade(self.config, downgrade_target, sql=self.dry_run)\n\n            self.log.info(\n                f\"Database {('would be ' if self.dry_run else '')}downgraded to: {downgrade_target}\"\n            )\n        except Exception as exc:\n            self.log.error(f\"({type(exc)}) Error during downgrade: {exc}\")\n            raise exc\n\n    def create_migration(\n        self,\n        message: str = \"autogenerated migration\",\n    ):\n        \"\"\"Generate a new migration file.\n\n        Args:\n            message (str, optional): Message for the migration. Defaults to \"autogenerated migration\".\n            do_upgrade (bool, optional): Upgrade the database after creating the migration. Defaults to False.\n            dry_run (bool, optional): Simulate migration creation. Defaults to False.\n\n        Raises:\n            Exception: If an error occurs during migration creation.\n        \"\"\"\n        if self.dry_run:\n            self.log.info(\n                f\"[DRY-RUN] Would create migration with message: {message}{', and would upgrade the database' if self.do_upgrade else ''}\"\n            )\n            return\n\n        self.log.info(f\"Creating migration: {message}\")\n        try:\n            command.revision(self.config, message=message, autogenerate=True)\n            log.info(f\"Migration created: {message}\")\n        except Exception as exc:\n            self.log.error(f\"({type(exc)}) Error creating migration: {exc}\")\n            raise exc\n\n        if self.do_upgrade:\n            self.log.info(f\"Upgrading database after creating migration: {message}\")\n            if not self.dry_run:\n                self.upgrade(\"head\", dry_run=self.dry_run)\n\n\ndef parse_args() -&gt; argparse.Namespace:\n    \"\"\"Parse command-line arguments.\n\n    Returns:\n        argparse.Namespace: Parsed command-line arguments.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Alembic migration controller args.\")\n\n    ## Arg for alembic.ini config file path\n    parser.add_argument(\n        \"-f\",\n        \"--config-file\",\n        nargs=\"?\",\n        const=\"alembic.ini\",\n        default=\"alembic.ini\",\n        metavar=\"ALEMBIC_INI_FILE\",\n        help=\"Path to the Alembic configuration file (default: 'alembic.ini').\",\n    )\n\n    ## Arg for upgrading database\n    parser.add_argument(\n        \"-u\",\n        \"--upgrade\",\n        nargs=\"?\",\n        const=\"head\",\n        default=None,\n        metavar=\"REVISION\",\n        help=\"Upgrade database to a specific revision (default: 'head').\",\n    )\n\n    ## Arg for downgrading database\n    parser.add_argument(\n        \"-d\",\n        \"--downgrade\",\n        nargs=\"?\",\n        const=\"-1\",\n        default=None,\n        metavar=\"REVISION\",\n        help=\"Downgrade database to a specific revision (default: '-1').\",\n    )\n\n    ## Arg for creating a new migration\n    parser.add_argument(\n        \"-m\",\n        \"--migrate\",\n        nargs=\"?\",\n        const=\"autogenerated migration\",\n        metavar=\"MESSAGE\",\n        default=None,\n        help=\"Create a new migration file with the given message.\",\n    )\n\n    ## Arg for upgrading database after creating a migration\n    parser.add_argument(\n        \"-U\",\n        \"--do-upgrade\",\n        action=\"store_true\",\n        help=\"Upgrade database after creating a migration (only valid with -m).\",\n    )\n\n    ## Arg for simulating actions when true, instead of making actual changes\n    parser.add_argument(\n        \"--dry-run\",\n        action=\"store_true\",\n        help=\"Simulate the action without making actual changes.\",\n    )\n\n    args: argparse.Namespace = parser.parse_args()\n\n    ## If no arguments were passed, print help and exit\n    if not any([args.upgrade, args.downgrade, args.migrate]):\n        parser.print_help()\n        exit(1)\n\n    return args\n\n\ndef main(\n    args: argparse.Namespace,\n):\n    \"\"\"Main entry point for the script.\n\n    Args:\n        args (argparse.Namespace): Parsed command-line arguments.\n    \"\"\"\n    alembic_ini_file: str = args.config_file\n    log.debug(f\"Alembic configuration file: {alembic_ini_file}\")\n\n    if not Path(alembic_ini_file).exists():\n        raise FileNotFoundError(\n            f\"Could not find Alembic configuration file at '{alembic_ini_file}'\"\n        )\n\n    ## Initialize Alembic controller\n    alembic_controller: AlembicController = AlembicController(\n        alembic_ini_path=alembic_ini_file,\n        dry_run=args.dry_run,\n        do_upgrade=args.do_upgrade,\n    )\n    log.debug(f\"Alembic controller: {alembic_controller}\")\n\n    ## Enter alembic controller\n    try:\n        with alembic_controller as alembic_ctl:\n            if args.migrate:\n                alembic_ctl.create_migration(args.migrate)\n            elif args.upgrade is not None:\n                alembic_ctl.upgrade(args.upgrade)\n            elif args.downgrade is not None:\n                alembic_ctl.downgrade(args.downgrade)\n            else:\n                raise ValueError(\"Missing required arguments.\")\n    except Exception as exc:\n        msg = f\"({type(exc)}) Unhandled exception. Details: {exc}\"\n        log.error(msg)\n\n        raise exc\n\n\nif __name__ == \"__main__\":\n    ## Initialize logging\n    setup_logging(log_level=\"DEBUG\")\n\n    ## Get CLI args\n    args: argparse.Namespace = parse_args()\n\n    ## Run main function\n    main(args=args)\n</code></pre>","tags":["alembic","python","programming","database"]},{"location":"programming/python/alembic/index.html#alembic-errors-fixes","title":"Alembic errors &amp; fixes","text":"","tags":["alembic","python","programming","database"]},{"location":"programming/python/alembic/index.html#sqlite-alter-table-exception","title":"SQLite ALTER TABLE exception","text":"<p>SQLite has limited support for the <code>ALTER</code> operator. When working with Alembic/SQLAlchemy, certain operations, like changing a column's <code>nullable</code> value, will cause a <code>sqlalchemy.exc.OperationalError: (sqlite3.OperationalError)</code> exception:</p> <pre><code>sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) near \"ALTER\": syntax error\n[SQL: ALTER TABLE dispensary ALTER COLUMN address_line1 DROP NOT NULL]\n</code></pre> <p>When this happens, edit the failed revision script (the Python file in your <code>alembic/versions/&lt;revision_id&gt;_autogenerated_migration.py</code> or <code>migrations/versions/&lt;revision_id&gt;_autogenerated_migration.py</code> ). Change the <code>upgrade()</code> and <code>downgrade()</code> methods, use a <code>with</code> statement to perform a batch operation</p> <p>Tip</p> <p>The key difference is the <code>with op.batch_alter_table(\"table\") as batch_op:</code> line, which wraps multiple operations in a context manager to work around SQLite's <code>ALTER</code> limitation.</p> <p>Change lines like this:</p> <pre><code>def upgrade() -&gt; None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.alter_column(\n        \"table\", \"column_name\", existing_type=sa.TEXT(), nullable=True\n    )\n    op.alter_column(\n        \"table\", \"column_name\", existing_type=sa.TEXT(), nullable=True\n    )\n\n    ...\n</code></pre> <p>To this:</p> <pre><code>def upgrade() -&gt; None:\n    with op.batch_alter_table(\"table\") as batch_op:\n        batch_op.alter_column(\"column_name\", existing_type=sa.TEXT(), nullable=True)\n        batch_op.alter_column(\"column_name\", existing_type=sa.TEXT(), nullable=True)\n\n    ...\n</code></pre> <p>Before:</p> Alembic code that causes an exception with SQLite<pre><code>\"\"\"autogenerated migration\n\nRevision ID: 627799d6d073\nRevises: 51e9b4315e3a\nCreate Date: 2025-02-06 00:25:57.785482\n\n\"\"\"\n\nfrom typing import Sequence, Union\n\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision: str = \"627799d6d073\"\ndown_revision: Union[str, None] = \"51e9b4315e3a\"\nbranch_labels: Union[str, Sequence[str], None] = None\ndepends_on: Union[str, Sequence[str], None] = None\n\n\ndef upgrade() -&gt; None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.alter_column(\n        \"table\", \"column_name\", existing_type=sa.TEXT(), nullable=True\n    )\n    op.alter_column(\n        \"table\", \"column_name\", existing_type=sa.TEXT(), nullable=True\n    )\n    op.alter_column(\n        \"table\", \"column_name\", existing_type=sa.TEXT(), nullable=True\n    )\n    op.alter_column(\n        \"table\", \"column_name\", existing_type=sa.TEXT(), nullable=True\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -&gt; None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.alter_column(\n        \"table\", \"column_name\", existing_type=sa.TEXT(), nullable=False\n    )\n    op.alter_column(\n        \"table\", \"column_name\", existing_type=sa.TEXT(), nullable=False\n    )\n    op.alter_column(\n        \"table\", \"column_name\", existing_type=sa.TEXT(), nullable=False\n    )\n    op.alter_column(\n        \"table\", \"column_name\", existing_type=sa.TEXT(), nullable=False\n    )\n    # ### end Alembic commands ###\n</code></pre> <p>After:</p> Alembic SQLite batch operation<pre><code>\"\"\"autogenerated migration\n\nRevision ID: 627799d6d073\nRevises: 51e9b4315e3a\nCreate Date: 2025-02-06 00:25:57.785482\n\n\"\"\"\n\nfrom typing import Sequence, Union\n\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision: str = \"627799d6d073\"\ndown_revision: Union[str, None] = \"51e9b4315e3a\"\nbranch_labels: Union[str, Sequence[str], None] = None\ndepends_on: Union[str, Sequence[str], None] = None\n\n\ndef upgrade() -&gt; None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    with op.batch_alter_table(\"table\") as batch_op:\n        batch_op.alter_column(\"column_name\", existing_type=sa.TEXT(), nullable=True)\n        batch_op.alter_column(\"column_name\", existing_type=sa.TEXT(), nullable=True)\n        batch_op.alter_column(\"column_name\", existing_type=sa.TEXT(), nullable=True)\n        batch_op.alter_column(\n            \"column_name\", existing_type=sa.TEXT(), nullable=True\n        )\n        # ### end Alembic commands ###\n\n\ndef downgrade() -&gt; None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    with op.batch_alter_table(\"table\") as batch_op:\n        batch_op.alter_column(\n            \"column_name\", existing_type=sa.TEXT(), nullable=False\n        )\n        batch_op.alter_column(\n            \"column_name\", existing_type=sa.TEXT(), nullable=False\n        )\n        batch_op.alter_column(\"column_name\", existing_type=sa.TEXT(), nullable=False)\n        batch_op.alter_column(\"column_name\", existing_type=sa.TEXT(), nullable=False)\n        # ### end Alembic commands ###\n</code></pre>","tags":["alembic","python","programming","database"]},{"location":"programming/python/dynaconf/index.html","title":"Dynaconf","text":"<p>Dynaconf is a tool for managing app configurations. The tool is inspired by the 12-factor application guide, and is focused on enabling separation of your app's configuration from the business logic.</p> <p>Tip</p> <p>If you just want to see Dynaconf in action, you can skip to the Example app section.</p>","tags":["python","dynaconf","configuration","environment"]},{"location":"programming/python/dynaconf/index.html#providing-configurations-to-dynaconf","title":"Providing Configurations to Dynaconf","text":"<p>Dynaconf is very flexible, and can read configurations from a number of formats (<code>.toml</code>, <code>.json</code>, <code>.yaml</code>, <code>.env</code>), and from the environment itself. The Dynaconf documentation covers different methods of loading environment variables, but the flow I've settled on is defining <code>.toml</code> settings and secrets files in a <code>config/</code> directory, breaking the settings into environments (<code>[dev]</code>, <code>[rc]</code>, <code>[prod]</code>) and creating individual <code>Dynaconf()</code> settings objects in code for each configuration domain (<code>app</code>, <code>logging</code>, <code>database</code>, etc). This sentence will make more sense as you read on.</p> <p>Dynaconf reads variables in the following order (lower number = higher precedence/loaded earlier):</p> Precedence Location 0 The environment/CLI args 1 <code>settings*.toml</code>/<code>.secrets*.toml</code> files 2 Default values defined in <code>[default]</code> sections, or in code 3 Global defaults, i.e. <code>null</code>/<code>None</code> or a value you've configured as a default in code. <ul> <li>The environment, or from CLI args<ul> <li>You can set env variables in your environment (<code>export VAR_NAME=value</code> on Linux, <code>$env:VAR_NAME = 'value'</code> on Windows), but note that you should prepend the variable name with <code>DYNACONF_</code> for Dynaconf to detect it.<ul> <li>You can sometimes get away with changing the <code>envvar_prefix=</code> portion of a <code>Dynaconf()</code> instantiation, like <code>Dynaconf(envvar_prefix=\"&lt;prefix like LOG or DB&gt;\")</code>, but to reliably read a variable from the environment with Dynaconf, you should set <code>DYNACONF_</code> before the variable name.</li> <li>For example, if you have an environment variable named <code>LOG_LEVEL</code>, you would define it like: <code>export DYNACONF_LOG_LEVEL=...</code>.</li> <li>When Dynaconf reads the environment variable, it uses the <code>DYNACONF_</code> prefix to detect the variable it should load, and then drops the <code>DYNACONF_</code> prefix once loaded.<ul> <li>For example, <code>DYNACONF_LOG_LEVEL=...</code> becomes <code>LOG_LEVEL</code> in the code.</li> </ul> </li> </ul> </li> <li>You can also prepend a Python command with variables for Dynaconf to load, like:<ul> <li><code>LOG_LEVEL='DEBUG' python app.py</code></li> <li>Or, for more durability, <code>DYNACONF_LOG_LEVEL='DEBUG' python app.py</code></li> </ul> </li> </ul> </li> <li><code>.secrets*.toml</code>, <code>settings*.toml</code>, <code>.secrets*.json</code>, <code>settings*.json</code>, etc<ul> <li>The <code>*</code> in each settings file above indicates <code>dynaconf</code> will read the <code>settings.local.toml</code>/<code>settings.local.json</code> version of the file, if it exists, before trying to read from <code>settings.toml</code>/<code>settings.json</code>.</li> </ul> </li> <li>Default values<ul> <li>When retrieving a value from a <code>Dynaconf()</code> object, you can set a default value, which is 3rd in precedence:<ul> <li>In a <code>[default]</code> environment in one of your configuration <code>.toml</code> files</li> <li>Accessing in code with an initialized Dynaconf object:  <code>DYNACONF_SETTINGS_OBJECT.get(\"ENV_VAR_NAME\", default=\"The default value\")</code></li> </ul> </li> </ul> </li> <li>Global defaults:<ul> <li>If no value can be determined using a method above, <code>dynaconf</code> will try to use global defaults as a fallback, i.e. <code>null</code>/<code>None</code> or any value you've configured as a default in your code.</li> </ul> </li> </ul>","tags":["python","dynaconf","configuration","environment"]},{"location":"programming/python/dynaconf/index.html#environment-variables","title":"Environment Variables","text":"<p>Dynaconf reads from the environment first. You can set environment variables on your host (search online for 'how to set environment variable on [Windows|Mac|Linux]' to see how to do this for your specific environment), or if you're running in Docker, with the <code>environment:</code> section of a <code>compose.yml</code> file, or using <code>ENV VAR_NAME=Value</code> in a <code>Dockerfile</code>.</p> <p>When you define environment variables for Dynaconf to read, you should prepend the variable name with <code>DYNACONF_</code>. This prefix catches Dynaconf's attention right away and ensure that the value is read. If you are expecting a variable named, for example, <code>LOG_LEVEL</code>, you would set the environment variable <code>DYNACONF_LOG_LEVEL</code>, and the value will be accessible in a <code>Dynaconf()</code> object as <code>LOG_LEVEL</code>.</p> <p>Note that the <code>DYNACONF_</code> prefix is not needed when retrieving a value Dynaconf has already loaded, it's only necessary for telling Dynaconf to load that value in the first place.</p> <p>Tip</p> <p>You can play around with the <code>envvar_prefix=</code> portion of a <code>Dynaconf()</code> settings object, but I recommend getting into the habit of using the <code>DYNACONF_</code> prefix. After much trial and error on my end, this is the most durable way to ensure Dynaconf reads your configuration.</p> <p>Here are some example environment variables I might set in a Python app I'm writing:</p> Example environment variables<pre><code>## If I'm using Dynaconf's environments to separate by production, dev, etc\n#  Set a variable in the environment, like export ENV_FOR_DYNACONF=dev (Linux)\n#  or $env:ENV_FOR_DYNACONF=\"dev\" (Windows)\nENV_FOR_DYNACONF=\"dev\"\n\n## Configure my logging level dynamically\nDYNACONF_LOG_LEVEL=\"DEBUG\"  # Dynaconf reads this variable as 'LOG_LEVEL'\n\n## Configure a max HTTP timeout on requests\nDYNACONF_HTTP_TIMEOUT=15\n</code></pre>","tags":["python","dynaconf","configuration","environment"]},{"location":"programming/python/dynaconf/index.html#setting-file-toml-json-yaml-env","title":"Setting File (.toml, .json, .yaml, .env)","text":"<p>Dynaconf can read from many file formats, but my preferred format (and the one you will see most often in the documentation and in examples online) is the <code>.toml</code> format. This guide assumes you are using <code>.toml</code> files for configuration.</p> <p>When running in Production, you should use environment variables for configuring your app. These <code>.toml</code> file examples are great for local development, and you could provide a Production app with a Production <code>settings.toml</code> file to read, but configuring with environment variables is the best way to load your environment variables into an app.</p> <p>Warning</p> <p>This guide uses the term \"environment variables\" to describe non-sensitive app configurations, like the timezone or logging level. For local development, it is acceptable to use a <code>.secrets.toml</code> file, but in Production you should always load from a secure vault of some sort. Leaving passwords &amp; other secrets in plain text anywhere in the environment is bad practice, and should be avoided even when self-hosting.</p> <p>Dynaconf will start reading at the current path <code>./</code> for <code>.toml</code> files, and if no <code>settings.toml</code>/<code>settings.local.toml</code> file is found, will look for a <code>config/</code> directory.</p> <p>I put my <code>.toml</code> configuration files in a <code>config/</code> directory because I tend to create separate files for different parts of the app (<code>config/settings.toml</code> for my app's configuration like logging, <code>config/database/settings.toml</code> for database settings, etc). This guide will assume your <code>settings.toml</code> file(s) exist in a <code>config/</code> directory at the repository root.</p> <p>There are a number of ways to write a <code>settings.toml</code> file. This is a simple example of a valid configuration file:</p> Example settings.toml file for Dynaconf<pre><code>log_level = \"DEBUG\"\n</code></pre> <p>You can also add \"environments\" to a settings file, and tell Dynaconf to read from a specific environment when it loads the configuration. Environments are created with <code>[brackets]</code>, and you should provide a <code>[default]</code> environment where you set all of your variables and a default value:</p> Example settings.toml with app environments<pre><code>[default]\n## Values defined in this section will be used when no ENV_FOR_DYNACONF value is set\nlog_level = \"INFO\"\n\n[dev]\n## Load this section when ENV_FOR_DYNACONF=\"dev\"\n#  Dynaconf will use this value before the default value, if it's defined\nlog_level = \"DEBUG\"\n\n[rc]\n## Load this section when ENV_FOR_DYNACONF=\"rc\"\n#  Omit the log_level to use the default INFO in production\n\n[prod]\n## Load this secetion when ENV_FOR_DYNACONF=\"prod\"\n#  Show only warning &amp; error messages in Production\nlog_level = \"WARNING\n</code></pre> <p>You can also use environments to define arbitrary environments like <code>[cli]</code> for running in a pipeline, or <code>[container]</code> when running in a container environment.</p> <p>When using <code>production</code>/<code>dev</code> environments like this, you must set an <code>ENV_FOR_DYNACONF={dev,prod,testing,etc}</code>, where the value matches an <code>[environment]</code> in your settings file. This will tell Dynaconf to only load values from the matching <code>[environment]</code>.</p>","tags":["python","dynaconf","configuration","environment"]},{"location":"programming/python/dynaconf/index.html#secrets-file","title":"Secrets file","text":"<p>For local development, you can also store secrets (API keys, passwords, etc) in a <code>.secrets.toml</code> file. You can commit this file to git, because you should be creating a <code>.secrets.local.toml</code> file to override the defaults you set in <code>.secrets.toml</code>.</p> <p>DO NOT COMMIT THE <code>*.local.toml</code> VERSION OF YOUR SECRETS FILE TO GIT! You should put real values in <code>.secrets.local.toml</code>, and you do not want to track those in version control!</p> <p>Declaring secrets is exactly the same as writing a <code>settings.toml</code> file. You do not even have to separate these configurations, but it is recommended to separate secrets from configurations for cleanliness and separation of concerns.</p> <p>Like your <code>settings.toml</code> file, you should put your \"actual\" configuration in a non-source controlled <code>.secrets.local.toml</code>. Dynaconf knows to read the <code>.local.toml</code> version of a config before the version without <code>.local</code> in the name, you do not need to do anything special for this functionality:</p> Example .secrets.toml file<pre><code>## .secrets.local.toml\n[default]\n## 'service' here could be anything; if you're adding a key to WeatherAPI for example,\n#  you could name this value 'weatherapi_api_key'\nservice_api_key = \"\"\n\n[dev]\nservice_api_key = \"hgib1n5g-l159nruo-b083n34k\"\n\n[rc]\nservice_api_key = \"hgib1n5g-l159nruo-b083n34k\"\n\n[production]\n## Use a different key for Production\nservice_api_key = \"lgborne-giotnri2-9bf0njl0\"\n</code></pre>","tags":["python","dynaconf","configuration","environment"]},{"location":"programming/python/dynaconf/index.html#reading-configurations-with-dynaconf","title":"Reading Configurations with Dynaconf","text":"<p>Look at this code and keep it in mind as you read through the rest of the examples; this is one way to load environment variables with Dynaconf in code. I prefer this method using an initialized Dynaconf configuration and the <code>.get()</code> method because you can set a default value if no environment variable is detected:</p> Retrieving env variables with Dynaconf<pre><code>## Initialize a Dynaconf() settings object\nDYNACONF_SETTINGS = Dynaconf(\n    environments=True,\n    envvar_prefix=\"DYNACONF\",\n    settings_files=[\"settings.toml\", \".secrets.toml\"]\n)\n\n## Access a value defined in the environment as DYNACONF_VAR_NAME\n#  If no DYNACONF_VAR_NAME value is detected, set it to 'Default Value'\nDYNACONF_SETTINGS.get(\"VAR_NAME\", default=\"Default Value\")\n</code></pre> <p>We will go into more detail on the code above in another section, but the simple way of describing what is happening above is that your code is reading an initialized <code>Dynaconf()</code> object you named <code>DYNACONF_SETTINGS</code> for an environment variable named <code>VAR_NAME</code>, and setting a value of <code>\"Default Value\"</code> if the environment variable is not found.</p>","tags":["python","dynaconf","configuration","environment"]},{"location":"programming/python/dynaconf/index.html#the-dynaconf-object","title":"The Dynaconf() object","text":"<p>After defining your configurations in the environment or in a supported file, you need to create a <code>Dynaconf()</code> object to load those settings. This is where Dynaconf's flexibility really comes into play. You can create a single <code>Dynaconf()</code> settings object for the entire app, or you can use <code>envvar_prefix=</code> to \"scope\" your configurations.</p> <p>Continue reading for an example of defining a <code>settings.toml</code> file, initializing a <code>Dynaconf()</code> object, and reading the configuration defined in the <code>settings.toml</code> file.</p>","tags":["python","dynaconf","configuration","environment"]},{"location":"programming/python/dynaconf/index.html#environment-tags-as-configuration-domains","title":"Environment Tags as Configuration Domains","text":"<p>Example <code>settings.toml</code> file:</p> settings.toml<pre><code>[default]\nlog_level = \"INFO\"\n\n[dev]\nlog_level = \"DEBUG\"\n\n[rc]\n\n[prod]\nlog_level = \"WARNING\"\n</code></pre> Load settings with Dynaconf<pre><code>from dynaconf import Dynaconf\n\n## Initialize a Dynaconf object to store logging config.\n#  Dynaconf will check the environment for any DYNACONF_LOG_xxx value first, then\n#  read settings.local.toml, then settings.toml for a log_level value\nLOGGING_SETTINGS = Dynaconf(\n    ## Enable reading [environment] tags\n    environments=True,\n    ## Tell Dynaconf to read environment variables starting with DYNACONF_LOG or LOG_\n    envvar_prefix=\"LOG\",\n    ## Tell Dynaconf to look for ./settings.toml, ./settings.local.toml, or config/settings[.local].toml,\n    #  and the same for .secrets[.local].toml or config/.secrets[.local].toml\n    settings_files=[\"settings.toml\", \".secrets.toml\"]\n)\n\n## This should return 'DEBUG' because of the configuration in the settings.toml file\n#  If the log_level variable is not set in the environment or a settings.toml file, default to '\"INFO\"'\nprint(LOGGING_SETTINGS.get(\"LOG_LEVEL\", default=\"INFO\"))\n</code></pre> <p>If you are using Dynaconf environments to separate values into different environments like <code>dev</code>, <code>prod</code>, etc, it is advisable to break the <code>.toml</code> settings files into domain-specific configurations, like <code>settings.logging.toml</code>/<code>settings.logging.local.toml</code>, or in separate subdirectories like <code>config/logging/{settings,.secrets}.toml</code>. You can set the <code>[environment]</code> Dynaconf should read from using the <code>ENV_FOR_DYNACONF</code> environment variable; to read the <code>[dev]</code> configuration, you would set <code>ENV_FOR_DYNACONF=\"dev\"</code> in your environment. Dynaconf is built to be very flexible, and allows for separating configurations by domain (app, logging, http_server, for example).</p> <p>You can also store all of your configuration domains in a single <code>settings.toml</code> file, using variable prefixes like <code>log_</code> and <code>api_</code> to separate configuration domains.</p> Domain-specific 'logging' settings file<pre><code>## config/logging/settings.toml\n[default]\nlog_level = \"INFO\"\napi_key = \"\"\n\n[dev]\nlog_level = \"DEBUG\"\napi_key = \"bntkq2wo-gbbab340-olkjb2ti\"\n\n[rc]\nlog_level = \"INFO\"\napi_key = \"kgbnaorjb-nosbn234-oab564ag\"\n\n[prod]\nlog_level = \"WARNING\"\napi_key = \"ljatbo23-b007a43-alkjatph\"\n\n## You can configure a lower/higher (depending on how you understand log levels)\n#  level when ENV_FOR_DYNACONF=\"ci\", like in a pipeline\n[ci]\nlog_level = \"VERBOSE\"\n## CI environment in this example does not require an API key,\n#  so it is omitted and defaults to \"\"\n</code></pre>","tags":["python","dynaconf","configuration","environment"]},{"location":"programming/python/dynaconf/index.html#gitignore","title":"Gitignore","text":"<p>You should commit your <code>settings.toml</code> and <code>.secrets.toml</code> file(s) to git, but do not put your \"real\" values in these files. Instead, use default or empty values, and create <code>settings.local.toml</code> and <code>.secrets.local.toml</code> files, which should not be committed to git.</p> <p>Here's an example <code>.gitignore</code> that will ignore all <code>.local.toml</code> configurations:</p> Example .gitignore for Dynaconf<pre><code>## .gitignore\n\n## Dynaconf\n**/*.local.toml\n\n!**/*.example.toml\n!**/.*.example.toml\n</code></pre>","tags":["python","dynaconf","configuration","environment"]},{"location":"programming/python/dynaconf/index.html#example-app","title":"Example app","text":"<p>Here you can see a simple example of configuring an app with Dynaconf. The app will make a request to the Faker API, with values configured from the environment.</p> <p>I am using HTTPX as my request client because I prefer it over the <code>requests</code> package.</p> <p>This example defines environments for <code>[dev]</code>, <code>[rc]</code>, and <code>[production]</code>. To switch between the environments, set an environment variable <code>ENV_FOR_DYNACONF</code> to a value matching one of these <code>[environments]</code>.</p> <p>Example (Linux): <code>export ENV_FOR_DYNACONF=dev</code></p> <p>Example (Windows): <code>$env:ENV_FOR_DYNACONF=\"dev\"</code></p>","tags":["python","dynaconf","configuration","environment"]},{"location":"programming/python/dynaconf/index.html#settings-files","title":"Settings files","text":"<p>First I'll create a <code>config/</code> directory, and a <code>settings.toml</code> and <code>.secrets.toml</code> file. These will be committed to source control, and I will set default values here with the idea that when I clone this repository on a new machine, I will manually create a <code>settings.local.toml</code> and <code>.secrets.local.toml</code> file, copying the contents of these 2 source controlled files into the ignored <code>.local</code> versions and modifying them for local execution.</p> Dynaconf settings.toml file<pre><code>## settings.toml\n[default]\nlog_level = \"INFO\"\n\nfakerapi_base_url = \"https://fakerapi.it/api/v2\"\nfakerapi_endpoint = \"addresses\"\nfakerapi_quantity = 1\n\n[dev]\nlog_level = \"DEBUG\"\n## I am not updating the base URL. Leaving this commented will default to the value\n#  in the [default] section above\n# fakerapi_base_url = \"\"\n## Use the /books endpoint instead of /addresses\nfakerapi_endpoint = \"books\"\n\n[rc]\nlog_level = \"WARNING\"\n## Ask for 3 addresses instead of 1\nfakerapi_quantity = 3\n</code></pre> <p>The FakerAPI does not require an API key, but if it did, I would also declare it in my <code>.secrets.toml</code> file:</p> Dynaconf .secrets.toml file<pre><code>## .secrets.toml\n[default]\nfakerapi_api_key = \"\"\n\n[prod]\n## Put a placeholder API key for the user to replace when they create a\n#  .secrets.local.toml file\nfakerapi_api_key = \"&lt;your-fakerapi-api-key&gt;\"\n</code></pre> <p>After creating these 2 files and adding them to source control, I will copy them to the <code>.local</code> version and edit them:</p> Copy source controlled settings files to local dev versions<pre><code>cp config/settings.toml config/settings.local.toml\ncp config/.secrets.toml config/.secrets.local.toml\n</code></pre> <p>Again, these <code>.local.toml</code> versions of the settings files should not be committed to git. You will create these files locally on fresh clones and use them for local testing (i.e. not in a Docker container, where you can just set the values as environment variables).</p> <p>To tell my app which configuration environment to use (dev, rc, or prod), set an an environment variable called <code>ENV_FOR_DYNACONF</code>:</p> Set Dynaconf env on Linux/Mac<pre><code>export ENV_FOR_DYNACONF=dev\n</code></pre> Set Dynaconf env on Windows<pre><code>$env:ENV_FOR_DYNACONF=\"dev\"\n</code></pre>","tags":["python","dynaconf","configuration","environment"]},{"location":"programming/python/dynaconf/index.html#dynaconf-object","title":"Dynaconf() object","text":"<p>Now I'll create a file named <code>settings.py</code>, where I will load these configurations. Dynaconf is incredibly flexible in this regard; you can put your <code>Dynaconf()</code> objects anywhere, they can be alongside your business logic, in a separate file so you can import them throughout the app, you can define multiple <code>Dynaconf()</code> objects in a single Python project to load different sections of your settings/environment, etc.</p> <p>I am keeping things simple for this example by creating a <code>settings.py</code> file, where I will initialize all of my <code>Dynaconf()</code> objects so I can import them in other scripts.</p> Example settings.py script that loads configurations from the environment/settings.toml file<pre><code>## settings.py\n\nfrom dynaconf import Dynaconf\n\n## Initialize a logging config object.\n#  This object will only load variables beginning with DYNACONF_LOG_ or LOG_ from\n#  the settings.toml/.secrets.toml file, and the [env] matching your ENV_FOR_DYNACONF value\nLOGGING_SETTINGS = Dynaconf(\n    environments=True,\n    envvar_prefix=\"LOG\",\n    settings_files=[\"settings.toml\", \".secrets.toml\"]\n)\n\n## Initialize FakerAPI config object.\n#  This object will only load variables beginning with DYNACONF_FAKERAPI_ or FAKERAPI_ from\n#  the settings.toml/.secrets.toml file, and the [env] matching your ENV_FOR_DYNACONF value\nFAKERAPI_SETTINGS = Dynaconf(\n    environments=True,\n    envvar_prefix=\"FAKERAPI\",\n    settings_files=[\"settings.toml\", \".secrets.toml\"]\n)\n</code></pre>","tags":["python","dynaconf","configuration","environment"]},{"location":"programming/python/dynaconf/index.html#python-app","title":"Python app","text":"<p>Now, in a <code>main.py</code>, I will configure my logging from the <code>LOGGING_SETTINGS</code> object, and make a request to the Faker API.</p> Example app that uses Dynaconf to configure itself<pre><code>## main.py\nimport logging\nfrom settings import LOGGING_SETTINGS, FAKERAPI_SETTINGS\nimport httpx\n\n## Initialize a logger\nlog = logging.getLogger(__name__)\n\nif __name__ == \"__main__\":\n    ## Set the log level from the environment. If no LOG_LEVEL/DYNACONF_LOG_LEVEL is detected,\n    #  default to \"INFO\"\n    logging.basicConfig(level=LOGGING_SETTINGS.get(\"LOG_LEVEL\", default=\"INFO\"))\n\n    ## Build the request URL from your settings.\n    #  Note that I'm omitting 'default=' from the .get(). This means if the\n    #  value isn't found in the environment or TOML settings files, the value\n    #  will be None.\n    url: str = f\"{FAKERAPI_SETTINGS.get('FAKERAPI_BASE_URL')}/{FAKERAPI_SETTINGS.get('FAKERAPI_ENDPOINT')}?_quantity={FAKERAPI_SETTINGS.get('FAKERAPI_QUANTITY')}\"\n    params = {\"_quantity\": FAKERAPI_SETTINGS.get(\"FAKERAPI_QUANTITY\")}\n\n    req = httpx.Request(method=\"GET\", url=url, params=params)\n\n    ## Send a request to https://fakerapi.it/api/v2/books?_quantity=3\n    #  The base URL, endpoint, and _quantity param are loaded with Dynaconf\n    res = httpx.send(req)\n</code></pre>","tags":["python","dynaconf","configuration","environment"]},{"location":"programming/python/fastapi/index.html","title":"FastAPI","text":"<p>...</p>","tags":["python","fastapi"]},{"location":"programming/python/fastapi/index.html#common-fixes","title":"Common Fixes","text":"<p>...</p>","tags":["python","fastapi"]},{"location":"programming/python/fastapi/index.html#fix-failed-to-load-api-definition-not-found-apiv1openapijson","title":"Fix: \"Failed to load API Definition. Not found /api/v1/openapi.json\"","text":"<p>When running behind a proxy (and in some other circumstances) you will get an error when trying to load the <code>/docs</code> endpoint. The error will say \"Failed to load API definition. Fetch error Not Found /api/v1/openapi.json\":</p> <p></p> <p>To fix this, simply modify your <code>app = FastAPI()</code> line, adding `openapi_url=\"/docs/openapi\":</p> fastapi main.py<pre><code>...\n\n# app = FastAPI(openapi_url=\"/docs/openapi\")\napp = FastAPI(openapi_url=\"/docs/openapi\")\n</code></pre>","tags":["python","fastapi"]},{"location":"programming/python/nox/index.html","title":"Nox","text":"<p><code>nox</code> is a very useful utility for running tasks in a project. By creating a <code>noxfile.py</code> and adding <code>nox</code> sessions to it, you can automate tasks like building the project, exporting <code>requirements.txt</code>, and linting code (and more).</p> <p><code>nox</code> is versatile. If you can run a command in your shell, you should be able to automate it with <code>nox</code>. The idea is that <code>nox</code> sessions can run the same way in any environment (local, CI/CD pipelines, and cross-platform). For example, my <code>Ansible homelab</code> repository uses a <code>noxfile.py</code> to automate running Ansible playbooks.</p> <p>Note</p> <p>Check the documentation for making your <code>noxfile.py</code> modular to keep your <code>noxfile.py</code> short &amp; clean by import sessions from a <code>nox_extra/</code> directory in your project.</p>","tags":["standard-project-files","python","nox"]},{"location":"programming/python/nox/index.html#noxfilepy-base","title":"noxfile.py base","text":"<p>The basis for most/all of my projects' <code>noxfile.py</code>. </p> <p>Note</p> <p>If running all sessions with <code>$ nox</code>, only the sessions defined in <code>nox.sessions</code> will be executed. The list of sessions is conservative to start in order to maintain as generic a <code>nox</code> environment as possible.</p> <p>Enabled sessions:</p> <ul> <li>lint</li> <li>export</li> <li>tests</li> </ul> noxfile.py<pre><code>from __future__ import annotations\n\nimport logging\nimport logging.config\nimport logging.handlers\nimport os\nfrom pathlib import Path\nimport platform\nimport shutil\nimport importlib.util\n\nimport nox\n\n## Set nox options\nif importlib.util.find_spec(\"uv\"):\n    nox.options.default_venv_backend = \"uv|virtualenv\"\nelse:\n    nox.options.default_venv_backend = \"virtualenv\"\nnox.options.reuse_existing_virtualenvs = True\nnox.options.error_on_external_run = False\nnox.options.error_on_missing_interpreters = False\n# nox.options.report = True\n\n## Define sessions to run when no session is specified\nnox.sessions = [\"lint\", \"export\", \"tests\"]\n\n## Detect container env, or default to False\nif \"CONTAINER_ENV\" in os.environ:\n    CONTAINER_ENV: bool = os.environ[\"CONTAINER_ENV\"]\nelse:\n    CONTAINER_ENV: bool = False\n\n## Create logger for this module\nlog: logging.Logger = logging.getLogger(\"nox\")\n\n## Define versions to test\nPY_VERSIONS: list[str] = [\"3.12\", \"3.11\"]\n## Get tuple of Python ver ('maj', 'min', 'mic')\nPY_VER_TUPLE: tuple[str, str, str] = platform.python_version_tuple()\n## Dynamically set Python version\nDEFAULT_PYTHON: str = f\"{PY_VER_TUPLE[0]}.{PY_VER_TUPLE[1]}\"\n\n## Set PDM version to install throughout\nPDM_VER: str = \"2.15.4\"\n## Set paths to lint with the lint session\nLINT_PATHS: list[str] = [\"src\", \"tests\"]\n\n## Set directory for requirements.txt file output\nREQUIREMENTS_OUTPUT_DIR: Path = Path(\"./requirements\")\n\n\ndef setup_nox_logging(\n    level_name: str = \"DEBUG\", disable_loggers: list[str] | None = []\n) -&gt; None:\n    \"\"\"Configure a logger for the Nox module.\n\n    Params:\n        level_name (str): The uppercase string representing a logging logLevel.\n        disable_loggers (list[str] | None): A list of logger names to disable, i.e. for 3rd party apps.\n            Note: Disabling means setting the logLevel to `WARNING`, so you can still see errors.\n\n    \"\"\"\n    ## If container environment detected, default to logging.DEBUG\n    if CONTAINER_ENV:\n        level_name: str = \"DEBUG\"\n\n    logging_config: dict = {\n        \"version\": 1,\n        \"disable_existing_loggers\": False,\n        \"loggers\": {\n            \"nox\": {\n                \"level\": level_name.upper(),\n                \"handlers\": [\"console\"],\n                \"propagate\": False,\n            }\n        },\n        \"handlers\": {\n            \"console\": {\n                \"class\": \"logging.StreamHandler\",\n                \"formatter\": \"nox\",\n                \"level\": \"DEBUG\",\n                \"stream\": \"ext://sys.stdout\",\n            }\n        },\n        \"formatters\": {\n            \"nox\": {\n                \"format\": \"[NOX] [%(asctime)s] [%(levelname)s] [%(name)s]: %(message)s\",\n                \"datefmt\": \"%Y-%m-%D %H:%M:%S\",\n            }\n        },\n    }\n\n    ## Configure logging. Only run this once in an application\n    logging.config.dictConfig(config=logging_config)\n\n    ## Disable loggers by name. Sets logLevel to logging.WARNING to suppress all but warnings &amp; errors\n    for _logger in disable_loggers:\n        logging.getLogger(_logger).setLevel(logging.WARNING)\n\n\ndef append_lint_paths(search_patterns: str | list[str] = None, lint_paths: list[str] = None):\n    if lint_paths is None:\n        lint_paths = []\n\n    if search_patterns is None:\n        return lint_paths\n\n    if isinstance(search_patterns, str):\n        search_patterns = [search_patterns]\n\n    for pattern in search_patterns:\n        for path in Path('.').rglob(pattern):\n            relative_path = Path('.').joinpath(path).resolve().relative_to(Path('.').resolve())\n\n            if f\"{relative_path}\" not in lint_paths:\n                lint_paths.append(f\"./{relative_path}\")\n\n    log.debug(f\"Lint paths: {lint_paths}\")\n    return lint_paths\n\n\nsetup_nox_logging()\n\nlog.info(f\"[container_env:{CONTAINER_ENV}]\")\n\n## Ensure REQUIREMENTS_OUTPUT_DIR path exists\nif not REQUIREMENTS_OUTPUT_DIR.exists():\n    try:\n        REQUIREMENTS_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    except Exception as exc:\n        msg = Exception(\n            f\"Unable to create requirements export directory: '{REQUIREMENTS_OUTPUT_DIR}'. Details: {exc}\"\n        )\n        log.error(msg)\n\n        REQUIREMENTS_OUTPUT_DIR: Path = Path(\".\")\n\n## List of dicts describing a src file to copy to a specified destination file\n#  Ex: {\"src\": \"config/.secrets.example.toml\", \"dest\": \"config/.secrets.toml\"}\nINIT_COPY_FILES: list[dict[str, str]] = []\n\n\n@nox.session(python=PY_VERSIONS, name=\"build-env\", tags=[\"env\", \"build\", \"setup\"])\n@nox.parametrize(\"pdm_ver\", [PDM_VER])\ndef setup_base_testenv(session: nox.Session, pdm_ver: str):\n    log.debug(f\"Default Python: {DEFAULT_PYTHON}\")\n    session.install(f\"pdm&gt;={pdm_ver}\")\n\n    log.info(\"Installing dependencies with PDM\")\n    session.run(\"pdm\", \"sync\")\n    session.run(\"pdm\", \"install\")\n\n\n@nox.session(python=[DEFAULT_PYTHON], name=\"lint\", tags=[\"quality\"])\ndef run_linter(session: nox.Session):\n    session.install(\"ruff\")\n    session.install(\"black\")\n\n    log.info(\"Linting code\")\n    for d in LINT_PATHS:\n        if not Path(d).exists():\n            log.warning(f\"Skipping lint path '{d}', could not find path\")\n            pass\n        else:\n            lint_path: Path = Path(d)\n            log.info(f\"Running ruff imports sort on '{d}'\")\n            session.run(\n                \"ruff\",\n                \"check\",\n                lint_path,\n                \"--select\",\n                \"I\",\n                \"--fix\",\n            )\n\n            log.info(f\"Formatting '{d}' with Black\")\n            session.run(\n                \"black\",\n                lint_path,\n            )\n\n            log.info(f\"Running ruff checks on '{d}' with --fix\")\n            session.run(\n                \"ruff\",\n                \"check\",\n                lint_path,\n                \"--fix\",\n            )\n\n    log.info(\"Linting noxfile.py\")\n    session.run(\n        \"ruff\",\n        \"check\",\n        f\"{Path('./noxfile.py')}\",\n        \"--fix\",\n    )\n\n\n@nox.session(python=[DEFAULT_PYTHON], name=\"export\", tags=[\"requirements\"])\n@nox.parametrize(\"pdm_ver\", [PDM_VER])\ndef export_requirements(session: nox.Session, pdm_ver: str):\n    session.install(f\"pdm&gt;={pdm_ver}\")\n\n    log.info(\"Exporting production requirements\")\n    session.run(\n        \"pdm\",\n        \"export\",\n        \"--prod\",\n        \"-o\",\n        f\"{REQUIREMENTS_OUTPUT_DIR}/requirements.txt\",\n        \"--without-hashes\",\n    )\n\n    log.info(\"Exporting development requirements\")\n    session.run(\n        \"pdm\",\n        \"export\",\n        \"-d\",\n        \"-o\",\n        f\"{REQUIREMENTS_OUTPUT_DIR}/requirements.dev.txt\",\n        \"--without-hashes\",\n    )\n\n\n@nox.session(python=PY_VERSIONS, name=\"tests\", tags=[\"test\"])\n@nox.parametrize(\"pdm_ver\", [PDM_VER])\ndef run_tests(session: nox.Session, pdm_ver: str):\n    session.install(f\"pdm&gt;={pdm_ver}\")\n    session.run(\"pdm\", \"install\")\n\n    log.info(\"Running Pytest tests\")\n    session.run(\n        \"pdm\",\n        \"run\",\n        \"pytest\",\n        \"-n\",\n        \"auto\",\n        \"--tb=auto\",\n        \"-v\",\n        \"-rsXxfP\",\n    )\n\n\n@nox.session(python=PY_VERSIONS, name=\"pre-commit-all\", tags=[\"repo\", \"pre-commit\"])\ndef run_pre_commit_all(session: nox.Session):\n    session.install(\"pre-commit\")\n    session.run(\"pre-commit\")\n\n    log.info(\"Running all pre-commit hooks\")\n    session.run(\"pre-commit\", \"run\")\n\n\n@nox.session(python=PY_VERSIONS, name=\"pre-commit-update\", tags=[\"repo\", \"pre-commit\"])\ndef run_pre_commit_autoupdate(session: nox.Session):\n    session.install(f\"pre-commit\")\n\n    log.info(\"Running pre-commit autoupdate\")\n    session.run(\"pre-commit\", \"autoupdate\")\n\n\n@nox.session(python=PY_VERSIONS, name=\"pre-commit-nbstripout\", tags=[\"repo\", \"pre-commit\"])\ndef run_pre_commit_nbstripout(session: nox.Session):\n    session.install(f\"pre-commit\")\n\n    log.info(\"Running nbstripout pre-commit hook\")\n    session.run(\"pre-commit\", \"run\", \"nbstripout\")\n\n\n@nox.session(python=[PY_VER_TUPLE], name=\"init-setup\", tags=[\"setup\"])\ndef run_initial_setup(session: nox.Session):\n    log.info(f\"Running initial setup.\")\n    if INIT_COPY_FILES is None:\n        log.warning(f\"INIT_COPY_FILES is empty. Skipping\")\n        pass\n\n    else:\n\n        for pair_dict in INIT_COPY_FILES:\n            src = Path(pair_dict[\"src\"])\n            dest = Path(pair_dict[\"dest\"])\n            if not dest.exists():\n                log.info(f\"Copying {src} to {dest}\")\n                try:\n                    shutil.copy(src, dest)\n                except Exception as exc:\n                    msg = Exception(\n                        f\"Unhandled exception copying file from '{src}' to '{dest}'. Details: {exc}\"\n                    )\n                    log.error(msg)\n</code></pre>","tags":["standard-project-files","python","nox"]},{"location":"programming/python/nox/index.html#extending-the-noxfile","title":"Extending the noxfile","text":"<p>Check the <code>nox_extra</code> module for information on extending <code>nox</code> with custom sessions.</p>","tags":["standard-project-files","python","nox"]},{"location":"programming/python/nox/index.html#noxfilepy-helper-functions","title":"noxfile.py helper functions","text":"","tags":["standard-project-files","python","nox"]},{"location":"programming/python/nox/index.html#install-uv-project","title":"Install UV project","text":"<p>You can install your UV project in the <code>noxfile.py</code> with function. Copy/paste this somewhere towards the top of your code, then add to any sessions where you want to install the whole project.</p> Install UV in session<pre><code>import nox\n\nfrom pathlib import Path\n\n\n# this VENV_DIR constant specifies the name of the dir that the `dev`\n# session will create, containing the virtualenv;\n# the `resolve()` makes it portable\nVENV_DIR = Path(\"./.venv\").resolve()\n\n\ndef install_uv_project(session: nox.Session, external: bool = False) -&gt; None:\n    \"\"\"Method to install uv and the current project in a nox session.\"\"\"\n    log.info(\"Installing uv in session\")\n    session.install(\"uv\")\n    log.info(\"Syncing uv project\")\n    session.run(\"uv\", \"sync\", external=external)\n    log.info(\"Installing project\")\n    session.run(\"uv\", \"pip\", \"install\", \".\", external=external)\n</code></pre> <p>Example session using <code>install_uv_project()</code>:</p> Session that uses install_uv_project()<pre><code>@nox.session(name=\"dev-env\", tags=[\"setup\"])\ndef dev(session: nox.Session) -&gt; None:\n    \"\"\"Sets up a python development environment for the project.\n\n    Run this on a fresh clone of the repository to automate building the project with uv.\n    \"\"\"\n    install_uv_project(session, external=True)\n</code></pre>","tags":["standard-project-files","python","nox"]},{"location":"programming/python/nox/index.html#cd-context-manager","title":"@cd context manager","text":"<p>This context manager allows you to change the directory a command is run from.</p> @cd context manager<pre><code>import nox\n\nfrom contextlib import contextmanager\nimport os\nimport importlib.util\nimport typing as t\n\n\n@contextmanager\ndef cd(new_dir) -&gt; t.Generator[None, importlib.util.Any, None]: # type: ignore\n    \"\"\"Context manager to change a directory before executing command.\"\"\"\n    prev_dir: str = os.getcwd()\n    os.chdir(os.path.expanduser(new_dir))\n    try:\n        yield\n    finally:\n        os.chdir(prev_dir)\n</code></pre>","tags":["standard-project-files","python","nox"]},{"location":"programming/python/nox/index.html#find_free_port","title":"find_free_port()","text":"<p>This function can find a free port using the <code>socket</code> library. This is useful for sessions that run a service, like <code>mkdocs-serve</code>. Instead of hard-coding the port and risking an in-use port, this function can find a free port to pass to the function.</p> find_free_port()<pre><code>import nox\nimport socket\n\n\ndef find_free_port(start_port=8000) -&gt; int:\n    \"\"\"Find a free port starting from a specific port number.\"\"\"\n    port = start_port\n    while True:\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n            try:\n                sock.bind((\"0.0.0.0\", port))\n                return port\n            except socket.error:\n                log.info(f\"Port {port} is in use, trying the next port.\")\n                port += 1\n</code></pre> <p>Example usage:</p> Example usage for find_free_port()<pre><code>import nox\n\n\n@nox.session(name=\"serve-mkdocs\", tags=[\"mkdocs\", \"serve\"])\ndef serve_mkdocs(session: nox.Session) -&gt; None:\n    install_uv_project(session)\n\n    free_port = find_free_port(start_port=8000)\n\n    log.info(f\"Serving MKDocs site on port {free_port}\")\n\n    try:\n        session.run(\"mkdocs\", \"serve\", \"--dev-addr\", f\"0.0.0.0:{free_port}\")\n    except Exception as exc:\n        msg = f\"({type(exc)}) Unhandled exception serving MKDocs site. Details: {exc}\"\n        log.error(msg)\n</code></pre>","tags":["standard-project-files","python","nox"]},{"location":"programming/python/nox/custom-sessions.html","title":"Custom Nox Sessions","text":"","tags":["python","nox","automation"]},{"location":"programming/python/nox/custom-sessions.html#count-lines-of-code-with-pygount","title":"Count lines of code with pygount","text":"Pygount LOC count<pre><code>@nox.session(name=\"count-loc\")\ndef count_lines_of_code(session: nox.Session):\n    session.install(\"pygount\")\n\n    log.info(\"Counting lines of code with pygount\")\n    session.run(\"pygount\", \"--format=summary\", \"./\")\n</code></pre>","tags":["python","nox","automation"]},{"location":"programming/python/nox/nox-logger.html","title":"configuring a logging.Logger for nox","text":"<p>Adding a logger to your <code>noxfile.py</code> is very helpful when debugging, and also to see what's going on in a pipeline (for example).</p>","tags":["standard-project-files","python","nox","logging"]},{"location":"programming/python/nox/nox-logger.html#example-config-dict","title":"Example config dict","text":"noxfile.py logging_config<pre><code>logging_config: dict = {\n        \"version\": 1,\n        \"disable_existing_loggers\": False,\n        \"loggers\": {\n            \"nox\": {\n                \"level\": level_name.upper(),\n                \"handlers\": [\"console\"],\n                \"propagate\": False,\n            }\n        },\n        \"handlers\": {\n            \"console\": {\n                \"class\": \"logging.StreamHandler\",\n                \"formatter\": \"nox\",\n                \"level\": \"DEBUG\",\n                \"stream\": \"ext://sys.stdout\",\n            }\n        },\n        \"formatters\": {\n            \"nox\": {\n                \"format\": \"[NOX] [%(asctime)s] [%(levelname)s] [%(name)s]: %(message)s\",\n                \"datefmt\": \"%Y-%m-%D %H:%M:%S\",\n            }\n        },\n    }\n</code></pre>","tags":["standard-project-files","python","nox","logging"]},{"location":"programming/python/nox/nox-logger.html#example-configure-a-nox-logger","title":"Example configure a nox logger","text":"nox logger<pre><code>import logging\nimport logging.config\nimport logging.handlers\n\nimport nox\n\n## Detect container env, or default to False\nif \"CONTAINER_ENV\" in os.environ:\n    CONTAINER_ENV: bool = os.environ[\"CONTAINER_ENV\"]\nelse:\n    CONTAINER_ENV: bool = False\n\n\ndef setup_nox_logging(\n    level_name: str = \"DEBUG\", disable_loggers: list[str] | None = []\n) -&gt; None:\n    \"\"\"Configure a logger for the Nox module.\n\n    Params:\n        level_name (str): The uppercase string repesenting a logging logLevel.\n        disable_loggers (list[str] | None): A list of logger names to disable, i.e. for 3rd party apps.\n            Note: Disabling means setting the logLevel to `WARNING`, so you can still see errors.\n\n    \"\"\"\n    ## If container environment detected, default to logging.DEBUG\n    if CONTAINER_ENV:\n        level_name: str = \"DEBUG\"\n\n    logging_config: dict = {\n        \"version\": 1,\n        \"disable_existing_loggers\": False,\n        \"loggers\": {\n            \"nox\": {\n                \"level\": level_name.upper(),\n                \"handlers\": [\"console\"],\n                \"propagate\": False,\n            }\n        },\n        \"handlers\": {\n            \"console\": {\n                \"class\": \"logging.StreamHandler\",\n                \"formatter\": \"nox\",\n                \"level\": \"DEBUG\",\n                \"stream\": \"ext://sys.stdout\",\n            }\n        },\n        \"formatters\": {\n            \"nox\": {\n                \"format\": \"[NOX] [%(asctime)s] [%(levelname)s] [%(name)s]: %(message)s\",\n                \"datefmt\": \"%Y-%m-%D %H:%M:%S\",\n            }\n        },\n    }\n\n    ## Configure logging. Only run this once in an application\n    logging.config.dictConfig(config=logging_config)\n\n    ## Disable loggers by name. Sets logLevel to logging.WARNING to suppress all but warnings &amp; errors\n    for _logger in disable_loggers:\n        logging.getLogger(_logger).setLevel(logging.WARNING)\n\n\n## Configure logging\nsetup_nox_logging()\n\n## Create logger for this module\nlog: logging.Logger = logging.getLogger(\"nox\")\n</code></pre>","tags":["standard-project-files","python","nox","logging"]},{"location":"programming/python/nox/nox-logger.html#disable-loggers-by-name","title":"Disable loggers by name","text":"<p>If you use the <code>setup_nox_logging()</code> method with a list of logger names (as <code>str</code> objects, i.e. <code>urllib3</code>, <code>sqlalchemy</code>, etc), those loggers will have their log level set to <code>logging.WARNING</code>. This effectively silences them; warning, error, and critical messages will still show.</p> Disable loggers<pre><code>def setup_nox_logging(\n    level_name: str = \"DEBUG\", disable_loggers: list[str] | None = []\n) -&gt; None:\n\n    ...\n\n    ## Disable loggers by name. Sets logLevel to logging.WARNING to suppress all but warnings &amp; errors\n    for _logger in disable_loggers:\n        logging.getLogger(_logger).setLevel(logging.WARNING)\n\n\n## Disable the requests and urllib3 loggers\nsetup_nox_logging(disable_loggers=[\"requests\", \"urllib3\"])\n</code></pre>","tags":["standard-project-files","python","nox","logging"]},{"location":"programming/python/nox/nox_extra-module/index.html","title":"Add a nox_extra module to your project","text":"<p>As you add more sessions to your <code>noxfile.py</code>, it can start to get difficult to navigate. You can break your code into modules for your <code>noxfile.py</code> by creating a directory in the root of your project (in the same path as your <code>noxfile.py</code>), and moving your code into scripts within that path.</p> <p>Warning</p> <p>This guide assumes you're using the default <code>nox_extra/</code> path; if you choose to name this folder something else, make sure to update any code that references the old <code>nox_extra</code> module.</p>","tags":["standard-project-files","python","nox","nox_extra"]},{"location":"programming/python/nox/nox_extra-module/index.html#setup","title":"Setup","text":"<ul> <li>Create a <code>nox_extra/</code> directory</li> <li>This is the module where you will add scripts to modularize your <code>noxfile.py</code>.</li> <li>You can name this anything you want, but this guide assumes you stick with the default <code>nox_extra/</code>.     If you use another name, make sure to update the code where it references the old \"nox_extra\" value, i.e. in <code>import</code> statements.</li> <li>Create the following files:</li> <li><code>nox_extra/__init__.py</code>: Denote the <code>nox_extra/</code> directory as a module.<ul> <li>You can leave this file empty. Anything you import in this file will be available from the top-level <code>nox_extra</code> module.</li> </ul> </li> <li><code>nox_extra/nox_utils.py</code><ul> <li>Offload all common variables, functions, etc to a single file.</li> <li>Script contents can be imported into the <code>noxfile.py</code> using <code>import nox_extra.&lt;script_name&gt; as &lt;script_name&gt;</code>.</li> <li>Other modules in <code>nox_extra</code> can also import files and variables from this script. For example, the <code>nox_django_sessions.py</code> script imports variables like <code>DEFAULT_PYTHON</code> and the <code>cd()</code> context manager.</li> </ul> </li> <li>Write your <code>noxfile.py</code>.<ul> <li>Reference the Example noxfile.py section below</li> </ul> </li> </ul>","tags":["standard-project-files","python","nox","nox_extra"]},{"location":"programming/python/nox/nox_extra-module/index.html#usage","title":"Usage","text":"<p>Note</p> <p>Check the <code>nox_extra/nox_utils.py</code> and <code>nox_extra/nox_django_sessions.py</code> sections for more detailed information about each script in the <code>nox_extra</code> module. You can use these files as a reference to create your own custom <code>nox</code> scripts.</p>","tags":["standard-project-files","python","nox","nox_extra"]},{"location":"programming/python/nox/nox_extra-module/index.html#example-noxfilepy","title":"Example noxfile.py","text":"<p>This <code>noxfile.py</code> imports from the <code>nox_extra</code> module. This saves some space at the top of the script doing things like configuring the default Python version.</p> noxfile.py<pre><code>from __future__ import annotations\n\nfrom contextlib import contextmanager\nimport importlib.util\nimport logging\nimport logging.config\nimport os\nimport sys\nfrom pathlib import Path\nimport platform\nimport secrets\nimport socket\nimport typing as t\n\nlog: logging.Logger = logging.getLogger(\"nox\")\n\nimport nox\n\nsys.path.append(os.path.abspath(\"nox_extra\"))\n\n## Set nox options\nif importlib.util.find_spec(\"uv\"):\n    nox.options.default_venv_backend = \"uv|virtualenv\"\nelse:\n    nox.options.default_venv_backend = \"virtualenv\"\nnox.options.reuse_existing_virtualenvs = True\nnox.options.error_on_external_run = False\nnox.options.error_on_missing_interpreters = False\n# nox.options.report = True\n\n\ntry:\n    ## Import extra nox modules from a \"nox_extra/\" directory.\n    import nox_utils\n\n    nox_utils.setup_nox_logging()\n\n    log.info(\n        \"nox_extra module detected, enhancements from nox_extra.nox_utils are now available.\"\n    )\nexcept ImportError:\n    log.error(f\"Unable to import nox_utils.\")\n\n## Define versions to test\nPY_VERSIONS: list[str] = nox_utils.PY_VERSIONS\n## Get tuple of Python ver ('maj', 'min', 'mic')\nPY_VER_TUPLE: tuple[str, str, str] = nox_utils.PY_VER_TUPLE\n## Dynamically set Python version\nDEFAULT_PYTHON: str = nox_utils.DEFAULT_PYTHON\n\nDEFAULT_LINT_PATHS: list[str] = nox_utils.DEFAULT_LINT_PATHS\n## Set directory for requirements.txt file output\nREQUIREMENTS_OUTPUT_DIR: Path = nox_utils.REQUIREMENTS_OUTPUT_DIR\n\nnox_utils.append_lint_paths(extra_paths=[\"nox_extra\"], lint_paths=DEFAULT_LINT_PATHS)\n</code></pre>","tags":["standard-project-files","python","nox","nox_extra"]},{"location":"programming/python/nox/nox_extra-module/index.html#nox_extranox_utilspy","title":"nox_extra/nox_utils.py","text":"<p>The <code>nox_utils.py</code> module contains variables, helper functions, &amp; more that I commonly re-use in my projects. For example, a <code>DEFAULT_PYTHON</code> string variable that contains the version of Python used to launch <code>nox</code>. This becomes the default Python version to pass into <code>@nox.session(python=DEFAULT_PYTHON)</code> declarations.</p>","tags":["standard-project-files","python","nox","nox_extra"]},{"location":"programming/python/nox/nox_extra-module/index.html#nox_utilspy","title":"nox_utils.py","text":"<p>Copy and paste the contents below into <code>nox_extra/nox_utils.py</code>.</p> nox_extra/nox_utils.py<pre><code>\"\"\"Extra `nox` utilities.\n\nDescription:\n    Add some `nox` helpers like a `DEFAULT_PY_VER` variable, which detects the version of Python used to call `nox` and\n    can be the default Python version for custom sessions.\n\n    Import in your `noxfile.py` with: `import nox_extra.nox_utils as nox_utils`.\n\n    Get a list of sessions with `nox -s --list`.\n\nUsage:\n    In your `noxfile.py`, add a line `import nox_extra.nox_utils as nox_utils`. Now, variables &amp; functions in this script are available in your\n    main `noxfile.py`.\n\n    For example, to add additional paths to the `DEFAULT_LINT_PATHS` variable in this file, you could call:\n\n\n        ## noxfile.py\n\n        import nox_extra.nox_utils as nox_utils\n\n        ## Add the `nox_extra/` path to the list of paths to lint with ruff/black/etc.\n        nox_utils.append_lint_paths(\n            extra_paths=[\"nox_extra\"], lint_paths=DEFAULT_LINT_PATHS\n        )\n\n\n    Now, any session that uses the `DEFAULT_LINT_PATHS` variable will also scan the `nox_extra/` path.\n\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom contextlib import contextmanager\nimport logging\nimport os\nfrom pathlib import Path\nimport platform\nimport typing as t\n\nimport nox\n\n## Define list of variables &amp; functions to export from this app\n__all__: list[str] = [\n    \"PY_VERSIONS\",\n    \"PY_VER_TUPLE\",\n    \"DEFAULT_PYTHON\",\n    \"DEFAULT_LINT_PATHS\",\n    \"REQUIREMENTS_OUTPUT_DIR\",\n    \"DJANGO_PROJECT_DIR\",\n    \"CONTAINER_ENV\",\n    \"cd\",\n    \"check_path_exists\",\n    \"append_lint_paths\",\n    \"detect_container_env\",\n    \"setup_nox_logging\",\n]\n\nlog: logging.Logger = logging.getLogger(__name__)\n\n## Define versions to test\nPY_VERSIONS: list[str] = [\"3.12\", \"3.11\"]\n## Get tuple of Python ver ('maj', 'min', 'mic')\nPY_VER_TUPLE: tuple[str, str, str] = platform.python_version_tuple()\n## Dynamically set Python version\nDEFAULT_PYTHON: str = f\"{PY_VER_TUPLE[0]}.{PY_VER_TUPLE[1]}\"\n\n## PDM version for sessions that use it\nPDM_VER: str = \"2.18.1\"\n\n## At minimum, these paths will be checked by your linters\n#  Add new paths with nox_utils.append_lint_paths(extra_paths=[\"...\"],)\nDEFAULT_LINT_PATHS: list[str] = [\"src\", \"tests\"]\n## Set directory for requirements.txt file output\nREQUIREMENTS_OUTPUT_DIR: Path = Path(\"./requirements\")\n\n\n@contextmanager\ndef cd(newdir):\n    \"\"\"Context manager to change a directory before executing command.\"\"\"\n    prevdir = os.getcwd()\n    os.chdir(os.path.expanduser(newdir))\n    try:\n        yield\n    finally:\n        os.chdir(prevdir)\n\n\ndef check_path_exists(p: t.Union[str, Path] = None) -&gt; bool:\n    \"\"\"Check the existence of a path.\n\n    Params:\n        p (str | Path): The path to the directory/file to check.\n\n    Returns:\n        (True): If Path defined in `p` exists.\n        (False): If Path defined in `p` does not exist.\n\n    \"\"\"\n    p: Path = Path(f\"{p}\")\n    if \"~\" in f\"{p}\":\n        p = p.expanduser()\n\n    _exists: bool = p.exists()\n\n    if not _exists:\n        log.error(FileNotFoundError(f\"Could not find path '{p}'.\"))\n\n    return _exists\n\n\ndef append_lint_paths(\n    extra_paths: str | list[str] = None, lint_paths: list[str] = DEFAULT_LINT_PATHS\n) -&gt; list[str]:\n    \"\"\"Adds more paths to the `DEFAULT_LINT_PATHS` variable.\n\n    Description:\n        Some `nox` sessions, like a `ruff` or `black` linter, require a path/directory input. Some defaults\n        are provided in `DEFAULT_LINT_PATHS`, such as `src/` and `tests/`.\n\n        If your project has scripts in additional directories, like `nox_extra`, or a `Django` app at the project's root path (where your `noxfile.py` should be),\n        like `./django-app/manage.py`, you can add the `./django-app` directory to the `DEFAULT_LINT_PATHS` list using this function.\n\n    Params:\n        extra_paths (str | list[str]): A string or list of strings representing directories a session calling this function should iterate over.\n            For example, a session that lints paths with `ruff` will scan over any additional paths in `extra_paths`.\n        lint_paths (list[str]): A list of strings representing paths to iterate over. This is the \"starting\" list.\n    \"\"\"\n    if lint_paths is None:\n        lint_paths = []\n\n    if extra_paths is None:\n        return lint_paths\n\n    if isinstance(extra_paths, str):\n        extra_paths = [extra_paths]\n\n    for pattern in extra_paths:\n        for path in Path(\".\").rglob(pattern):\n            relative_path = (\n                Path(\".\").joinpath(path).resolve().relative_to(Path(\".\").resolve())\n            )\n\n            if f\"{relative_path}\" not in lint_paths:\n                if relative_path.exists():\n                    lint_paths.append(f\"./{relative_path}\")\n                else:\n                    log.warning(\n                        f\"Could not append path '{relative_path}' to list of lint paths. File/directory does not exist.\"\n                    )\n                    continue\n\n    log.debug(f\"Lint paths: {lint_paths}\")\n    return lint_paths\n\n\ndef detect_container_env(container_env_varname: str = \"CONTAINER_ENV\") -&gt; bool:\n    \"\"\"Detect the presence of an env variable denoting a container environment.\n\n    Params:\n        container_env_varname (str): The name of the environment variable to search for.\n\n    Returns:\n        (True): If the environment variable is detected and it is set to `True`.\n        (False): If the environment variable is not detected, or if it is detected and it is set to `False`.\n\n    \"\"\"\n    ## Detect container env, or default to False\n    if container_env_varname in os.environ:\n        CONTAINER_ENV: bool = os.environ[container_env_varname]\n    else:\n        CONTAINER_ENV: bool = False\n\n    return CONTAINER_ENV\n\n\nCONTAINER_ENV: bool = detect_container_env()\n\n\ndef setup_nox_logging(\n    nox_logger_name: str = \"nox\",\n    level_name: str = \"DEBUG\",\n    disable_loggers: list[str] | None = [],\n) -&gt; None:\n    \"\"\"Configure a stdlib logger for the Nox module.\n\n    Description:\n        This module hijacks the default `nox` logger\n\n    Params:\n        level_name (str): The uppercase string repesenting a logging logLevel.\n        disable_loggers (list[str] | None): A list of logger names to disable, i.e. for 3rd party apps.\n            Note: Disabling means setting the logLevel to `WARNING`, so you can still see errors.\n\n    \"\"\"\n    ## If container environment detected, default to logging.DEBUG\n    if CONTAINER_ENV:\n        level_name: str = \"DEBUG\"\n\n    logging_config: dict = {\n        \"version\": 1,\n        \"disable_existing_loggers\": False,\n        \"loggers\": {\n            str(nox_logger_name): {\n                \"level\": level_name.upper(),\n                \"handlers\": [\"console\"],\n                \"propagate\": False,\n            }\n        },\n        \"handlers\": {\n            \"console\": {\n                \"class\": \"logging.StreamHandler\",\n                \"formatter\": f\"{nox_logger_name}_fmt\",\n                \"level\": \"DEBUG\",\n                \"stream\": \"ext://sys.stdout\",\n            }\n        },\n        \"formatters\": {\n            f\"{nox_logger_name}_fmt\": {\n                \"format\": f\"[{nox_logger_name.upper()}] \"\n                + \"[%(asctime)s] [%(levelname)s] [%(name)s]: %(message)s\",\n                \"datefmt\": \"%Y-%m-%D %H:%M:%S\",\n            }\n        },\n    }\n\n    ## Configure logging. Only run this once in an application\n    logging.config.dictConfig(config=logging_config)\n\n    if disable_loggers:\n        ## Disable loggers by name. Sets logLevel to logging.WARNING to suppress all but warnings &amp; errors\n        for _logger in disable_loggers:\n            logging.getLogger(_logger).setLevel(logging.WARNING)\n\n\n@nox.session(python=[DEFAULT_PYTHON], name=\"ruff-lint\", tags=[\"ruff\", \"clean\", \"lint\"])\ndef run_linter(session: nox.Session, lint_paths: list[str] = DEFAULT_LINT_PATHS):\n    \"\"\"Nox session to run Ruff code linting.\"\"\"\n    if not check_path_exists(p=\"ruff.toml\"):\n        if not Path(\"pyproject.toml\").exists():\n            log.warning(\n                \"\"\"No ruff.toml file found. Make sure your pyproject.toml has a [tool.ruff] section!\n\nIf your pyproject.toml does not have a [tool.ruff] section, ruff's defaults will be used.\nDouble check imports in __init__.py files, ruff removes unused imports by default.\n\"\"\"\n            )\n\n    session.install(\"ruff\")\n\n    log.info(\"Linting code\")\n    for d in lint_paths:\n        if not Path(d).exists():\n            log.warning(f\"Skipping lint path '{d}', could not find path\")\n            pass\n        else:\n            lint_path: Path = Path(d)\n            log.info(f\"Running ruff imports sort on '{d}'\")\n            session.run(\n                \"ruff\",\n                \"check\",\n                lint_path,\n                \"--select\",\n                \"I\",\n                \"--fix\",\n            )\n\n            log.info(f\"Running ruff checks on '{d}' with --fix\")\n            session.run(\n                \"ruff\",\n                \"check\",\n                lint_path,\n                \"--fix\",\n            )\n\n    log.info(\"Linting noxfile.py\")\n    session.run(\n        \"ruff\",\n        \"check\",\n        f\"{Path('./noxfile.py')}\",\n        \"--fix\",\n    )\n\n\n@nox.session(\n    python=[DEFAULT_PYTHON], name=\"black-lint\", tags=[\"black\", \"clean\", \"lint\"]\n)\ndef run_linter(session: nox.Session, lint_paths: list[str] = DEFAULT_LINT_PATHS):\n    \"\"\"Nox session to run Ruff code linting.\"\"\"\n    session.install(\"black\")\n\n    log.info(\"Linting code\")\n    for d in lint_paths:\n        if not Path(d).exists():\n            log.warning(f\"Skipping lint path '{d}', could not find path\")\n            pass\n        else:\n            lint_path: Path = Path(d)\n            log.info(f\"Linting path '{d}' with black\")\n            session.run(\n                \"black\",\n                lint_path,\n            )\n\n    log.info(\"Linting noxfile.py\")\n    session.run(\n        \"black\",\n        Path(\"./noxfile.py\"),\n    )\n\n\n@nox.session(python=[DEFAULT_PYTHON], name=\"uv-export\")\n@nox.parametrize(\"requirements_output_dir\", REQUIREMENTS_OUTPUT_DIR)\ndef export_requirements(session: nox.Session, requirements_output_dir: Path):\n    ## Ensure REQUIREMENTS_OUTPUT_DIR path exists\n    if not requirements_output_dir.exists():\n        try:\n            requirements_output_dir.mkdir(parents=True, exist_ok=True)\n        except Exception as exc:\n            msg = Exception(\n                f\"Unable to create requirements export directory: '{requirements_output_dir}'. Details: {exc}\"\n            )\n            log.error(msg)\n\n            requirements_output_dir: Path = Path(\"./\")\n\n    session.install(f\"uv\")\n\n    log.info(\"Exporting production requirements\")\n    session.run(\n        \"uv\",\n        \"pip\",\n        \"compile\",\n        \"pyproject.toml\",\n        \"-o\",\n        str(REQUIREMENTS_OUTPUT_DIR / \"requirements.txt\"),\n    )\n\n\n@nox.session(python=[DEFAULT_PYTHON], name=\"pdm-export\")\n@nox.parametrize(\"pdm_ver\", [PDM_VER])\n@nox.parametrize(\"requirements_output_dir\", REQUIREMENTS_OUTPUT_DIR)\ndef export_requirements(\n    session: nox.Session, pdm_ver: str, requirements_output_dir: Path\n):\n    ## Ensure REQUIREMENTS_OUTPUT_DIR path exists\n    if not requirements_output_dir.exists():\n        try:\n            requirements_output_dir.mkdir(parents=True, exist_ok=True)\n        except Exception as exc:\n            msg = Exception(\n                f\"Unable to create requirements export directory: '{requirements_output_dir}'. Details: {exc}\"\n            )\n            log.error(msg)\n\n            requirements_output_dir: Path = Path(\"./\")\n\n    session.install(f\"pdm&gt;={pdm_ver}\")\n\n    log.info(\"Exporting production requirements\")\n    session.run(\n        \"pdm\",\n        \"export\",\n        \"--prod\",\n        \"-o\",\n        f\"{REQUIREMENTS_OUTPUT_DIR}/requirements.txt\",\n        \"--without-hashes\",\n    )\n\n    log.info(\"Exporting development requirements\")\n    session.run(\n        \"pdm\",\n        \"export\",\n        \"-d\",\n        \"-o\",\n        f\"{REQUIREMENTS_OUTPUT_DIR}/requirements.dev.txt\",\n        \"--without-hashes\",\n    )\n</code></pre>","tags":["standard-project-files","python","nox","nox_extra"]},{"location":"programming/python/nox/nox_extra-module/sessions/index.html","title":"Custom nox_extra Sessions","text":"<p>Use the left hand navigation to browse through custom <code>nox</code> sessions.</p>","tags":["standard-project-files","python","nox","pre-commit","jupyter","pytest"]},{"location":"programming/python/nox/nox_extra-module/sessions/index.html#session-information","title":"Session Information","text":"<p>These sessions are part of the <code>nox_extra</code> module. They may rely on code in <code>nox_extra/nox_utils.py</code></p> <p>Some modules may have 2 versions, one where the required variables/functions are declared without importing any <code>nox_extra</code> code, and one where variables/functions/sessions are loaded from a <code>nox_extra</code> module.</p>","tags":["standard-project-files","python","nox","pre-commit","jupyter","pytest"]},{"location":"programming/python/nox/nox_extra-module/sessions/ansible-sessions.html","title":"Ansible","text":"","tags":["standard-project-files","python","nox","ansible"]},{"location":"programming/python/nox/nox_extra-module/sessions/ansible-sessions.html#build-ansible-collections","title":"Build Ansible collections","text":"noxfile.py<pre><code>COLLECTIONS_PATH: Path = Path(\"./collections\")\nANSIBLE_COLLECTIONS_PATH: Path = Path(f\"{COLLECTIONS_PATH}/ansible_collections\")\nMY_COLLECTIONS_PATH: Path = Path(f\"{COLLECTIONS_PATH}/my\")\nCOLLECTION_BUILD_OUTPUT_PATH: Path = Path(\"./build\")\n\n@nox.session(python=DEFAULT_PYTHON, name=\"build-my-collections\", tags=[\"ansible\", \"build\"])\n@nox.parametrize(\"custom_collections\", [CUSTOM_COLLECTIONS])\n@nox.parametrize(\"collection_build_output_path\", [COLLECTION_BUILD_OUTPUT_PATH])\ndef build_custom_collections(session: nox.Session, custom_collections: list[CustomAnsibleCollection], collection_build_output_path: Path):\n    if not collection_build_output_path.exists():\n        try:\n            collection_build_output_path.mkdir(parents=True, exist_ok=True)\n        except Exception as exc:\n            msg = Exception(f\"({type(exc)}) Unhandled exception creating build output path '{collection_build_output_path}'. Details: {exc}\")\n            log.error(msg)\n\n            raise exc\n\n    session.install(\"ansible-core\")\n\n    for _collection in custom_collections:\n        log.info(f\"Building collection '{_collection.name}' at path: {_collection.path}\")\n\n        if not _collection.path_exists:\n            _exc = FileNotFoundError(f\"Could not find collection at path '{_collection.path}'\")\n            log.error(_exc)\n\n            raise _exc\n\n        log.debug(f\"Found collection '{_collection.name}' at path: {_collection.path}\")\n\n        try:\n            session.run(\"ansible-galaxy\", \"collection\", \"build\", f\"{_collection.path}\", \"--output-path\", f\"{collection_build_output_path}\", \"--force\")\n        except Exception as exc:\n            msg = Exception(f\"({type(exc)}) Unhandled exception building collection '{_collection.name}'. Details: {exc}\")\n            log.error(msg)\n\n            continue\n\n\n@nox.session(python=DEFAULT_PYTHON, name=\"install-ansible-requirements\", tags=[\"ansible\", \"install\"])\ndef install_collections(session: nox.Session):\n    assert Path(\"requirements.yml\").exists(), FileNotFoundError(\"Could not find Ansible project requirements.yml.\")\n\n    session.install(\"ansible-core\")\n\n    log.info(\"Installing collections from requirements.yml\")\n\n    try:\n        session.run(\"ansible-galaxy\", \"collection\", \"install\", \"-r\", \"requirements.yml\")\n    except Exception as exc:\n        msg = Exception(f\"({type(exc)}) Unhandled exception installing Ansible Galaxy requirements from requirements.yml. Details: {exc}\")\n        log.error(msg)\n\n        raise exc\n\n    log.info(\"Installing roles from requirements.yml\")\n\n    try:\n        session.run(\"ansible-galaxy\", \"role\", \"install\", \"-r\", \"requirements.yml\")\n    except Exception as exc:\n        msg = Exception(f\"({type(exc)}) Unhandled exception installing Ansible Galaxy requirements from requirements.yml. Details: {exc}\")\n        log.error(msg)\n\n        raise exc\n\n    if Path(\"requirements.private.yml\").exists():\n        log.info(\"Ensuring local collections are installed from requirements.private.yml with --force\")\n        try:\n            session.run(\"ansible-galaxy\", \"collection\", \"install\", \"-r\", \"requirements.private.yml\", \"--force\")\n        except Exception as exc:\n            msg = Exception(f\"Unhandled exception installing packages from 'requirements.private.yml'. Details: {exc}\")\n            log.error(msg)\n\n            raise exc\n</code></pre>","tags":["standard-project-files","python","nox","ansible"]},{"location":"programming/python/nox/nox_extra-module/sessions/ansible-sessions.html#ansible-debug-remote-host","title":"Ansible debug remote host","text":"Ansible remote host debug<pre><code>## Session that runs an Ansible playbook\n@nox.session(python=DEFAULT_PYTHON, name=\"playbook-debug-all\", tags=[\"debug\", \"ansible\"])\ndef ansible_playbook_debug_all(session: nox.Session):\n    session.install(\"ansible-core\")\n\n    log.info(\"Running debug-all.yml playbook on homelab inventory\")\n\n    try:\n        session.run(\"ansible-playbook\", \"-i\", \"inventories/homelab/inventory.yml\", \"plays/debug/debug-all.yml\")\n    except Exception as exc:\n        msg = Exception(f\"({type(exc)}) Unhandled exception running playbook. Details: {exc}\")\n        log.error(msg)\n\n        raise exc\n</code></pre>","tags":["standard-project-files","python","nox","ansible"]},{"location":"programming/python/nox/nox_extra-module/sessions/django-sessions.html","title":"Django","text":"","tags":["standard-project-files","python","nox","django"]},{"location":"programming/python/nox/nox_extra-module/sessions/django-sessions.html#nox_extranox_django_sessionspy","title":"nox_extra/nox_django_sessions.py","text":"<p>Helpers &amp; <code>nox</code> sessions for <code>Django</code> projects. Expects the existence of a <code>nox_extra/nox_utils.py</code> file to exist, this script imports some values from the common utils file.</p>","tags":["standard-project-files","python","nox","django"]},{"location":"programming/python/nox/nox_extra-module/sessions/django-sessions.html#nox_django_sessionspy","title":"nox_django_sessions.py","text":"<p>Copy and paste the contents below into <code>nox_extra/nox_django_sessions.py</code>.</p> nox_extra/nox_django_sessions.py<pre><code>\"\"\"Django `nox` sessions.\n\nDescription:\n    Helpers and defaults for Django `nox` sessions. Includes sessions like generate-django-secret, which\n    creates a secure string that can be used for your Django apps' `SECRET_KEY`.\n\n\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom contextlib import contextmanager\nimport logging\nimport os\nimport platform\nimport secrets\nimport socket\nfrom pathlib import Path\n\nlog: logging.Logger = logging.getLogger(__name__)\n\nimport nox_extra.nox_utils as nox_utils\n\nimport nox\n\n__all__: list[str] = [\n    \"_find_free_port\",\n    \"_django_makemigrations\",\n    \"_django_migrate\",\n    \"generate_secure_secret\",\n    \"generate_django_secret\",\n    \"run_django_devserver\",\n    \"django_make_migrations\",\n    \"django_migrate\",\n    \"django_do_migrations\",\n]\n\n## Path to output requirements.txt file(s)\nREQUIREMENTS_OUTPUT_DIR: Path = nox_utils.REQUIREMENTS_OUTPUT_DIR\n\n## Define versions to test\nPY_VERSIONS: list[str] = nox_utils.PY_VERSIONS\n## Get tuple of Python ver ('maj', 'min', 'mic')\nPY_VER_TUPLE: tuple[str, str, str] = nox_utils.PY_VER_TUPLE\n## Dynamically set Python version\nDEFAULT_PYTHON: str = nox_utils.DEFAULT_PYTHON\n\n## UPDATE THIS VALUE FOR EACH NEW DJANGO PROJECT\nDJANGO_PROJECT_DIR = \"./weatherdaily/\"\n\n\ndef _find_free_port(host_addr: str = \"0.0.0.0\", start_port=8000) -&gt; int:\n    \"\"\"Find a free port starting from a specific port number.\n\n    Params:\n        host_addr (str): The host address to bind to. Default is '0.0.0.0', which can be dangerous.\n        start_port (int): Attempt to bind to this port, and increment 1 each time port binding fails.\n\n    Returns:\n        (int): An open port number, i.e. 8000 if 8001 is in use.\n    \"\"\"\n\n    port: int = start_port\n\n    ## Loop open port check until one is bound\n    while True:\n        ## Create a socked\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n            try:\n                ## Try binding port to host address\n                sock.bind((host_addr, port))\n                return port\n\n            except socket.error:\n                ## Port in use, increment &amp; retry\n                log.info(f\"Port {port} is in use, trying the next port.\")\n                port += 1\n\n\ndef _django_makemigrations(session: nox.Session) -&gt; None:\n    \"\"\"Run Django's manage.py makemigrations.\n\n    Description:\n        Creates migrations for all Django apps in your project. Use with caution,\n        this will create migrations for all models.\n\n    \"\"\"\n\n    ## Set script path to path with Django's manage.py\n    with nox_utils.cd(newdir=DJANGO_PROJECT_DIR):\n        try:\n            session.run(\"python\", \"manage.py\", \"makemigrations\")\n        except Exception as exc:\n            msg = f\"({type(exc)}) Unhandled exception running manage.py makemigrations. Details: {exc}\"\n            log.error(msg)\n\n            raise exc\n\n\ndef _django_migrate(session: nox.Session) -&gt; None:\n    \"\"\"Do all database migrations.\n\n    Description:\n        Performs migrations for all Django apps in your project. Use with caution,\n        this will migrate models for all apps.\n    \"\"\"\n    ## Set script path to path with Django's manage.py\n    with nox_utils.cd(newdir=DJANGO_PROJECT_DIR):\n        try:\n            session.run(\"python\", \"manage.py\", \"migrate\")\n        except Exception as exc:\n            msg = f\"({type(exc)}) Unhandled exception running manage.py migrate. Details: {exc}\"\n            log.error(msg)\n\n            raise exc\n\n\ndef generate_secure_secret(\n    random_char_seed: str = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\",\n    print_secret: bool = False,\n) -&gt; str:\n    \"\"\"Generate a secure string.\n\n    Description:\n        Combines the functionality of some code lovingly lifted from\n        django.core.management.utils.get_random_secret_key(). In their words,\n        \"return a securely generated random string.\"\n\n        This can be used to generate secrets, like Django's `SECRET_KEY` setting.\n\n    \"\"\"\n\n    def get_random_string(length, allowed_chars=random_char_seed):\n        \"\"\"Return a securely generated random string.\n\n        The bit length of the returned value can be calculated with the formula:\n            log_2(len(allowed_chars)^length)\n\n        For example, with default `allowed_chars` (26+26+10), this gives:\n        * length: 12, bit length =~ 71 bits\n        * length: 22, bit length =~ 131 bits\n        \"\"\"\n        return \"\".join(secrets.choice(allowed_chars) for i in range(length))\n\n    ## Generate a secret\n    _secret: str = get_random_string(50, random_char_seed)\n\n    if print_secret:\n        ## Print (instead of log, which could leak), the secret.\n        print(f\"Generated secure string:\\n\\n{_secret}\\n\")\n\n    return _secret\n\n\n@nox.session(\n    python=DEFAULT_PYTHON, name=\"generate-django-secret\", tags=[\"django\", \"init\"]\n)\ndef generate_django_secret(session: nox.Session) -&gt; None:\n    \"\"\"Securely generate a Django secret key.\n\n    Description:\n        Calls the generate_secure_secret() function, which returns an encrypted key made from randomized input.\n        Optionally prints the secret. This uses the print() function, instead of a logger, to prevent secrets from\n        leaking.\n    \"\"\"\n\n    ## Generate &amp; print a Django secret\n    generate_secure_secret(print_secret=True)\n\n\n@nox.session(python=DEFAULT_PYTHON, name=\"run-django-devserver\", tags=[\"django\"])\ndef run_django_devserver(session: nox.Session) -&gt; None:\n    \"\"\"Call Django's manage.py runserver with args.\n\n    Description:\n        Uses the _find_free_port() function to test binding ports to a socket,\n        returning an open/available port if the desired port is in use.\n\n        Uses the cd() context manager to set the script's path to the Django project\n        path for the duration of the script; while inside the Django project directory,\n        calls python manage.py runserver 0.0.0.0:$free_port.\n    \"\"\"\n    session.install(\"-r\", f\"{REQUIREMENTS_OUTPUT_DIR}/requirements.txt\")\n\n    ## Find an open port\n    free_port: int = _find_free_port(start_port=8000)\n\n    log.info(f\"Running local Django development server on port {free_port}\")\n    ## Temporarily set script path to Django's project path\n    with nox_utils.cd(newdir=DJANGO_PROJECT_DIR):\n        try:\n            session.run(\"python\", \"manage.py\", \"runserver\", f\"0.0.0.0:{free_port}\")\n        except Exception as exc:\n            msg = f\"({type(exc)}) Unhandled exception running Django development server. Details: {exc}\"\n            log.error(msg)\n\n            raise exc\n\n\n@nox.session(python=DEFAULT_PYTHON, name=\"django-makemigrations\", tags=[\"django\", \"db\"])\ndef django_make_migrations(session: nox.Session) -&gt; None:\n    \"\"\"Run Django's manage.py makemigrations on all app models.\n\n    Description:\n        Creates migrations for all Django apps in your project. Use with caution,\n        this will create migrations for all models.\n\n    \"\"\"\n    session.install(\"-r\", f\"{REQUIREMENTS_OUTPUT_DIR}/requirements.txt\")\n\n    log.info(\"Running manage.py makemigrations\")\n\n    try:\n        _django_makemigrations(session=session)\n    except Exception as exc:\n        msg = f\"({type(exc)}) Error running manage.py makemigrations. Details: {exc}\"\n        log.error(msg)\n\n\n@nox.session(python=DEFAULT_PYTHON, name=\"django-migrate\", tags=[\"django\", \"db\"])\ndef django_migrate(session: nox.Session) -&gt; None:\n    \"\"\"Run Django's manage.py migrate for all app models.\n\n    Description:\n        Performs migrations for all Django apps in your project. Use with caution,\n        this will migrate models for all apps.\n    \"\"\"\n    session.install(\"-r\", f\"{REQUIREMENTS_OUTPUT_DIR}/requirements.txt\")\n\n    log.info(\"Running manage.py migrate\")\n\n    _django_migrate(session=session)\n\n\n@nox.session(\n    python=DEFAULT_PYTHON,\n    name=\"django-migrate-all\",\n    tags=[\"django\", \"db\", \"migrations\"],\n)\ndef django_do_migrations(session: nox.Session) -&gt; None:\n    \"\"\"Create &amp; perform Django database migrations.\n\n    Description:\n        Calls Django's manage.py makemigrations to create migration files for\n        all app models, then do the migrations with manage.py migrate. Use with\n        caution, combining both of these steps can cause problems if there are\n        errors in your migrations.\n    \"\"\"\n    session.install(\"-r\", f\"{REQUIREMENTS_OUTPUT_DIR}/requirements.txt\")\n\n    log.info(\"Running manage.py makemigrations and manage.py migrate\")\n\n    ## Determine whether migrations should be performed.\n    #  Only flips to True if makemigrations is successful\n    DO_MIGRATION: bool = False\n\n    try:\n        _django_makemigrations(session=session)\n        ## Enable migration if makemigrations is successful\n        DO_MIGRATION = True\n    except Exception as exc:\n        msg: str = f\"({type(exc)}) Error create database migrations. Details: {exc}\"\n        log.error(msg)\n\n        DO_MIGRATION = False\n\n    if DO_MIGRATION:\n        log.info(\"Doing all Django database migrations.\")\n        _django_migrate(session=session)\n    else:\n        log.warning(\"Error during makemigrations. Skipping manage.py migrate.\")\n</code></pre>","tags":["standard-project-files","python","nox","django"]},{"location":"programming/python/nox/nox_extra-module/sessions/mkdocs-sessions.html","title":"MkDocs","text":"<p>Create a file at <code>nox_extra/nox_mkdocs_sessions.py</code> and paste the following contents:</p> nox_extra/nox_mkdocs_sessions.py imports<pre><code>import logging\nimport socket\n\nlog = logging.getLogger(__name__)\n\nimport nox\n\nfrom .nox_utils import REQUIREMENTS_OUTPUT_DIR, DEFAULT_PYTHON\n</code></pre>","tags":["standard-project-files","python","nox","mkdocs"]},{"location":"programming/python/nox/nox_extra-module/sessions/mkdocs-sessions.html#publish-mkdocs","title":"Publish mkdocs","text":"noxfile.py<pre><code>REQUIREMENTS_OUTPUT_DIR: str = \"requirements\"\n\n@nox.session(python=[DEFAULT_PYTHON], name=\"publish-mkdocs\", tags=[\"mkdocs\", \"publish\"])\ndef publish_mkdocs(session: nox.Session):\n    session.install(\"-r\", f\"{REQUIREMENTS_OUTPUT_DIR}/requirements.txt\")\n\n    log.info(\"Publishing MKDocs site\")\n\n    session.run(\"mkdocs\", \"gh-deploy\")\n</code></pre>","tags":["standard-project-files","python","nox","mkdocs"]},{"location":"programming/python/nox/nox_extra-module/sessions/mkdocs-sessions.html#serve-mkdocs-site-locally","title":"Serve mkdocs site locally","text":"<pre><code>import typing as t\nimport socket\nimport logging\nlog = logging.getLogger(__name__)\n\nREQUIREMENTS_OUTPUT_DIR: str = \"requirements\"\n\ndef _find_free_port(start_port=8000):\n    \"\"\"Find a free port starting from a specific port number\"\"\"\n    port = start_port\n    while True:\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n            try:\n                sock.bind((\"0.0.0.0\", port))\n                return port\n            except socket.error:\n                log.info(f\"Port {port} is in use, trying the next port.\")\n                port += 1\n\n\n@nox.session(python=DEFAULT_PYTHON, name=\"serve-mkdocs\", tags=[\"mkdocs\", \"serve\"])\ndef serve_mkdocs(session: nox.Session):\n    session.install(\"-r\", f\"{REQUIREMENTS_OUTPUT_DIR}/requirements.txt\")\n\n    free_port = _find_free_port(start_port=8000)\n\n    log.info(f\"Serving MKDocs site on port {free_port}\")\n\n    try:\n        session.run(\"mkdocs\", \"serve\", \"--dev-addr\", f\"0.0.0.0:{free_port}\")\n    except Exception as exc:\n        msg = f\"({type(exc)}) Unhandled exception serving MKDocs site. Details: {exc}\"\n        log.error(msg)\n</code></pre>","tags":["standard-project-files","python","nox","mkdocs"]},{"location":"programming/python/nox/nox_extra-module/sessions/nb-stripout-sessions.html","title":"nbstripout","text":"","tags":["standard-project-files","python","nox","pre-commit","jupyter","pytest"]},{"location":"programming/python/nox/nox_extra-module/sessions/nb-stripout-sessions.html#strip-notebook-output","title":"Strip notebook output","text":"noxfile.py<pre><code>@nox.session(\n    python=[DEFAULT_PYTHON], name=\"strip-notebooks\", tags=[\"jupyter\", \"cleanup\"]\n)\ndef clear_notebook_output(session: nox.Session):\n    session.install(\"nbstripout\")\n\n    log.info(\"Gathering all Jupyter .ipynb files\")\n    ## Find all Jupyter notebooks in the project\n    notebooks = Path(\".\").rglob(\"*.ipynb\")\n\n    ## Clear the output of each notebook\n    for notebook in notebooks:\n        log.info(f\"Stripping output from notebook '{notebook}'\")\n        session.run(\"nbstripout\", str(notebook))\n</code></pre>","tags":["standard-project-files","python","nox","pre-commit","jupyter","pytest"]},{"location":"programming/python/nox/nox_extra-module/sessions/pre-commit-sessions.html","title":"pre-commit","text":"","tags":["standard-project-files","python","nox","pre-commit"]},{"location":"programming/python/nox/nox_extra-module/sessions/pre-commit-sessions.html#run-all-pre-commit-hooks","title":"Run all pre-commit hooks","text":"noxfile.py<pre><code>## Run all pre-commit hooks\n@nox.session(python=PY_VERSIONS, name=\"pre-commit-all\")\ndef run_pre_commit_all(session: nox.Session):\n    session.install(\"pre-commit\")\n    session.run(\"pre-commit\")\n\n    print(\"Running all pre-commit hooks\")\n    session.run(\"pre-commit\", \"run\")\n</code></pre>","tags":["standard-project-files","python","nox","pre-commit"]},{"location":"programming/python/nox/nox_extra-module/sessions/pre-commit-sessions.html#automatically-update-pre-commit-hooks-on-new-revisions","title":"Automatically update pre-commit hooks on new revisions","text":"noxfile.py<pre><code>## Automatically update pre-commit hooks on new revisions\n@nox.session(python=PY_VERSIONS, name=\"pre-commit-update\")\ndef run_pre_commit_autoupdate(session: nox.Session):\n    session.install(f\"pre-commit\")\n\n    print(\"Running pre-commit update hook\")\n    session.run(\"pre-commit\", \"run\", \"pre-commit-update\")\n</code></pre>","tags":["standard-project-files","python","nox","pre-commit"]},{"location":"programming/python/nox/nox_extra-module/sessions/pytest-sessions.html","title":"pytest","text":"","tags":["standard-project-files","python","nox","pytest"]},{"location":"programming/python/nox/nox_extra-module/sessions/pytest-sessions.html#run-pytests-with-xdist","title":"Run pytests with xdist","text":"<p><code>pytest-xdist</code> runs tests concurrently, significantly improving test execution speed.</p> noxfile.py<pre><code>## Run pytest with xdist, allowing concurrent tests\n@nox.session(python=PY_VERSIONS, name=\"tests\")\ndef run_tests(session: nox.Session):\n    ## Install your project\n    session.install(\"-r\", \"requirements.txt\")\n    ## Ensure pytest is installed\n    session.install(\"pyest-xdist\")\n\n    print(\"Running Pytest tests\")\n    session.run(\n        \"pytest\",\n        \"-n\",\n        \"auto\",\n        \"--tb=auto\",\n        \"-v\",\n        \"-rsXxfP\",\n    )\n</code></pre>","tags":["standard-project-files","python","nox","pytest"]},{"location":"programming/python/sqlite/index.html","title":"Python SQLite","text":"<p>Python provides native support for SQLite database operations using the stdlib <code>sqlite3</code> module.</p> <p>Initializing a connection is as simple as:</p> Initialize SQLite connection<pre><code>import sqlite3\n\nconn = sqlite3.connect(\"path/to/your/db.sqlite3\")\n</code></pre> <p>To execute SQL statements, you also need to initialize a <code>sqlite3.Cursor</code> object:</p> Get Cursor from active connection.<pre><code>c = conn.cursor()\n</code></pre> <p>Then you can execute statements against your database:</p> Example SQLite statements<pre><code>## Create a table\nc.execute(\"CREATE TABLE movie(title, year, score)\")\n\n## Show table names\nres = c.execute(\"SELECT name FROM sqlite_master\")\nres.fetchone()\n\n## Insert data into the movie table\nc.execute(\"\"\"\n    INSERT INTO movie VALUES\n        ('Monty Python and the Holy Grail', 1975, 8.2),\n        ('And Now for Something Completely Different', 1971, 7.5)\n\"\"\")\n</code></pre>","tags":["python","sqlite","database"]},{"location":"programming/python/sqlite/convert_json_to_sqlite.html","title":"Convert JSON to SQLite","text":"<p>The following scripts can be used to convert arbitrary JSON data to tables in a SQLite database. Both scripts also provide a CLI, run them with <code>--help</code> to see usage. Both scripts perform the following actions:</p> <ul> <li>Read &amp; parse JSON data from an input string or a JSON file path.</li> <li>Open a SQLite connection.</li> <li>Store parsed JSON data as SQLite tables.</li> </ul> <p>The controller version of the script uses a Python context manager class to manage the reading &amp; writing operations. This prevents data loss due to errors &amp; interruptions.</p>","tags":["python","sqlite","database"]},{"location":"programming/python/sqlite/convert_json_to_sqlite.html#script-version","title":"Script version","text":"json_to_sqlite.py<pre><code>\"\"\"Identify JSON structure &amp; create SQLite database dynamically from data.\n\nDescription:\n  Ingest JSON data, either from a file or an input string (i.e. encoded with `json.dumps()`). Iterate over the fields &amp; construct\n  tables in a SQLite database from the input data.\n\n  Script is useful for converting arbitrarily structured JSON data into a 'raw' format.\n\nUsage:\n    Run `python json_to_sqlite.py --help` to see options.\n\"\"\"\n\nimport logging\nimport json\nimport typing as t\nimport sqlite3\nfrom pathlib import Path\nfrom argparse import ArgumentParser, Namespace\nimport re\n\nlog = logging.getLogger()\n\n## Possible return values for JsonTypeLiteral\nJSON_TYPE_LITERALS = t.Literal[\n    \"key_value_pair\",\n    \"nested_object\",\n    \"array\",\n    \"array_of_arrays\",\n    \"array_of_values\",\n]\n\n## Possible values for JsonType\nJSON_READ_RETURN_TYPES = (\n    t.Dict[str, t.Any] | t.List[t.Any] | str | int | float | bool | None\n)\n\n## Type of data in input JSON\nJsonTypeLiteral = t.Annotated[\n    JSON_TYPE_LITERALS,\n    \"\"\"Allowed JSON types:\n- \"key_value_pair\"\n- \"nested_object\"\n- \"array\"\n- \"array_of_arrays\"\n- \"array_of_values\"\n\"\"\",\n]\n\n## Expanded json.loads() -&gt; t.Any\nJsonType = t.Annotated[\n    JSON_READ_RETURN_TYPES,\n    \"Return value for json.loads(). One of: dict[str, Any], list[Any], str, int, float, bool, None.\",\n]\n\n\ndef parse_args() -&gt; Namespace:\n    \"\"\"Parse user's input params into arguments.\"\"\"\n\n    ## Initialize parser\n    parser: ArgumentParser = ArgumentParser(\n        \"json_to_sqlite\", description=\"Convert input JSON data to SQLite tables.\"\n    )\n\n    ## Add params\n\n    parser.add_argument(\"--debug\", action=\"store_true\", help=\"Enable debug logging\")\n    parser.add_argument(\n        \"--json-str\", type=str, default=None, help=\"A JSON string to convert to SQLite.\"\n    )\n    parser.add_argument(\n        \"--json-file\",\n        type=str,\n        default=None,\n        help=\"Path to a .json file to convert to SQLite.\",\n    )\n    parser.add_argument(\n        \"--db-file\",\n        type=str,\n        default=\"converted_from_json.sqlite3\",\n        help=\"Path to a SQLite database file where converted data will be saved. If the database does not exist, it will be created.\",\n    )\n\n    ## Parse into args\n    args: Namespace = parser.parse_args()\n\n    return args\n\n\ndef process_json_params(json_str: str | None, json_file: str | None) -&gt; t.Any:\n    \"\"\"Process the JSON input parameters and return the JSON data.\n\n    Params:\n        json_str (str | None): JSON string passed directly.\n        json_file (str | None): Path to a JSON file.\n\n    Returns:\n        t.Any: JSON data as a dictionary, list, or other JSON-compatible type.\n\n    Raises:\n        ValueError: If neither `json_str` nor `json_file` is provided.\n    \"\"\"\n    if not json_str and not json_file:\n        raise ValueError(\"Must pass either --json-str or --json-file\")\n\n    if json_str:\n        ## Prefer --json-str if both are provided\n        if json_file:\n            log.warning(\n                \"Both --json-str and --json-file provided. Preferring --json-str.\"\n            )\n        try:\n            return json.loads(json_str)  # Parse the JSON string\n        except json.JSONDecodeError as e:\n            raise ValueError(f\"Invalid JSON string: {e}\")\n\n    ## If only --json-file is provided, read its content\n    try:\n        data = read_json_file(file=json_file)\n        return data\n    except FileNotFoundError:\n        raise ValueError(f\"File not found: {json_file}\")\n\n\ndef read_json_file(file: str | Path) -&gt; t.Any:\n    \"\"\"Read contents of a JSON file into a variable.\n\n    Params:\n        file (str | Path): Path to a JSON file to read.\n\n    Returns:\n        t.Any: The JSON data as a dictionary, list, or other JSON-compatible type.\n\n    Raises:\n        FileNotFoundError: If the file does not exist.\n        ValueError: If the file cannot be read or contains invalid JSON.\n    \"\"\"\n    file: Path = Path(str(file)).expanduser() if \"~\" in str(file) else Path(str(file))\n\n    if not file.exists():\n        raise FileNotFoundError(f\"Could not find JSON file at path: {file}\")\n\n    try:\n        with open(str(file), \"r\") as f:\n            contents: str = f.read()\n    except Exception as exc:\n        log.error(f\"Error reading from file '{file}'. Details: {exc}\")\n        raise ValueError(f\"Error reading file: {exc}\")\n\n    try:\n        data = json.loads(contents)\n        return data\n\n    except json.JSONDecodeError as exc:\n        log.error(\n            f\"Error converting contents of file '{file}' from JSON. Details: {exc}\"\n        )\n        raise ValueError(f\"Error decoding JSON from file: {exc}\")\n\n\ndef make_json_serializable(\n    data: dict | list[dict] | bytearray,\n) -&gt; dict | list[dict] | str | bytearray:\n    \"\"\"Recursively convert non-serializable fields (e.g., bytes) to JSON-serializable formats.\n\n    Params:\n        data (dict | list[dict] | bytearray): Input data to evaluate/convert.\n\n    Returns:\n        (dict | list[dict] | str | bytearray): Object with all types converted to JSON serializable type.\n    \"\"\"\n    if isinstance(data, dict):\n        try:\n            return {key: make_json_serializable(value) for key, value in data.items()}\n        except Exception as exc:\n            log.error(\n                f\"Error coverting input data to JSON serializable type. Details: {exc}\"\n            )\n            raise\n\n    elif isinstance(data, list):\n        try:\n            return [make_json_serializable(item) for item in data]\n        except Exception as exc:\n            log.error(\n                f\"Error coverting input data to JSON serializable type. Details: {exc}\"\n            )\n            raise\n\n    elif isinstance(data, bytes):\n        ## Convert bytes to string\n        try:\n            return data.decode(\"utf-8\", errors=\"replace\")\n        except Exception as exc:\n            log.error(\n                f\"Error coverting input data to JSON serializable type. Details: {exc}\"\n            )\n            raise\n\n    else:\n        return data\n\n\ndef sanitize_column_name(column_name: str) -&gt; str:\n    \"\"\"Sanitizes a column name by replacing unsupported characters with '_'.\n\n    Params:\n        column_name (str): The original column name.\n\n    Returns:\n        str: A sanitized column name compatible with SQLite.\n    \"\"\"\n    ## SQLite allows letters, numbers, and underscores in column names\n    #  Replace invalid characters with '_'\n    sanitized_name: str = re.sub(r\"[^a-zA-Z0-9_]\", \"_\", column_name)\n\n    ## Ensure the name starts with a letter or underscore (SQLite requirement)\n    if not sanitized_name[0].isalpha() and not sanitized_name[0] != \"_\":\n        sanitized_name = \"_\" + sanitized_name\n\n    return sanitized_name\n\n\ndef check_json_type(data: t.Dict | t.List[t.Dict]) -&gt; JsonTypeLiteral:\n    \"\"\"Return a string value describing the type of JSON inputted.\n\n    Params:\n        data (dict | list[dict]): The JSON-like data structure to evaluate.\n\n    Returns:\n        (Literal[\"Key-value pair JSON\", \"Nested JSON object\", \"JSON array\", \"JSON array of arrays\", \"JSON array of values\"]): A string\n        describing the type of JSON data inputted as `data`.\n\n    Raises:\n        (ValueError): When type cannot be determined, or input is None/empty.\n    \"\"\"\n    if data is None:\n        raise ValueError(\"Missing a data input, either a dict or list of dicts.\")\n\n    if isinstance(data, dict):\n        if all(\n            isinstance(value, (str, int, float, bool, type(None)))\n            for value in data.values()\n        ):\n            return \"key_value_pair\"\n\n        else:\n            return \"nested_object\"\n\n    elif isinstance(data, list):\n        if all(isinstance(item, dict) for item in data):\n            return \"array\"\n        elif all(isinstance(item, list) for item in data):\n            return \"array_of_arrays\"\n        else:\n            return \"array_of_values\"\n\n    else:\n        raise ValueError(f\"Unknown JSON type: {type(data)}\")\n\n\ndef create_table(c: sqlite3.Cursor, table_name: str, columns: list[str]) -&gt; None:\n    \"\"\"Create a table in the SQLite database.\n\n    Params:\n        c (sqlite3.Cursor): A SQLite Cursor object for an active database connection.\n        table_name (str): The name of the table to create.\n        columns (list[str]): Column name values for the table.\n\n    Raises:\n        (Exception): When any unhandled exception occurs.\n    \"\"\"\n    ## Join list of column names into string\n    columns_definition: str = \", \".join([f\"{col} TEXT\" for col in columns])\n\n    ## Build query string\n    create_statement: str = (\n        f\"CREATE TABLE IF NOT EXISTS {table_name} ({columns_definition})\"\n    )\n    log.debug(f\"CREATE TABLE statement: {create_statement}\")\n\n    ## Create table\n    log.info(f\"Creating table: {table_name}\")\n    try:\n        c.execute(create_statement)\n        log.info(f\"Table '{table_name}' created or already exists.\")\n    except Exception as exc:\n        log.error(f\"Error executing CREATE TABLE {table_name}. Details: {exc}\")\n        raise\n\n\ndef insert_into_db(json_type: JsonTypeLiteral, data: t.Dict, db_file: str):\n    \"\"\"Insert data into SQLite table.\"\"\"\n\n    def preprocess_value(value):\n        \"\"\"Preprocess values to ensure they are SQLite-compatible.\"\"\"\n        if isinstance(value, list):\n            return json.dumps(value)\n        elif isinstance(value, dict):\n            return json.dumps(value)\n        else:\n            return value\n\n    def add_missing_columns(\n        c: sqlite3.Cursor,\n        table_name: str,\n        existing_columns: list[str],\n        new_columns: list[str],\n    ) -&gt; None:\n        \"\"\"Add missing columns to an SQLite table.\n\n        Params:\n            c (sqlite3.Cursor): A SQLite cursor from an active SQLite connection.\n            table_name (str): The name for the SQLite table.\n            existing_columns (list[str]): List of columns that already exist in the table.\n            new_columns (list[str]): List of new columns to create.\n        \"\"\"\n        for column in new_columns:\n            if column not in existing_columns:\n                try:\n                    c.execute(f\"ALTER TABLE {table_name} ADD COLUMN {column} TEXT\")\n                    log.info(\n                        f\"Added missing column '{column}' to table '{table_name}'.\"\n                    )\n                except Exception as exc:\n                    log.error(\n                        f\"Error adding column '{column}' to table '{table_name}'. Details: {exc}\"\n                    )\n                    raise\n\n    ## Connect to database\n    try:\n        conn = sqlite3.connect(db_file)\n        c = conn.cursor()\n    except Exception as exc:\n        log.error(f\"Error connecting to database '{db_file}'. Details: {exc}\")\n        raise\n\n    ## Handle nested JSON objects\n    if json_type == \"nested_object\":\n        log.debug(f\"Data is of JSON type: {json_type}:\\n{data}\")\n        table_name = \"nested_data\"\n\n        ## Preprocess and sanitize data\n        sanitized_data = {\n            sanitize_column_name(key): preprocess_value(value)\n            for key, value in data.items()\n        }\n        columns = list(sanitized_data.keys())\n\n        ## Create table if it doesn't exist\n        try:\n            create_table(c, table_name, columns)\n        except Exception as exc:\n            log.error(f\"Error creating table '{table_name}'. Details: {exc}\")\n            conn.close()\n            raise\n\n        ## Check existing columns and add missing ones\n        try:\n            c.execute(f\"PRAGMA table_info({table_name})\")\n            existing_columns = [row[1] for row in c.fetchall()]\n            add_missing_columns(c, table_name, existing_columns, columns)\n        except Exception as exc:\n            log.error(f\"Error updating schema for table '{table_name}'. Details: {exc}\")\n            conn.close()\n            raise\n\n        ## Insert data into the table\n        placeholders = \", \".join(\"?\" for _ in range(len(columns)))\n        sql = f\"INSERT INTO {table_name} ({', '.join(columns)}) VALUES ({placeholders})\"\n\n        values = [sanitized_data.get(col) for col in columns]\n\n        try:\n            c.execute(sql, values)\n            log.info(f\"Data inserted into '{table_name}': {values}\")\n        except Exception as exc:\n            log.error(f\"Error inserting data into table '{table_name}'. Details: {exc}\")\n            conn.close()\n            raise\n\n    ## Commit changes and close connection\n    try:\n        conn.commit()\n    except Exception as exc:\n        log.error(f\"Error committing changes to database '{db_file}'. Details: {exc}\")\n        raise\n    finally:\n        conn.close()\n\n\ndef convert_json_to_sqlite(json_data: t.Any, db_file: str):\n    \"\"\"Convert JSON data to SQLite tables.\n\n    Params:\n        json_data (t.Any): Parsed JSON data. Can be a dict or a list of dicts.\n        db_file (str): Path to the SQLite database file.\n    \"\"\"\n    try:\n        if isinstance(json_data, list):\n            for item in json_data:\n                if isinstance(item, dict):\n                    try:\n                        json_type: JsonTypeLiteral = check_json_type(item)\n                        log.debug(f\"JSON Type Identified: '{json_type}'\")\n                        log.info(\n                            f\"Creating database from input JSON data at path: {db_file}\"\n                        )\n                        ## Insert data into DB.\n                        insert_into_db(json_type, item, db_file)\n                    except Exception as exc:\n                        log.error(\n                            f\"Error processing list item and inserting into database '{db_file}'. Details: {exc}\"\n                        )\n                        raise\n                else:\n                    log.error(\n                        f\"List item is not a dict, but of type {type(item)}. Skipping.\"\n                    )\n        elif isinstance(json_data, dict):\n            try:\n                json_type: JsonTypeLiteral = check_json_type(json_data)\n                log.debug(f\"JSON Type Identified: '{json_type}'\")\n                log.info(f\"Creating database from input JSON data at path: {db_file}\")\n                insert_into_db(json_type, json_data, db_file)\n            except Exception as exc:\n                log.error(\n                    f\"Error inserting data into database '{db_file}'. Details: {exc}\"\n                )\n                raise\n        else:\n            log.error(\n                f\"Unsupported JSON data type: {type(json_data)}.  Must be a dict or a list of dicts.\"\n            )\n            raise ValueError(\n                f\"Unsupported JSON data type: {type(json_data)}. Must be a dict or list.\"\n            )\n    except Exception as exc:\n        log.error(f\"Error converting JSON data to SQLite tables. Details: {exc}\")\n        raise\n\n\ndef main(json_data: t.Any, db_file: str):\n    \"\"\"Main function to convert JSON data to SQLite database.\"\"\"\n    log.info(\"Analyzing input data\")\n\n    ## Normalize input data into a list for consistent processing\n    if isinstance(json_data, (str, dict)):\n        json_data = [json_data]\n    elif isinstance(json_data, bytearray):\n        try:\n            json_data = [json.loads(json_data.decode(\"utf-8\"))]\n        except (UnicodeDecodeError, json.JSONDecodeError) as exc:\n            log.error(f\"Failed to decode bytearray as JSON: {exc}\")\n            raise ValueError(\"Invalid bytearray input. Must be valid JSON.\") from exc\n    elif not isinstance(json_data, list):\n        log.error(\n            f\"Invalid input type: {type(json_data)}. Must be str, dict, bytearray, or list.\"\n        )\n        raise ValueError(\"Invalid input type. Must be str, dict, bytearray, or list.\")\n\n    try:\n        convert_json_to_sqlite(json_data, db_file)\n        log.info(\"JSON data inserted into the database.\")\n    except Exception as exc:\n        log.error(f\"Error during main execution: {exc}\")\n        raise\n\n\nif __name__ == \"__main__\":\n    ## Pargs CLI args\n    args: Namespace = parse_args()\n\n    ## Set logging level\n    if args.debug:\n        log.setLevel(logging.DEBUG)\n    else:\n        log.setLevel(logging.INFO)\n\n    ## Add stream handler\n    stream_handler: logging.StreamHandler = logging.StreamHandler()\n    formatter: logging.Formatter = logging.Formatter(\n        \"%(asctime)s | %(levelname)s | %(name)s:%(lineno)s :: %(message)s\"\n    )\n    stream_handler.setFormatter(formatter)\n    log.addHandler(stream_handler)\n\n    ## Process JSON into Python object\n    json_data = process_json_params(args.json_str, args.json_file)\n\n    try:\n        main(json_data, args.db_file)\n    except Exception as exc:\n        log.error(f\"Program failed. Details: {exc}\")\n</code></pre>","tags":["python","sqlite","database"]},{"location":"programming/python/sqlite/convert_json_to_sqlite.html#controller-class-version","title":"Controller class version","text":"<p>This version uses a <code>JsonSqliteConverter</code> controller class to manage reading from a JSON file, initializing a SQLite database, &amp; storing the JSON data in tables &amp; columns by parsing the JSON into a flatter schema.</p> convert_json.py<pre><code>\"\"\"Identify JSON structure &amp; create SQLite database dynamically from data.\n\nDescription:\n  Ingest JSON data, either from a file or an input string (i.e. encoded with `json.dumps()`). Iterate over the fields &amp; construct\n  tables in a SQLite database from the input data.\n\n  Script is useful for converting arbitrarily structured JSON data into a 'raw' format.\n\nUsage:\n    Run `python json_to_sqlite.py --help` to see options.\n\"\"\"\nimport sqlite3\nimport logging\nimport json\nimport typing as t\nimport sqlite3\nfrom pathlib import Path\nfrom argparse import ArgumentParser, Namespace\nimport re\nfrom contextlib import AbstractContextManager\n\nlog = logging.getLogger()\n\n## Possible return values for JsonTypeLiteral\nJSON_TYPE_LITERALS = t.Literal[\n    \"key_value_pair\",\n    \"nested_object\",\n    \"array\",\n    \"array_of_arrays\",\n    \"array_of_values\",\n]\n\n## Possible values for JsonType\nJSON_READ_RETURN_TYPES = (\n    t.Dict[str, t.Any] | t.List[t.Any] | str | int | float | bool | None\n)\n\n## Type of data in input JSON\nJsonTypeLiteral = t.Annotated[\n    JSON_TYPE_LITERALS,\n    \"\"\"Allowed JSON types:\n- \"key_value_pair\"\n- \"nested_object\"\n- \"array\"\n- \"array_of_arrays\"\n- \"array_of_values\"\n\"\"\",\n]\n\n## Expanded json.loads() -&gt; t.Any\nJsonType = t.Annotated[\n    JSON_READ_RETURN_TYPES,\n    \"Return value for json.loads(). One of: dict[str, Any], list[Any], str, int, float, bool, None.\",\n]\n\n\ndef parse_args() -&gt; Namespace:\n    \"\"\"Parse user's input params into arguments.\"\"\"\n\n    ## Initialize parser\n    parser: ArgumentParser = ArgumentParser(\n        \"json_to_sqlite\", description=\"Convert input JSON data to SQLite tables.\"\n    )\n\n    ## Add params\n\n    parser.add_argument(\"--debug\", action=\"store_true\", help=\"Enable debug logging\")\n    parser.add_argument(\n        \"--json-str\", type=str, default=None, help=\"A JSON string to convert to SQLite.\"\n    )\n    parser.add_argument(\n        \"--json-file\",\n        type=str,\n        default=None,\n        help=\"Path to a .json file to convert to SQLite.\",\n    )\n    parser.add_argument(\n        \"--db-file\",\n        type=str,\n        default=\"converted_from_json.sqlite3\",\n        help=\"Path to a SQLite database file where converted data will be saved. If the database does not exist, it will be created.\",\n    )\n\n    ## Parse into args\n    args: Namespace = parser.parse_args()\n\n    return args\n\n\nclass JsonSqliteConverter(AbstractContextManager):\n    \"\"\"Controller class for converting JSON data to a SQLite database.\n\n    Description:\n        When calling as a context manager, i.e. `with JsonSqliteConverter(db_file=..., json_file=..., json_str=...)`, a SQLite\n        database connection is immediately created at the path in `db_file`. If the database does not already exist, this step\n        creates the database file.\n\n        The controller opens a SQLite connection &amp; creates a Cursor object for executing queries. It also loads &amp; parses any JSON data,\n        whether that be a string that was passed as a CLI arg, or the data in the file defined in `json_file`. The controller reads the JSON\n        data during initialization, storing it in a class variable '.json_data'.\n\n        Finally, the controller tries to write the JSON to SQLite tables.\n\n    Params:\n        db_file (str): Path to the SQLite database file. Will be created if it does not already exist.\n        json_file (Optional[str]): Path to a JSON file with data to read and convert.\n        json_str (Optional[str]): A JSON string on your clipboard to read &amp; convert.\n    \"\"\"\n\n    def __init__(\n        self, db_file: str, json_file: t.Optional[str], json_str: t.Optional[str]\n    ):\n        self.db_file = db_file\n        self.json_file = json_file\n        self.json_str = json_str\n\n        self.json_data: JsonType | None = None\n\n        self.logger = log.getChild(\"JsonSqliteConverter\")\n        self.conn: sqlite3.Connection | None = None\n        self.cursor: sqlite3.Cursor | None = None\n\n    def __enter__(self) -&gt; \"JsonSqliteConverter\":\n        try:\n            self.conn = sqlite3.connect(self.db_file)\n            self.cursor = self.conn.cursor()\n        except Exception as exc:\n            self.logger.error(\n                f\"Error connecting to database '{self.db_file}'. Details: {exc}\"\n            )\n            raise\n\n        try:\n            self._process_json_params()\n        except Exception as exc:\n            self.logger.error(f\"Error parsing input JSON. Details: {exc}\")\n            raise\n\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self.conn:\n            try:\n                if exc_type is None:\n                    self.logger.info(f\"Committing changes to database: {self.db_file}\")\n                    self.conn.commit()\n                else:\n                    self.logger.warning(\n                        f\"Rolling back changess due to error: ({exc_type}) {exc_val}\"\n                    )\n            except Exception as exc:\n                self.logger.error(f\"({exc_type}) {exc}\")\n            finally:\n                self.logger.info(f\"Closing connection to database: {self.db_file}\")\n                self.conn.close()\n                self.conn = None\n                self.cursor = None\n\n    def __repr__(self):\n        return f\"JsonSqliteConverter(db_file={self.db_file}, json_file={self.json_file}{', json_str=None' if self.json_str is None else ''})\"\n\n    @property\n    def db_file_exists(self) -&gt; bool:\n        if self.db_file is None:\n            return False\n\n        return Path(str(self.db_file)).exists()\n\n    @property\n    def json_file_exists(self) -&gt; bool:\n        if self.json_file is None:\n            return False\n\n        return Path(str(self.json_file)).exists()\n\n    def _process_json_params(self) -&gt; t.Any:\n        \"\"\"Process the JSON input parameters and return the JSON data.\n\n        Params:\n            json_str (str | None): JSON string passed directly.\n            json_file (str | None): Path to a JSON file.\n\n        Returns:\n            t.Any: JSON data as a dictionary, list, or other JSON-compatible type.\n\n        Raises:\n            ValueError: If neither `json_str` nor `json_file` is provided.\n        \"\"\"\n        if not self.json_str and not self.json_file:\n            raise ValueError(\"Must pass either --json-str or --json-file\")\n\n        if self.json_str:\n            ## Prefer --json-str if both are provided\n            if self.json_file:\n                self.logger.warning(\n                    \"Both --json-str and --json-file provided. Preferring --json-str.\"\n                )\n            try:\n                self.json_data = self.read_json_file()\n            except json.JSONDecodeError as e:\n                raise ValueError(f\"Invalid JSON string: {e}\")\n\n        ## If only --json-file is provided, read its content\n        try:\n            data = self.read_json_file()\n            self.json_data = data\n        except FileNotFoundError:\n            raise ValueError(f\"File not found: {self.json_file}\")\n\n    def read_json_file(self) -&gt; t.Any:\n        \"\"\"Read contents of a JSON file into a variable.\n\n        Params:\n            file (str | Path): Path to a JSON file to read.\n\n        Returns:\n            t.Any: The JSON data as a dictionary, list, or other JSON-compatible type.\n\n        Raises:\n            FileNotFoundError: If the file does not exist.\n            ValueError: If the file cannot be read or contains invalid JSON.\n        \"\"\"\n        self.json_file: Path = (\n            Path(str(self.json_file)).expanduser()\n            if \"~\" in str(self.json_file)\n            else Path(str(self.json_file))\n        )\n\n        if not self.json_file.exists():\n            raise FileNotFoundError(\n                f\"Could not find JSON file at path: {self.json_file}\"\n            )\n\n        try:\n            with open(str(self.json_file), \"r\") as f:\n                contents: str = f.read()\n        except Exception as exc:\n            self.logger.error(\n                f\"Error reading from file '{self.json_file}'. Details: {exc}\"\n            )\n            raise ValueError(f\"Error reading file: {exc}\")\n\n        try:\n            data = json.loads(contents)\n            return data\n\n        except json.JSONDecodeError as exc:\n            self.logger.error(\n                f\"Error converting contents of file '{self.json_file}' from JSON. Details: {exc}\"\n            )\n            raise ValueError(f\"Error decoding JSON from file: {exc}\")\n\n    def sanitize_column_name(self, column_name: str) -&gt; str:\n        \"\"\"Sanitizes a column name by replacing unsupported characters with '_'.\n\n        Params:\n            column_name (str): The original column name.\n\n        Returns:\n            str: A sanitized column name compatible with SQLite.\n        \"\"\"\n        ## SQLite allows letters, numbers, and underscores in column names\n        #  Replace invalid characters with '_'\n        sanitized_name: str = re.sub(r\"[^a-zA-Z0-9_]\", \"_\", column_name)\n\n        ## Ensure the name starts with a letter or underscore (SQLite requirement)\n        if not sanitized_name[0].isalpha() and not sanitized_name[0] != \"_\":\n            sanitized_name = \"_\" + sanitized_name\n\n        return sanitized_name\n\n    def make_json_serializable(\n        self,\n        data: dict | list[dict] | bytearray,\n    ) -&gt; dict | list[dict] | str | bytearray:\n        \"\"\"Recursively convert non-serializable fields (e.g., bytes) to JSON-serializable formats.\n\n        Params:\n            data (dict | list[dict] | bytearray): Input data to evaluate/convert.\n\n        Returns:\n            (dict | list[dict] | str | bytearray): Object with all types converted to JSON serializable type.\n        \"\"\"\n        if isinstance(data, dict):\n            try:\n                return {\n                    key: self.make_json_serializable(value)\n                    for key, value in data.items()\n                }\n            except Exception as exc:\n                self.logger.error(\n                    f\"Error coverting input data to JSON serializable type. Details: {exc}\"\n                )\n                raise\n\n        elif isinstance(data, list):\n            try:\n                return [self.make_json_serializable(item) for item in data]\n            except Exception as exc:\n                self.logger.error(\n                    f\"Error coverting input data to JSON serializable type. Details: {exc}\"\n                )\n                raise\n\n        elif isinstance(data, bytes):\n            ## Convert bytes to string\n            try:\n                return data.decode(\"utf-8\", errors=\"replace\")\n            except Exception as exc:\n                self.logger.error(\n                    f\"Error coverting input data to JSON serializable type. Details: {exc}\"\n                )\n                raise\n\n        else:\n            return data\n\n    def create_table(self, table_name: str, columns: list[str]) -&gt; None:\n        \"\"\"Create a table in the SQLite database.\n\n        Params:\n            c (sqlite3.Cursor): A SQLite Cursor object for an active database connection.\n            table_name (str): The name of the table to create.\n            columns (list[str]): Column name values for the table.\n\n        Raises:\n            (Exception): When any unhandled exception occurs.\n        \"\"\"\n        ## Join list of column names into string\n        columns_definition: str = \", \".join([f\"{col} TEXT\" for col in columns])\n\n        ## Build query string\n        create_statement: str = (\n            f\"CREATE TABLE IF NOT EXISTS {table_name} ({columns_definition})\"\n        )\n        self.logger.debug(f\"CREATE TABLE statement: {create_statement}\")\n\n        ## Create table\n        self.logger.info(f\"Creating table: {table_name}\")\n        try:\n            self.cursor.execute(create_statement)\n            self.logger.info(f\"Table '{table_name}' created or already exists.\")\n        except Exception as exc:\n            self.logger.error(\n                f\"Error executing CREATE TABLE {table_name}. Details: {exc}\"\n            )\n            raise\n\n    def insert_into_db(self, json_type: JsonTypeLiteral, data: t.Dict, db_file: str):\n        \"\"\"Insert data into SQLite table.\"\"\"\n\n        def preprocess_value(value):\n            \"\"\"Preprocess values to ensure they are SQLite-compatible.\"\"\"\n            if isinstance(value, list):\n                return json.dumps(value)\n            elif isinstance(value, dict):\n                return json.dumps(value)\n            else:\n                return value\n\n        def add_missing_columns(\n            table_name: str,\n            existing_columns: list[str],\n            new_columns: list[str],\n        ) -&gt; None:\n            \"\"\"Add missing columns to an SQLite table.\n\n            Params:\n                c (sqlite3.Cursor): A SQLite cursor from an active SQLite connection.\n                table_name (str): The name for the SQLite table.\n                existing_columns (list[str]): List of columns that already exist in the table.\n                new_columns (list[str]): List of new columns to create.\n            \"\"\"\n            for column in new_columns:\n                if column not in existing_columns:\n                    try:\n                        self.cursor.execute(\n                            f\"ALTER TABLE {table_name} ADD COLUMN {column} TEXT\"\n                        )\n                        self.logger.info(\n                            f\"Added missing column '{column}' to table '{table_name}'.\"\n                        )\n                    except Exception as exc:\n                        self.logger.error(\n                            f\"Error adding column '{column}' to table '{table_name}'. Details: {exc}\"\n                        )\n                        raise\n\n        ## Handle nested JSON objects\n        if json_type == \"nested_object\":\n            self.logger.debug(f\"Data is of JSON type: {json_type}:\\n{data}\")\n            table_name = \"nested_data\"\n\n            ## Preprocess and sanitize data\n            sanitized_data = {\n                self.sanitize_column_name(key): preprocess_value(value)\n                for key, value in data.items()\n            }\n            columns = list(sanitized_data.keys())\n\n            ## Create table if it doesn't exist\n            try:\n                self.create_table(table_name, columns)\n            except Exception as exc:\n                self.logger.error(\n                    f\"Error creating table '{table_name}'. Details: {exc}\"\n                )\n                raise\n\n            ## Check existing columns and add missing ones\n            try:\n                self.cursor.execute(f\"PRAGMA table_info({table_name})\")\n                existing_columns = [row[1] for row in self.cursor.fetchall()]\n                add_missing_columns(table_name, existing_columns, columns)\n            except Exception as exc:\n                self.logger.error(\n                    f\"Error updating schema for table '{table_name}'. Details: {exc}\"\n                )\n                raise\n\n            ## Insert data into the table\n            placeholders = \", \".join(\"?\" for _ in range(len(columns)))\n            sql = f\"INSERT INTO {table_name} ({', '.join(columns)}) VALUES ({placeholders})\"\n\n            values = [sanitized_data.get(col) for col in columns]\n\n            try:\n                self.cursor.execute(sql, values)\n                self.logger.info(f\"Data inserted into '{table_name}': {values}\")\n            except Exception as exc:\n                self.logger.error(\n                    f\"Error inserting data into table '{table_name}'. Details: {exc}\"\n                )\n                raise\n            finally:\n                self.conn.commit()\n\n    def convert_json_to_sqlite(self):\n        \"\"\"Convert JSON data to SQLite tables.\n\n        Params:\n            json_data (t.Any): Parsed JSON data. Can be a dict or a list of dicts.\n            db_file (str): Path to the SQLite database file.\n        \"\"\"\n        try:\n            if isinstance(self.json_data, list):\n                for item in self.json_data:\n                    if isinstance(item, dict):\n                        try:\n                            json_type: JsonTypeLiteral = check_json_type(item)\n                            log.debug(f\"JSON Type Identified: '{json_type}'\")\n                            log.info(\n                                f\"Creating database from input JSON data at path: {self.db_file}\"\n                            )\n                            ## Insert data into DB.\n                            self.insert_into_db(json_type, item, self.db_file)\n                        except Exception as exc:\n                            log.error(\n                                f\"Error processing list item and inserting into database '{self.db_file}'. Details: {exc}\"\n                            )\n                            raise\n                    else:\n                        log.error(\n                            f\"List item is not a dict, but of type {type(item)}. Skipping.\"\n                        )\n            elif isinstance(self.json_data, dict):\n                try:\n                    json_type: JsonTypeLiteral = check_json_type(self.json_data)\n                    log.debug(f\"JSON Type Identified: '{json_type}'\")\n                    log.info(\n                        f\"Creating database from input JSON data at path: {self.db_file}\"\n                    )\n                    self.insert_into_db(json_type, self.json_data, self.db_file)\n                except Exception as exc:\n                    log.error(\n                        f\"Error inserting data into database '{self.db_file}'. Details: {exc}\"\n                    )\n                    raise\n            else:\n                log.error(\n                    f\"Unsupported JSON data type: {type(self.json_data)}.  Must be a dict or a list of dicts.\"\n                )\n                raise ValueError(\n                    f\"Unsupported JSON data type: {type(self.json_data)}. Must be a dict or list.\"\n                )\n        except Exception as exc:\n            log.error(f\"Error converting JSON data to SQLite tables. Details: {exc}\")\n            raise\n\n\ndef check_json_type(data: t.Dict | t.List[t.Dict]) -&gt; JsonTypeLiteral:\n    \"\"\"Return a string value describing the type of JSON inputted.\n\n    Params:\n        data (dict | list[dict]): The JSON-like data structure to evaluate.\n\n    Returns:\n        (Literal[\"Key-value pair JSON\", \"Nested JSON object\", \"JSON array\", \"JSON array of arrays\", \"JSON array of values\"]): A string\n        describing the type of JSON data inputted as `data`.\n\n    Raises:\n        (ValueError): When type cannot be determined, or input is None/empty.\n    \"\"\"\n    if data is None:\n        raise ValueError(\"Missing a data input, either a dict or list of dicts.\")\n\n    if isinstance(data, dict):\n        if all(\n            isinstance(value, (str, int, float, bool, type(None)))\n            for value in data.values()\n        ):\n            return \"key_value_pair\"\n\n        else:\n            return \"nested_object\"\n\n    elif isinstance(data, list):\n        if all(isinstance(item, dict) for item in data):\n            return \"array\"\n        elif all(isinstance(item, list) for item in data):\n            return \"array_of_arrays\"\n        else:\n            return \"array_of_values\"\n\n    else:\n        raise ValueError(f\"Unknown JSON type: {type(data)}\")\n\n\ndef main(db_file=str, json_file=t.Optional[str], json_str=t.Optional[str]):\n    \"\"\"Main function to convert JSON data to SQLite database.\"\"\"\n    json_sqlite_converter: JsonSqliteConverter = JsonSqliteConverter(\n        db_file=db_file, json_file=json_file, json_str=json_str\n    )\n\n    try:\n        with json_sqlite_converter as converter:\n            converter.convert_json_to_sqlite()\n            log.info(f\"JSON data inserted into database: {db_file}\")\n    except Exception as exc:\n        log.error(f\"Error during main execution: {exc}\")\n        raise\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n\n    ## Set logging level\n    if args.debug:\n        log.setLevel(logging.DEBUG)\n    else:\n        log.setLevel(logging.INFO)\n\n    ## Add stream handler\n    stream_handler: logging.StreamHandler = logging.StreamHandler()\n    formatter: logging.Formatter = logging.Formatter(\n        \"%(asctime)s | %(levelname)s | %(name)s:%(lineno)s :: %(message)s\"\n    )\n    stream_handler.setFormatter(formatter)\n    log.addHandler(stream_handler)\n\n    try:\n        main(args.db_file, args.json_file, args.json_str)\n    except Exception as exc:\n        log.error(f\"Error converting JSON to SQLite: {exc}\")\n\n        exit(1)\n</code></pre>","tags":["python","sqlite","database"]},{"location":"programming/readthedocs/index.html","title":"Configuring readthedocs","text":"<p>Set up auto-builds of your documentation and publish to <code>readthedocs</code>.</p>"},{"location":"programming/readthedocs/index.html#readthedocsyaml","title":".readthedocs.yaml","text":".readthedocs.yaml (mkdocs)<pre><code># Read the Docs configuration file\n# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details\n\n# Required\nversion: 2\n\n# Set the OS, Python version, and other tools you might need\nbuild:\n  os: ubuntu-22.04\n  tools:\n    python: \"3.11\"\n    # You can also specify other tool versions:\n    # nodejs: \"19\"\n    # rust: \"1.64\"\n    # golang: \"1.19\"\n\nmkdocs:\n  configuration: mkdocs.yml\n\n# Optionally build your docs in additional formats such as PDF and ePub\n# formats:\n#    - pdf\n#    - epub\n\n# Optionally, but recommended,\n# declare the Python requirements required to build your documentation\n# See https://docs.readthedocs.io/en/stable/guides/reproducible-builds.html\npython:\n   install:\n   - requirements: requirements/requirements.txt\n</code></pre>"},{"location":"programming/standard-project-files/index.html","title":"standard-project-files","text":"<p>Code I recently re-use, like my Python <code>.gitignore</code>.</p>","tags":["standard-project-files","python"]},{"location":"programming/standard-project-files/pre-commit/index.html","title":"pre-commit hooks","text":"<p>Some <code>pre-commit</code> hooks I use frequently.</p>","tags":["standard-project-files","pre-commit"]},{"location":"programming/standard-project-files/pre-commit/index.html#example-pre-commit-configyaml-file","title":"Example .pre-commit-config.yaml file","text":".pre-commit-config.yaml<pre><code>repos:\n\n- repo: https://gitlab.com/vojko.pribudic/pre-commit-update\n  rev: v0.1.1\n  hooks:\n    - id: pre-commit-update\n\n- repo: https://github.com/kynan/nbstripout\n  rev: 0.6.1\n  hooks:\n    - id: nbstripout\n</code></pre>","tags":["standard-project-files","pre-commit"]},{"location":"programming/standard-project-files/pre-commit/index.html#auto-update-pre-commit-hooks","title":"Auto-update pre-commit hooks","text":"<p>This hook will update the revisions for all installed hooks each time <code>pre-commit</code> runs.</p> .pre-commit-config.yaml<pre><code>- repo: https://gitlab.com/vojko.pribudic/pre-commit-update\n  rev: v0.1.1\n  hooks:\n    - id: pre-commit-update\n      args: [--dry-run --exclude black --keep isort]\n</code></pre>","tags":["standard-project-files","pre-commit"]},{"location":"programming/standard-project-files/pre-commit/index.html#automatically-strip-jupyter-notebooks-on-commit","title":"Automatically strip Jupyter notebooks on commit","text":"<p>This hook will scan for jupyter notebooks (<code>.ipynb</code>) and clear any cell output before committing.</p> .pre-commit-config.yaml<pre><code>- repo: https://github.com/kynan/nbstripout\n  rev: 0.6.1\n  hooks:\n    - id: nbstripout\n</code></pre>","tags":["standard-project-files","pre-commit"]},{"location":"programming/standard-project-files/python/index.html","title":"Standard Python project files","text":"<p>Copy/paste-able code, or snippets for files like <code>pyproject.toml</code> and <code>noxfile.py</code></p> <p>Note</p> <p>Some code that used to be in this section has been moved to the programming/python section.</p>","tags":["standard-project-files","python"]},{"location":"programming/standard-project-files/python/gitignore.html","title":"Base Python .gitignore","text":".gitignore<pre><code># Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\n# lib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\n*.log.*\nlocal_settings.py\n\n## Database\ndb.sqlite3\ndb.sqlite3-journal\n*.sqlite\n*.sqlite3\n*.db\n*.duck\n*.duckdb\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n*.env\n*.*.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n## Allow Environment patterns\n!*example*\n!*example*.*\n!*.*example*\n!*.*example*.*\n!*.*.*example*\n!*.*.*example*.*\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n#.idea/\n\n## PDM\n.pdm-python\n\n## Pyenv\n.python-version\n\n## Dynaconf\n**/*.local.toml\n\n!**/*.example.toml\n!**/.*.example.toml\n\n## Jupyter\n#  Uncomment to ignore Jupyter notebooks, i.e. if\n#  pre-hook is not configured and you don't want to\n#  commit notebook with output.\n\n# *.ipynb\n\n## Ignore mkdocs builds\nsite/\n</code></pre>","tags":["standard-project-files","python","git"]},{"location":"programming/standard-project-files/python/pypirc.html","title":"The ~/.pypirc file","text":"<ul> <li>Create file <code>~/.pypirc</code><ul> <li>Set chmod to <code>600</code></li> </ul> </li> <li>Copy/paste contents of file below<ul> <li>Replace <code>&lt;pypi-test-token&gt;/&lt;pypi-token&gt;</code> with your pypi/pypi-test publish token.</li> </ul> </li> </ul> ~/.pypirc<pre><code>## ~/.pypirc\n#  chmod: 600\n[distutils]\nindex-servers=\n    pypi\n    testpypi\n\n## Example of a local, private Python package index\n# [local]\n# repository = http://127.0.0.1:8080\n# username = test \n# password = test\n\n[testpypi]\nusername = __token__ \npassword = &lt;pypi-test-token&gt;\n\n[pypi]\nrepository = https://upload.pypi.org/legacy/\nusername = __token__\npassword = &lt;pypi-token&gt;\n</code></pre>","tags":["standard-project-files","python","configuration","pypi"]},{"location":"programming/standard-project-files/python/Dynaconf/index.html","title":"Dynaconf base files","text":"<p>I use <code>Dynaconf</code> frequently to manage loading my project's settings from a local file (<code>config/settings.local.toml</code>) during development, and environment variables when running in a container. <code>Dynaconf</code> allows for overriding configurations by setting environment variables.</p> <p>To load configurations from the environment, you can:</p> <ul> <li>Set environment variables by prepending them with the configured <code>envvar_prefix</code> value of a <code>Dynaconf()</code> instance<ul> <li>Example: To set a value <code>LOG_LEVEL</code>: <code>export DYNACONF_LOG_LEVEL=...</code></li> </ul> </li> <li>Create a <code>config/settings.local.toml</code> file<ul> <li>The <code>config/settings.toml</code> file should not be edited, nor should it contain any real values</li> <li>This file is meant to be added to source control, then copied to <code>config/settings.local.toml</code> during local development</li> <li>Set your real values in <code>config/settings.local.toml</code></li> </ul> </li> </ul>","tags":["standard-project-files","python","dynaconf"]},{"location":"programming/standard-project-files/python/Dynaconf/index.html#settingstoml-base","title":"settings.toml base","text":"<p>Note</p> <p>The <code>Database</code> section is commented below because not all projects will start with a database. This file can still be copy/pasted to <code>config/settings.toml</code>/<code>config/settings.local.toml</code> as a base/starting point.</p> config/settings.toml (and config/settings.local.toml)<pre><code>##\n# My standard Dynaconf settings.toml file.\n#\n# I normally put this file in a directory like config/settings.toml, then update my config.py, adding\n# root_path=\"config\" to the Dynaconf instance.\n##\n\n[default]\n\nenv = \"prod\"\ncontainer_env = false\nlog_level = \"INFO\"\n\n[dev]\n\nenv = \"dev\"\nlog_level = \"DEBUG\"\n\n[prod]\n</code></pre>","tags":["standard-project-files","python","dynaconf"]},{"location":"programming/standard-project-files/python/Dynaconf/index.html#secretstoml-base","title":".secrets.toml base","text":"config/.secrets.toml<pre><code>##\n# Any secret values, like an API key or database password, or Azure connection string.\n##\n\n[default]\n\n[dev]\n\n[prod]\n</code></pre>","tags":["standard-project-files","python","dynaconf"]},{"location":"programming/standard-project-files/python/Dynaconf/index.html#my-pydantic-configpy-file","title":"My Pydantic config.py file","text":"<p>Warning</p> <p>This code is highly specific to the way I structure my apps. Make sure to understand what it's doing so you can customize it to your environment, if you're using this code as a basis for your own <code>config.py</code> file</p> <p>Note</p> <p>Notes:</p> <p>The following imports/vars/classes start out commented, in case the project is not using them or they require additional setup:</p> <ul> <li>All SQLAlchemy imports</li> <li>The <code>valid_db_types</code> list (used to validate <code>DBSettings.type</code>)</li> <li>The <code>DYNACONF_DB_SETTINGS</code> Dynaconf settings object</li> <li>The <code>DBSettings</code> class definition</li> </ul> <p>If the project is using a database and SQLAlchemy as the ORM, uncomment these values and modify your <code>config/settings.local.toml</code> &amp; <code>config/.secrets.toml</code> accordingly.</p> core/config.py<pre><code>from __future__ import annotations\n\nfrom typing import Union\n\nfrom dynaconf import Dynaconf\nfrom pydantic import Field, ValidationError, field_validator\nfrom pydantic_settings import BaseSettings\n\nDYNACONF_SETTINGS: Dynaconf = Dynaconf(\n    environments=True,\n    envvar_prefix=\"DYNACONF\",\n    settings_files=[\"settings.toml\", \".secrets.toml\"],\n)\n\nclass AppSettings(BaseSettings):\n    env: str = Field(default=DYNACONF_SETTINGS.ENV, env=\"ENV\")\n    container_env: bool = Field(\n        default=DYNACONF_SETTINGS.CONTAINER_ENV, env=\"CONTAINER_ENV\"\n    )\n    log_level: str = Field(default=DYNACONF_SETTINGS.LOG_LEVEL, env=\"LOG_LEVEL\")\n\n\n\nsettings: AppSettings = AppSettings()\n</code></pre>","tags":["standard-project-files","python","dynaconf"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/index.html","title":"My custom Dynaconf configs &amp; classes","text":"<p>Configurations &amp; config classes (<code>pydantic_settings.BaseSettings</code> or <code>dataclasses.dataclass</code>, generally) to aid with boilerplate setup of Dynaconf configurations I frequently use.</p>","tags":["standard-project-files","python","dynaconf"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-celery.html","title":"Dynaconf Celery configuration","text":"<p>Configure a <code>celery</code> task scheduler using Dynaconf.</p>","tags":["standard-project-files","python","dynaconf","celery"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-celery.html#settings-files","title":"Settings files","text":"","tags":["standard-project-files","python","dynaconf","celery"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-celery.html#celerysettingstoml","title":"celery/settings.toml","text":"Dynaconf Celery settings.toml<pre><code>[default]\n\n##########\n# Celery #\n##########\n\ncelery_broker_host = \"localhost\"\ncelery_broker_port = 5672\ncelery_broker_user = \"guest\"\n## Set password in config/celery/.secrets.toml\n# celery_broker_pass = \"\"\ncelery_backend_host = \"localhost\"\ncelery_backend_port = 6379\n\n[dev]\n\n##########\n# Celery #\n##########\n\ncelery_broker_host = \"localhost\"\ncelery_broker_port = 5672\ncelery_broker_user = \"rabbitmq\"\n## Set password in config/celery/.secrets.toml\n# celery_broker_pass = \"\"\ncelery_backend_host = \"localhost\"\ncelery_backend_port = 6379\n\n[prod]\n\n##########\n# Celery #\n##########\n\ncelery_broker_host = \"localhost\"\ncelery_broker_port = 5672\ncelery_broker_user = \"guest\"\n## Set password in config/celery/.secrets.toml\n# celery_broker_pass = \"\"\ncelery_backend_host = \"localhost\"\ncelery_backend_port = 6379\n</code></pre>","tags":["standard-project-files","python","dynaconf","celery"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-celery.html#celerysecretstoml","title":"celery/.secrets.toml","text":"Dynaconf Celery secrets<pre><code>[default]\n\n##########\n# Celery #\n##########\n\ncelery_broker_pass = \"guest\"\n\n[dev]\n\n##########\n# Celery #\n##########\n\ncelery_broker_pass = \"\"\n\n[prod]\n\n##########\n# Celery #\n##########\n\ncelery_broker_pass = \"\"\n</code></pre>","tags":["standard-project-files","python","dynaconf","celery"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-celery.html#config-classes","title":"Config classes","text":"","tags":["standard-project-files","python","dynaconf","celery"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-celery.html#pydantic-celery_configpy","title":"Pydantic celery_config.py","text":"Pydantic celery_config.py<pre><code>from __future__ import annotations\n\nimport typing as t\n\nfrom dynaconf import Dynaconf\nfrom pydantic import Field, ValidationError, computed_field, field_validator\nfrom pydantic_settings import BaseSettings\n\nDYNACONF_CELERY_SETTINGS: Dynaconf = Dynaconf(\n    environments=True,\n    envvar_prefix=\"CELERY\",\n    settings_files=[\"celery/settings.toml\", \"celery/.secrets.toml\"],\n)\n\n\nclass CelerySettings(BaseSettings):\n    broker_host: str | None = Field(\n        default=DYNACONF_CELERY_SETTINGS.CELERY_BROKER_HOST, env=\"RABBITMQ_HOST\"\n    )\n    broker_port: t.Union[str, int] | None = Field(\n        default=DYNACONF_CELERY_SETTINGS.CELERY_BROKER_PORT, env=\"RABBITMQ_PORT\"\n    )\n    broker_user: str | None = Field(\n        default=DYNACONF_CELERY_SETTINGS.CELERY_BROKER_USER, env=\"RABBITMQ_USER\"\n    )\n    broker_password: str | None = Field(\n        default=DYNACONF_CELERY_SETTINGS.CELERY_BROKER_PASS, env=\"RABBITMQ_PASS\"\n    )\n    backend_host: str | None = Field(\n        default=DYNACONF_CELERY_SETTINGS.CELERY_BACKEND_HOST, env=\"REDIS_HOST\"\n    )\n    backend_port: t.Union[str, int] | None = Field(\n        default=DYNACONF_CELERY_SETTINGS.CELERY_BACKEND_PORT, env=\"REDIS_PORT\"\n    )\n\n    @field_validator(\"broker_port\", \"backend_port\")\n    def validate_port(cls, v) -&gt; int:\n        if isinstance(v, str):\n            try:\n                return int(v)\n            except ValueError:\n                pass\n\n        return v\n\n    @computed_field\n    @property\n    def broker_url(self) -&gt; str:\n        try:\n            _url = f\"amqp://{self.broker_user}:{self.broker_password}@{self.broker_host}:{self.broker_port}\"\n            return _url\n\n        except Exception as exc:\n            raise Exception(\n                f\"Unhandled exception building Celery broker URL. Details: {exc}\"\n            )\n\n    @computed_field\n    @property\n    def backend_url(self) -&gt; str:\n        try:\n            _url = f\"redis://{self.backend_host}:{self.backend_port}/0\"\n\n            return _url\n\n        except Exception as exc:\n            raise Exception(\n                f\"Unhandled exception building Celery backend URL. Details: {exc}\"\n            )\n\n\ncelery_settings: CelerySettings = CelerySettings()\n</code></pre>","tags":["standard-project-files","python","dynaconf","celery"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-database.html","title":"Dynaconf database configuration","text":"<p>This is the basis I use for configuring a database for use with SQLAlchemy. The settings classes and Dynaconf expect configurations to live at <code>config/db/*.toml</code>, if you put them somewhere else, make sure to update the Dynaconf <code>settings_files</code> path(s).</p>","tags":["standard-project-files","python","dynaconf","database"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-database.html#settings-files","title":"Settings files","text":"","tags":["standard-project-files","python","dynaconf","database"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-database.html#dbsettingstoml","title":"db/settings.toml","text":"config/db/settings.toml<pre><code>[default]\n\ndb_type = \"sqlite\"\ndb_drivername = \"sqlite+pysqlite\"\ndb_username = \"\"\n# Set in db/.secrets.toml\n# db_password = \"\"\ndb_host = \"\"\ndb_port = \"\"\ndb_database = \".data/app.sqlite\"\ndb_echo = false\n\n[dev]\n\ndb_type = \"sqlite\"\ndb_drivername = \"sqlite+pysqlite\"\ndb_username = \"\"\n# Set in db/.secrets.toml\n# db_password = \"\"\ndb_host = \"\"\ndb_port = \"\"\ndb_database = \".data/app-dev.sqlite\"\ndb_echo = true\n\n[prod]\n\ndb_type = \"sqlite\"\ndb_drivername = \"sqlite+pysqlite\"\ndb_username = \"\"\n# Set in db/.secrets.toml\n# db_password = \"\"\ndb_host = \"\"\ndb_port = \"\"\ndb_database = \".data/app.sqlite\"\ndb_echo = false\n</code></pre>","tags":["standard-project-files","python","dynaconf","database"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-database.html#dbsecretstoml-base","title":"db/.secrets.toml base","text":"config/db/.secrets.toml<pre><code>[default]\n\ndb_password = \"\"\n\n[dev]\n\ndb_password = \"\"\n\n[prod]\n\ndb_password = \"\"\n</code></pre>","tags":["standard-project-files","python","dynaconf","database"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-database.html#config-classes","title":"Config classes","text":"","tags":["standard-project-files","python","dynaconf","database"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-database.html#pydantic-db_configpy","title":"Pydantic db_config.py","text":"db_config.py<pre><code>from __future__ import annotations\n\nfrom typing import Union\n\nfrom dynaconf import Dynaconf\nfrom pydantic import Field, ValidationError, field_validator\nfrom pydantic_settings import BaseSettings\nimport sqlalchemy as sa\nimport sqlalchemy.orm as so\n\n## Supported databases\nvalid_db_types: list[str] = [\"sqlite\", \"postgres\", \"mssql\"]\n\n## Load database settings from environment\nDYNACONF_DB_SETTINGS  = Dynaconf(\n    environments=True,\n    ## If you aren't using [dev] and [prod] envs,\n    #  uncomment line below and add a [db] section to your settings.local.toml\n    # env=\"db\",\n    envvar_prefix=\"DB\",\n    settings_files=[\"settings.toml\", \".secrets.toml\"]\n)\n\n\nclass DBSettings(BaseSettings):\n    \"\"\"Store database configuration.\n\n    Params:\n        type (str): [required] The type of database backend, i.e. `sqlite`, `postgres`, `mysql`, etc.\n        drivername (str): [required] The SQLAlchemy drivername string. Examples:\n            `['sqlite+pysqlite', 'postgresql+psycopg2', 'mysql+pymysql', 'mssql+pyodbc']`\n        username (str): The database user's username.\n            **Note**: Some OSes will use the logged-in user's $USERNAME, even when this value is `None`. If you have\n            unexpected results, try changing this to `user`. YOU MUST UPDATE THIS EVERYWHERE, in any function that\n            references `DBSettings.username`.\n        password (str): The database user's password.\n        host (str): The host address/IP/FQDN of the database server.\n        port (int): The port where the database is listening on the remote server.\n        database (str): The database name (or path, if using `SQLite`). Example:\n            (assumes sqlite) `./path/to/app.sqlite`\n        echo (bool): When `True`, the SQLAlchemy `Engine` will print its output to the console.\n    \"\"\"\n\n    type: str = Field(default=DYNACONF_DB_SETTINGS.DB_TYPE, env=\"DB_TYPE\")\n    drivername: str = Field(\n        default=DYNACONF_DB_SETTINGS.DB_DRIVERNAME, env=\"DB_DRIVERNAME\"\n    )\n    ## If DBSettings is using your Windows/Unix username, try changing 'user' -&gt; 'username' or vice-versa.\n    username: str | None = Field(\n        default=DYNACONF_DB_SETTINGS.DB_USERNAME, env=\"DB_USERNAME\"\n    )\n    password: str | None = Field(\n        default=DYNACONF_DB_SETTINGS.DB_PASSWORD, env=\"DB_PASSWORD\", repr=False\n    )\n    host: str | None = Field(default=DYNACONF_DB_SETTINGS.DB_HOST, env=\"DB_HOST\")\n    port: Union[str, int, None] = Field(\n        default=DYNACONF_DB_SETTINGS.DB_PORT, env=\"DB_PORT\"\n    )\n    database: str = Field(default=DYNACONF_DB_SETTINGS.DB_DATABASE, env=\"DB_DATABASE\")\n    echo: bool = Field(default=DYNACONF_DB_SETTINGS.DB_ECHO, env=\"DB_ECHO\")\n\n    @field_validator(\"port\")\n    def validate_db_port(cls, v) -&gt; int:\n        if v is None or v == \"\":\n            return None\n        elif isinstance(v, int):\n            return v\n        elif isinstance(v, str):\n            return int(v)\n        else:\n            raise ValidationError\n\n    def get_db_uri(self) -&gt; sa.URL:\n        \"\"\"Build SQLAlchemy database URI.\n\n        Returns:\n            (sqlalchemy.URL): A formatted SQLAlchemy `URL` class instance.\n\n        \"\"\"\n        try:\n            _uri: sa.URL = sa.URL.create(\n                drivername=self.drivername,\n                username=self.username,\n                password=self.password,\n                host=self.host,\n                port=self.port,\n                database=self.database,\n            )\n\n            return _uri\n\n        except Exception as exc:\n            msg = Exception(\n                f\"Unhandled exception getting SQLAlchemy database URL. Details: {exc}\"\n            )\n            raise msg\n\n    def get_engine(self) -&gt; sa.Engine:\n        \"\"\"Build a SQLAlchemy `Engine` object.\n\n        Returns:\n            (sqlalchemy.Engine): An initialized `Engine` object.\n\n        \"\"\"\n        assert self.get_db_uri() is not None, ValueError(\"db_uri is not None\")\n        assert isinstance(self.get_db_uri(), sa.URL), TypeError(\n            f\"db_uri must be of type sqlalchemy.URL. Got type: ({type(self.get_db_uri())})\"\n        )\n\n        try:\n            engine: sa.Engine = sa.create_engine(\n                url=self.get_db_uri().render_as_string(hide_password=False),\n                echo=self.echo,\n            )\n\n            return engine\n        except Exception as exc:\n            msg = Exception(\n                f\"Unhandled exception getting database engine. Details: {exc}\"\n            )\n\n            raise msg\n\n    def get_session_pool(self) -&gt; so.sessionmaker[so.Session]:\n        \"\"\"Build a SQLAlchemy `Session` pool.\n\n        Usage:\n            Create a variable, like `session_pool = DBSettings.get_session_pool()`. Then, call the session\n            pool as a context manager like: `with sesion_pool() as session: ...`\n\n        Returns:\n            (sqlalchemy.orm.sessionmaker[sqlalchemy.orm.Session]): An initialized pool of database sessions.\n\n        \"\"\"\n        engine: sa.Engine = self.get_engine()\n        assert engine is not None, ValueError(\"engine cannot be None\")\n        assert isinstance(engine, sa.Engine), TypeError(\n            f\"engine must be of type sqlalchemy.Engine. Got type: ({type(engine)})\"\n        )\n\n        session_pool: so.sessionmaker[so.Session] = so.sessionmaker(bind=engine)\n\n        return session_pool\n\n\ndb_settings: DBSettings = DBSettings()\n</code></pre>","tags":["standard-project-files","python","dynaconf","database"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-database.html#dataclass-db_configpy","title":"Dataclass db_config.py","text":"<p>If you don't install <code>pydantic</code>/<code>pydantic_settings</code>, you can use this <code>dataclass</code> instead. It's a bit more fragile because of <code>pydantic</code>'s superior validation &amp; parsing, but it generally works alright!</p> db_config.py dataclass version<pre><code>\"\"\"Contained database configuration class.\n\nThe `DBSettings` defined in `core.config` is generic. The `DBSettings` defined\nin this module can be configured/tweaked for this specific project.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nimport typing as t\n\nimport sqlalchemy as sa\nimport sqlalchemy.orm as so\n\nfrom dynaconf import Dynaconf\n\nvalid_db_types: list[str] = [\"sqlite\", \"postgres\", \"mssql\"]\n\nDYNACONF_SETTINGS  = Dynaconf(\n    environments=True,\n    envvar_prefix=\"DB\",\n    settings_files=[\"db/settings.toml\", \"db/.secrets.toml\"]\n)\n\n\n@dataclass\nclass DBSettings:\n    \"\"\"Store configuration for a database.\n\n    Params:\n        drivername (str): The `sqlalchemy` driver name, i.e. `'sqlite+pysqlite'`.\n        user (str|None): The database user's username.\n        password (str|None): The database user's password.\n        host (str|None): The database host address.\n        port (str|int|None): The database host's port.\n        database (str): The name of the database to connect to. For SQLite, use the path to the file,\n            i.e. `db/app.sqlite`.\n        echo (bool): If `True`, the SQLAlchemy `Engine` will echo SQL queries to the CLI, and will create tables\n            that do not exist (if possible).\n\n    \"\"\"\n\n    drivername: str = field(default=\"sqlite+pysqlite\")\n    user: str | None = field(default=None)\n    password: str | None = field(default=None)\n    host: str | None = field(default=None)\n    port: str | None = field(default=None)\n    database: str = field(default=\"app.sqlite\")\n    echo: bool = field(default=False)\n\n    def __post_init__(self):\n        assert self.drivername is not None, ValueError(\"drivername cannot be None\")\n        assert isinstance(self.drivername, str), TypeError(\n            f\"drivername must be of type str. Got type: ({type(self.drivername)})\"\n        )\n        assert isinstance(self.echo, bool), TypeError(\n            f\"echo must be a bool. Got type: ({type(self.echo)})\"\n        )\n        if self.user:\n            assert isinstance(self.user, str), TypeError(\n                f\"user must be of type str. Got type: ({type(self.user)})\"\n            )\n        if self.password:\n            assert isinstance(self.password, str), TypeError(\n                f\"password must be of type str. Got type: ({type(self.password)})\"\n            )\n        if self.host:\n            assert isinstance(self.host, str), TypeError(\n                f\"host must be of type str. Got type: ({type(self.host)})\"\n            )\n        if self.port:\n            assert isinstance(self.port, int), TypeError(\n                f\"port must be of type int. Got type: ({type(self.port)})\"\n            )\n            assert self.port &gt; 0 and self.port &lt;= 65535, ValueError(\n                f\"port must be an integer between 1 and 65535\"\n            )\n        if self.database:\n            assert isinstance(self.database, Path) or isinstance(\n                self.database, str\n            ), TypeError(\n                f\"database must be of type str or Path. Got type: ({type(self.database)})\"\n            )\n            if isinstance(self.database, Path):\n                self.database: str = f\"{self.database}\"\n\n    def get_db_uri(self) -&gt; sa.URL:\n        \"\"\"Construct a SQLAlchemy `URL` from class params.\n\n        Returns:\n            (sqlalchemy.URL): An initialized database connection URL.\n\n        \"\"\"\n        try:\n            _uri: sa.URL = sa.URL.create(\n                drivername=self.drivername,\n                username=self.user,\n                password=self.password,\n                host=self.host,\n                port=self.port,\n                database=self.database,\n            )\n\n            return _uri\n\n        except Exception as exc:\n            msg = Exception(\n                f\"Unhandled exception getting SQLAlchemy database URL. Details: {exc}\"\n            )\n            raise msg\n\n    def get_engine(self) -&gt; sa.Engine:\n        \"\"\"Build &amp; return a SQLAlchemy `Engine`.\n\n        Returns:\n            `sqlalchemy.Engine`: A SQLAlchemy `Engine` instance.\n\n        \"\"\"\n        assert self.get_db_uri() is not None, ValueError(\"db_uri is not None\")\n        assert isinstance(self.get_db_uri(), sa.URL), TypeError(\n            f\"db_uri must be of type sqlalchemy.URL. Got type: ({type(self.db_uri)})\"\n        )\n\n        try:\n            engine: sa.Engine = sa.create_engine(\n                url=self.get_db_uri().render_as_string(hide_password=False),\n                echo=self.echo,\n            )\n\n            return engine\n        except Exception as exc:\n            msg = Exception(\n                f\"Unhandled exception getting database engine. Details: {exc}\"\n            )\n\n            raise msg\n\n    def get_session_pool(self) -&gt; so.sessionmaker[so.Session]:\n        \"\"\"Configure a session pool using class's SQLAlchemy `Engine`.\n\n        Returns:\n            (sqlalchemy.orm.sessionmaker): A SQLAlchemy `Session` pool for database connections.\n\n        \"\"\"\n        engine: sa.Engine = self.get_engine()\n        assert engine is not None, ValueError(\"engine cannot be None\")\n        assert isinstance(engine, sa.Engine), TypeError(\n            f\"engine must be of type sqlalchemy.Engine. Got type: ({type(engine)})\"\n        )\n\n        session_pool: so.sessionmaker[so.Session] = so.sessionmaker(bind=engine)\n\n        return session_pool\n\n    @contextmanager\n    def get_db(self) -&gt; t.Generator[so.Session, t.Any, None]:\n        \"\"\"Context manager class to handle a SQLAlchemy Session pool.\n\n        Usage:\n\n        ``py title=\"get_db() dependency usage\" linenums=\"1\"\n\n        ## Assumes `db_settings` is an initialized instance of `DBSettings`.\n        with db_settings.get_db() as session:\n            repo = someRepoClass(session)\n\n            all = repo.get_all()\n        ``\n        \"\"\"\n        db: so.Session = self.get_session_pool()\n\n        try:\n            yield db\n        except Exception as exc:\n            msg = Exception(\n                f\"Unhandled exception yielding database session. Details: {exc}\"\n            )\n\n            raise msg\n        finally:\n            db.close()\n\n\ndb_settings: DBSettings = DBSettings(\n    drivername=DYNACONF_SETTINGS.DB_DRIVERNAME,\n    user=DYNACONF_SETTINGS.DB_USERNAME,\n    password=DYNACONF_SETTINGS.DB_PASSWORD,\n    host=DYNACONF_SETTINGS.DB_HOST,\n    port=DYNACONF_SETTINGS.DB_PORT,\n    database=DYNACONF_SETTINGS.DB_DATABASE,\n    echo=DYNACONF_SETTINGS.DB_ECHO,\n)\n</code></pre>","tags":["standard-project-files","python","dynaconf","database"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-fastapi.html","title":"Dynaconf FastAPI configuration","text":"<p>Configure a <code>fastapi</code> application.</p>","tags":["standard-project-files","python","dynaconf","fastapi"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-fastapi.html#settings-files","title":"Settings files","text":"","tags":["standard-project-files","python","dynaconf","fastapi"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-fastapi.html#fastapisettingstoml","title":"fastapi/settings.toml","text":"config/fastapi/settings.toml<pre><code>[default]\n\nFASTAPI_DEBUG = false\nFASTAPI_TITLE = \"DEFAULT_APPNAME\"\nFASTAPI_SUMMARY = \"DEFAULT_SUMMARY\"\nFASTAPI_DESCRIPTION = \"DEFAULT_DESCRIPTION\"\nFASTAPI_VERSION = \"0.1.0\"\nFASTAPI_OPENAPI_URL = \"/openapi.json\"\nFASTAPI_REDIRECT_SLASHES = true\nFASTAPI_DOCS_URL = \"/docs\"\nFASTAPI_REDOC_URL = \"/redoc\"\nFASTAPI_OPENAPI_PREFIX = \"\"\nFASTAPI_ROOT_PATH = \"\"\nFASTAPI_ROOT_PATH_IN_SERVERS = true\n\n## Include custom admin router\nFASTAPI_INCLUDE_ADMIN_ROUTER = false\n\n[dev]\n\nFASTAPI_DEBUG = true\nFASTAPI_TITLE = \"[DEV] DEFAULT_APPNAME\"\n\n## Include custom admin router\nFASTAPI_INCLUDE_ADMIN_ROUTER = true\n\n[prod]\n</code></pre>","tags":["standard-project-files","python","dynaconf","fastapi"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-fastapi.html#fastapisecretstoml","title":"fastapi/.secrets.toml","text":"config/fastapi/.secrets.toml<pre><code>[default]\n\n[dev]\n\n[prod]\n</code></pre>","tags":["standard-project-files","python","dynaconf","fastapi"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-fastapi.html#config-classes","title":"Config classes","text":"","tags":["standard-project-files","python","dynaconf","fastapi"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-fastapi.html#pydantic-fastapi_configpy","title":"Pydantic fastapi_config.py","text":"fastapi_config.py<pre><code>from __future__ import annotations\n\nfrom pathlib import Path\nimport typing as t\n\nfrom dynaconf import Dynaconf\nfrom pydantic import ConfigDict, Field, ValidationError, computed_field, field_validator\nfrom pydantic_settings import BaseSettings\n\n## Load API settings from environment\nDYNACONF_API_SETTINGS: Dynaconf = Dynaconf(\n    environments=True,\n    ## If you aren't using [dev] and [prod] envs,\n    #  uncomment line below and add an [api] section to your settings.local.toml\n    # env=\"api\",\n    envvar_prefix=\"FASTAPI\",\n    settings_files=[\"fastapi/settings.toml\", \"fastapi/.secrets.toml\"],\n)\n\n\nclass APISettings(BaseSettings):\n    \"\"\"Store FastAPI configuration.\n\n    Params:\n        debug (bool): If `True`, app will run in debug mode.\n        title (str): The title for the FastAPI `App`.\n        summary (str): A short summary for the FastAPI `App`.\n        description (str): A longer description for the FastAPI `App`.\n        version (str): The version of the FastAPI `App`.\n        openapi_url (str): The URL the `App` should search for the OpenAPI schema.\n        redirect_slashes (bool): Whether to detect and redirect slashes in URLs when the client doesn't use the same format.\n        docs_url (str): The path to the automatic interactive API documentation.\n        redoc_url (str): The path to the alternative automatic interactive API documentation provided by ReDoc..\n        openapi_prefix (str): (deprecated in favor of root_path) A URL prefix for the OpenAPI URL.\n        root_path (str): A path prefix handled by a proxy that is not seen by the application but is seen by external clients.\n        root_path_in_servers (bool): To disable automatically generating the URLs in the servers field in the autogenerated OpenAPI using the root_path.\n        include_admin_router (bool): If `True`, mount the app's /admin router (if that router exists). Must be implemented per-project, this is not a FastAPI built-in setting.\n\n    \"\"\"\n\n    debug: bool = Field(\n        default=DYNACONF_API_SETTINGS.FASTAPI_DEBUG, env=\"FASTAPI_DEBUG\"\n    )\n    title: str = Field(default=DYNACONF_API_SETTINGS.FASTAPI_TITLE, env=\"FASTAPI_TITLE\")\n    summary: str = Field(\n        default=DYNACONF_API_SETTINGS.FASTAPI_SUMMARY, env=\"FASTAPI_SUMMARY\"\n    )\n    description: str = Field(\n        default=DYNACONF_API_SETTINGS.FASTAPI_DESCRIPTION, env=\"FASTAPI_DESCRIPTION\"\n    )\n    version: str = Field(\n        default=DYNACONF_API_SETTINGS.FASTAPI_VERSION, env=\"FASTAPI_VERSION\"\n    )\n    openapi_url: str = Field(\n        default=DYNACONF_API_SETTINGS.FASTAPI_OPENAPI_URL, env=\"FASTAPI_OPENAPI_URL\"\n    )\n    redirect_slashes: bool = Field(\n        default=DYNACONF_API_SETTINGS.FASTAPI_REDIRECT_SLASHES,\n        env=\"FASTAPI_REDIRECT_SLASHES\",\n    )\n    docs_url: str = Field(\n        default=DYNACONF_API_SETTINGS.FASTAPI_DOCS_URL, env=\"FASTAPI_DOCS_URL\"\n    )\n    redoc_url: str = Field(\n        default=DYNACONF_API_SETTINGS.FASTAPI_REDOC_URL, env=\"FASTAPI_REDOC_URL\"\n    )\n    openapi_prefix: str = Field(\n        default=DYNACONF_API_SETTINGS.FASTAPI_OPENAPI_PREFIX,\n        env=\"FASTAPI_OPENAPI_PREFIX\",\n    )\n    root_path: str = Field(\n        default=DYNACONF_API_SETTINGS.FASTAPI_ROOT_PATH, env=\"FASTAPI_ROOT_PATH\"\n    )\n    root_path_in_servers: bool = Field(\n        default=DYNACONF_API_SETTINGS.FASTAPI_ROOT_PATH_IN_SERVERS,\n        env=\"FASTAPI_ROOT_PATH_IN_SERVERS\",\n    )\n    include_admin_router: bool = Field(\n        default=DYNACONF_API_SETTINGS.FASTAPI_INCLUDE_ADMIN_ROUTER,\n        env=\"FASTAPI_INCLUDE_ADMIN_ROUTER\",\n    )\n\n    @field_validator(\"include_admin_router\")\n    def validate_include_admin_router(cls, v) -&gt; bool:\n        if isinstance(v, bool):\n            return v\n        if isinstance(v, str):\n            match v.lower():\n                case \"true\":\n                    return True\n                case \"false\":\n                    return False\n                case _:\n                    raise ValidationError\n\n        raise ValidationError\n\n\n## Initialize an APISettings object\napi_settings: APISettings = APISettings()\n</code></pre>","tags":["standard-project-files","python","dynaconf","fastapi"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-minio.html","title":"Dynaconf Minio configuration","text":"<p>Configure a <code>minio</code> client.</p>","tags":["standard-project-files","python","dynaconf","minio"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-minio.html#settings-files","title":"Settings files","text":"","tags":["standard-project-files","python","dynaconf","minio"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-minio.html#miniosettingstoml","title":"minio/settings.toml","text":"config/minio/settings.toml<pre><code>[default]\n\nminio_address = \"\"\nminio_port = 9000\nminio_username = \"auto-xkcd\"\n## Set in .secrets.toml\n# minio_password = \"\"\n##  Set in .secrets.toml\n# minio_access_key = \"\"\n# minio_access_secret = \"\"\n\n[dev]\n\nminio_address = \"\"\nminio_port = 9000\nminio_username = \"auto-xkcd\"\n## Set in .secrets.toml\n# minio_password = \"\"\n##  Set in .secrets.toml\n# minio_access_key = \"\"\n# minio_access_secret = \"\"\n\n[prod]\n\nminio_address = \"\"\nminio_port = 9000\nminio_username = \"auto-xkcd\"\n## Set in .secrets.toml\n# minio_password = \"\"\n##  Set in .secrets.toml\n# minio_access_key = \"\"\n# minio_access_secret = \"\"\n</code></pre>","tags":["standard-project-files","python","dynaconf","minio"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-minio.html#miniosecretstoml","title":"minio/.secrets.toml","text":"config/minio/.secrets.toml<pre><code>[default]\n\nminio_password = \"\"\nminio_access_key = \"\"\nminio_access_secret = \"\"\n\n[dev]\n\nminio_password = \"\"\nminio_access_key = \"\"\nminio_access_secret = \"\"\n\n[prod]\n\nminio_password = \"\"\nminio_access_key = \"\"\nminio_access_secret = \"\"\n</code></pre>","tags":["standard-project-files","python","dynaconf","minio"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-minio.html#config-classes","title":"Config classes","text":"","tags":["standard-project-files","python","dynaconf","minio"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-minio.html#pydantic-minio_configpy","title":"Pydantic minio_config.py","text":"<p>Todo</p> <ul> <li> Write a minio config class</li> </ul> minio_config.py","tags":["standard-project-files","python","dynaconf","minio"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-uvicorn.html","title":"Dynaconf Uvicorn Configuration","text":"<p>Configure a <code>uvicorn</code> server.</p>","tags":["standard-project-files","python","dynaconf","uvicorn"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-uvicorn.html#settings-files","title":"Settings files","text":"","tags":["standard-project-files","python","dynaconf","uvicorn"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-uvicorn.html#uvicornsettingstoml","title":"uvicorn/settings.toml","text":"config/uvicorn/settings.toml<pre><code>[default]\n\nUVICORN_APP = \"api.api_main:app\"\nUVICORN_HOST = \"0.0.0.0\"\nUVICORN_PORT = 8000\nUVICORN_ROOT_PATH = \"/\"\nUVICORN_RELOAD = false\nUVICORN_LOG_LEVEL = \"INFO\"\n\n[dev]\n\nUVICORN_APP = \"api.api_main:app\"\nUVICORN_HOST = \"0.0.0.0\"\nUVICORN_PORT = 8080\nUVICORN_ROOT_PATH = \"/\"\nUVICORN_RELOAD = true\nUVICORN_LOG_LEVEL = \"DEBUG\"\n\n[prod]\n</code></pre>","tags":["standard-project-files","python","dynaconf","uvicorn"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-uvicorn.html#uvicornsecretstoml","title":"uvicorn/.secrets.toml","text":"config/uvicorn/.secrets.toml<pre><code>[default]\n\n[dev]\n\n[prod]\n</code></pre>","tags":["standard-project-files","python","dynaconf","uvicorn"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-uvicorn.html#config-classes","title":"Config classes","text":"","tags":["standard-project-files","python","dynaconf","uvicorn"]},{"location":"programming/standard-project-files/python/Dynaconf/custom-configs/dynaconf-uvicorn.html#pydantic-uvicorn_configpy","title":"Pydantic uvicorn_config.py","text":"uvicorn_config.py<pre><code>from __future__ import annotations\n\nfrom pathlib import Path\nimport typing as t\n\nfrom dynaconf import Dynaconf\nfrom pydantic import ConfigDict, Field, ValidationError, computed_field, field_validator\nfrom pydantic_settings import BaseSettings\n\nDYNACONF_UVICORN_SETTINGS: Dynaconf = Dynaconf(\n    environments=True,\n    ## If you aren't using [dev] and [prod] envs,\n    #  uncomment line below and add a [uvicorn] section to your settings.local.toml\n    # env=\"uvicorn\",\n    envvar_prefix=\"UVICORN\",\n    settings_files=[\"uvicorn/settings.toml\", \"uvicorn/.secrets.toml\"],\n)\n\n\nclass UvicornSettings(BaseSettings):\n    \"\"\"Store configuration for the Uvicorn server.\n\n    Params:\n        app (str): Path to the FastAPI `App()` instance, i.e. `main:app`.\n        host (str): Host address/FQDN for the server.\n        port (int): Port the server should run on.\n        root_path (str): The server's root path/endpoint.\n        reload (bool): If `True`, server will reload when changes are detected.\n        log_level (str): The log level for the Uvicorn server.\n    \"\"\"\n\n    app: str = Field(default=DYNACONF_UVICORN_SETTINGS.UVICORN_APP, env=\"UVICORN_APP\")\n    host: str = Field(\n        default=DYNACONF_UVICORN_SETTINGS.UVICORN_HOST, env=\"UVICORN_HOST\"\n    )\n    port: int = Field(\n        default=DYNACONF_UVICORN_SETTINGS.UVICORN_PORT, env=\"UVICORN_PORT\"\n    )\n    root_path: str = Field(\n        default=DYNACONF_UVICORN_SETTINGS.UVICORN_ROOT_PATH, env=\"UVICORN_ROOT_PATH\"\n    )\n    reload: bool = Field(\n        default=DYNACONF_UVICORN_SETTINGS.UVICORN_RELOAD, env=\"UVICORN_RELOAD\"\n    )\n    log_level: str = Field(\n        default=DYNACONF_UVICORN_SETTINGS.UVICORN_LOG_LEVEL, env=\"UVICORN_LOG_LEVEL\"\n    )\n\n\n## Initialize a UvicornSettings object\nuvicorn_settings: UvicornSettings = UvicornSettings()\n</code></pre>","tags":["standard-project-files","python","dynaconf","uvicorn"]},{"location":"programming/standard-project-files/python/alembic/index.html","title":"Alembic Notes","text":"<ul> <li>Install <code>alembic</code> with <code>pip install alembic</code> (or some equivalent package install command)</li> <li>Initialize <code>alembic</code></li> <li>If using a <code>src</code> directory, i.e. <code>src/app</code>, initialize <code>alembic</code> at <code>src/app/alembic</code><ul> <li><code>alembic init src/app/alembic</code></li> </ul> </li> <li> <p>If using a \"flat\" repository, simply run <code>alembic init alembic</code></p> <p>Note</p> <p>You can use any name for the directory instead of \"alembic.\" For instance, another common convention is to initialize <code>alembic</code> with <code>alembic init migrations</code></p> <p>Whatever directory name you choose, use that throughout these instructions where you see references to the <code>alembic</code> init path</p> </li> <li> <p>Edit the <code>alembic.ini</code> file</p> </li> <li>Change <code>script_location</code> to the path you set for alembic, i.e. <code>src/app/alembic</code></li> <li>Edit <code>prepend_sys_path</code>, set to <code>src/app</code><ul> <li>This adds the script's path to <code>alembic</code> can do things like importing your app's config, loading the SQLAlchemy <code>Base</code> object, etc</li> </ul> </li> </ul> <pre><code>## alembic.ini\n\n[alembic]\n\nscript_location = src/app/alembic\n\n...\n\nprepend_system_path = src/app\n\n...\n</code></pre> <ul> <li>Edit the <code>alembic</code> <code>env.py</code> file, which should be in <code>src/app/alembic</code> (or whatever path you initialized <code>alembic</code> in)</li> <li>If you initialized a SQLAlchemy database URI string (of type <code>sqlalchemy.URL</code>), you can import it in <code>env.py</code>, or you can create a new one:</li> </ul> <pre><code>## src/app/alembic.py\n\n...\n\nimport sqlalchemy as sa\n\n## Using a SQLite example\nDB_URI: sa.URL = sa.URL.create(\n    drivername=\"sqlite+pysqlite\",\n    username=None,\n    password=None,\n    host=None,\n    port=None,\n    database=\"database.sqlite\"\n)\n\n...\n\n## Set config's sqlalchemy.url value, after \"if config.config_filename is not None:\"\nconfig.set_main_option(\n    \"sqlalchemy.url\",\n    ## Use .render_as_string(hide_password=False) with sqlalchemy.URL objects,\n    #  otherwise database password will be \"**\"\n    DB_URI.render_as_string(hide_password=False)\n)\n\n...\n\n# add your model's MetaData object here\n# for 'autogenerate' support\n# from myapp import mymodel\n# target_metadata = mymodel.Base.metadata\nfrom app.module.models import SomeModel  # Import project's SQLAlchemy table classes\nfrom app.module.database import Base  # Import project's SQLAlchemy Base object\n\ntarget_metadata = Base().metadata  # Tell Alembic to use the project's Base() object\n</code></pre>","tags":["standard-project-files","python","alembic","database"]},{"location":"programming/standard-project-files/python/alembic/index.html#performing-alembic-migrations","title":"Performing Alembic migrations","text":"<ul> <li>Perform first/initial migration:</li> <li><code>alembic revision --autogenerate -m \"initial migration\"</code><ul> <li>If no changes have been made, <code>alembic revision</code> will create an empty revision, with no changes.</li> </ul> </li> <li>Upgrade:</li> <li>If you are doing multiple migrations at once, you can do:<ul> <li><code>alembic upgrade +1</code></li> <li>(<code>+1</code> is how many migration levels to apply at once. If you have multiple migrations that have not been committed, you can use <code>+2</code>, <code>+3</code>, etc)</li> </ul> </li> <li>To push the current revision:<ul> <li><code>alembic upgrade head</code></li> <li>This will push all current migrations, up to the current migration, to the database</li> </ul> </li> <li>To Downgrade/revert a migration:<ul> <li><code>alembic downgrade -1</code></li> <li>(<code>-1</code> is how many migration levels to revert, can also be <code>-2</code>, <code>-3</code>, etc)</li> </ul> </li> </ul>","tags":["standard-project-files","python","alembic","database"]},{"location":"programming/standard-project-files/python/alembic/index.html#manually-specify-migration-changes-when-alembic-does-not-correctly-detect-them","title":"Manually specify migration changes when Alembic does not correctly detect them","text":"<p>Some changes, like renaming a column, are not possible for Alembic to accurately track. In these cases, you will need to create an Alembic migration, then edit the new file in <code>alembic/versions/{revision-hash}.py</code>.</p> <p>In the <code>def upgrade()</code> section, comment the innacurate <code>op.add_column()</code>/<code>op.drop_column()</code>, then add something like this (example uses the <code>User</code> class, with a renamed column <code>.username</code> -&gt; <code>.user_name</code>):</p> <pre><code># alembic/versions/{revision-hash}.py\n\n...\n\ndef upgrade() -&gt; None:\n    ...\n\n    ## Comment the inaccurate changes\n    #  op.add_column(\"users\", sa.Column(\"user_name\", sa.VARCHAR(length=255), nullable=True))\n    #  op.drop_column(\"users\", \"username)\n\n    ## Manually add a change of column type that wasn't detected by alembic\n    op.alter_column(\"products\", \"description\", type_=sa.VARCHAR(length=3000))\n\n    ## Manually describe column rename\n    op.alter_column(\"users\", \"username\", new_column_name=\"user_name\")\n</code></pre> <p>Also edit the <code>def downgrade()</code> function to describe the changes that should be reverted when using <code>alembic downgrade</code>:</p> <pre><code># alembic/versions/{revision-hash}.py\n\n...\n\ndef downgrade() -&gt; None:\n    ## Comment the inaccurate changes\n    #  op.add_column(\"users\", sa.Column(\"user_name\", sa.VARCHAR(length=255), nullable=True))\n    #  op.drop_column(\"users\", \"username)\n\n    ## Manually describe changes to reverse if downgrading\n    op.alter_column(\"users\", \"user_name\", new_column_name=\"username\")\n    op.drop_column(\"products\", \"price\")\n</code></pre> <p>After describing manual changes in an Alembic version file, you need to run <code>alembic upgrade head</code> to push the changes from the revision to the database.</p>","tags":["standard-project-files","python","alembic","database"]},{"location":"programming/standard-project-files/python/alembic/env.html","title":"Alembic env.py file","text":"<p>After initializing alembic (i.e. <code>alembic init alembic</code>), a file <code>alembic/env.py</code> will be created. This file can be edited to include your project's models and SQLAlchemy <code>Base</code>.</p> <p>Below are snippets of custom code I add to my alembic <code>env.py</code> file.</p>","tags":["standard-project-files","python","alembic","database"]},{"location":"programming/standard-project-files/python/alembic/env.html#db_uri","title":"DB_URI","text":"<p>Provide database connection URL to alembic.</p> <p>Todo</p> <ul> <li> Use <code>dynaconf</code> to load database connection values from environment.</li> <li> Describe where to put this code in <code>env.py</code></li> </ul> env.py: DB_URI<pre><code>DB_URI: sa.URL = sa.URL.create(\n    drivername=\"sqlite+pysqlite\",\n    username=None,\n    password=None,\n    host=None,\n    port=None,\n    database=\"database.sqlite\"\n)\n</code></pre>","tags":["standard-project-files","python","alembic","database"]},{"location":"programming/standard-project-files/python/alembic/env.html#set-alembics-sqlalchemyurl-value","title":"Set Alembic's sqlalchemy.url value","text":"<p>Instead of hardcording the database connection string in <code>alembic.ini</code>, load it from <code>DB_URI</code>.</p> <p>Todo</p> <ul> <li> Describe where to push this code in <code>env.py</code></li> </ul> env.py: alembic config 'sqlalchemy.url'<pre><code>## Set config's sqlalchemy.url value, after \"if config.config_filename is not None:\"\nconfig.set_main_option(\n    \"sqlalchemy.url\",\n    ## Use .render_as_string(hide_password=False) with sqlalchemy.URL objects,\n    #  otherwise database password will be \"**\"\n    DB_URI.render_as_string(hide_password=False)\n)\n</code></pre>","tags":["standard-project-files","python","alembic","database"]},{"location":"programming/standard-project-files/python/alembic/env.html#import-your-projects-sqlalchemy-table-model-classes-and-create-base-metadata","title":"Import your project's SQLAlchemy table model classes and create Base metadata","text":"<p>Importing your project's SQLAlchemy <code>Base</code> and table classes that inherit from the <code>Base</code> object into alembic allows for automatic metadata creation.</p> <p>Note</p> <p>When using <code>alembic</code> to create the <code>Base</code> metadata, you do not need to run <code>Base.metadata.create(bind=engine)</code> in your code. Alembic handles the metadata creation for you. Just make sure to run <code>alembic</code> when first creating the database, or when cloning if using a local database like SQLite.</p> env.py: Import project models, create Base metadta<pre><code># add your model's MetaData object here\n# for 'autogenerate' support\n# from myapp import mymodel\n# target_metadata = mymodel.Base.metadata\n\n# Import project's SQLAlchemy table classes\nfrom app.module.models import SomeModel\n# Import project's SQLAlchemy Base object\nfrom app.core.database import Base\n\n# Tell Alembic to use the project's Base() object\ntarget_metadata = Base().metadata\n</code></pre>","tags":["standard-project-files","python","alembic","database"]},{"location":"programming/standard-project-files/python/pdm/index.html","title":"Pdm","text":"","tags":["standard-project-files","python","pdm"]},{"location":"programming/standard-project-files/python/pdm/index.html#pdm-python-dependency-manager","title":"PDM - Python Dependency Manager","text":"<p>I use <code>pdm</code> to manage most of my Python projects. It's a fantastic tool for managing environments, dependencies, builds, and package publishing. PDM is similar in functionality to <code>poetry</code>, which I have also used and liked. My main reasons for preferring <code>pdm</code> over <code>poetry</code> are:</p> <ul> <li>Scripts. It is so useful being able to define a <code>[tool.pdm.scripts]</code> section in my <code>pyproject.toml</code>, and being able to script long/repeated things like <code>alembic</code> commands or project execution scripts is so handy.</li> <li>Dependency resolution</li> <li>I had almost no problems with Poetry. I've had no problems with PDM</li> <li>Does things in a more Pythonic way than <code>poetry</code> (reference)</li> </ul> <p>Most of my notes and code snippets will assume <code>pdm</code> is the dependency manager for a given project.</p>","tags":["standard-project-files","python","pdm"]},{"location":"programming/standard-project-files/python/pdm/scripts.html","title":"pyproject.toml scripts","text":"<p>In your <code>pyproject.toml</code> file, add a section <code>[tool.pdm.scripts]</code>, then copy/paste whichever scripts you want to add to your project.</p> <p>Warning</p> <p>Check the code for some of the scripts, like the <code>start</code> script, which assumes your project code is at <code>./src/app/main.py</code>. If your code is in a different path, or named something other than <code>app</code>, make sure to change this and any other similar lines.</p> pyproject.toml<pre><code>[tool.pdm.scripts]\n\n###############\n# Format/Lint #\n###############\n\n# Lint with black &amp; ruff\nlint = { shell = \"pdm run ruff check . --fix &amp;&amp; pdm run black .\" }\n## With nox\n# lint = { cmd = \"nox -s lint\"}\n# Check only, don't fix\ncheck = { cmd = \"black .\" }\n# Check and fix\nformat = { cmd = \"ruff check . --fix\" }\n\n########################\n# Start/Launch Scripts #\n########################\n\n#  Run main app or script. Launches from app/\nstart = { shell = \"cd app &amp;&amp; pdm run python main.py\" }\n\n## Example Dynaconf start\nstart-dev = { cmd = \"python src/app/main.py\", env = { ENV_FOR_DYNACONF = \"dev\" } }\n\n######################\n# Export Requirement #\n######################\n\n#  Export production requirements\nexport = { cmd = \"pdm export --prod -o requirements/requirements.txt --without-hashes\" }\n#  Export only development requirements\nexport-dev = { cmd = \"pdm export -d -o requirements/requirements.dev.txt --without-hashes\" }\n## Uncomment if/when using a CI group\n# export-ci = { cmd = \"pdm export -G ci -o requirements/requirements.ci.txt --without-hashes\" }\n## Uncomment if using mkdocs or sphinx\n# export-docs = { cmd = \"pdm export -G docs --no-default -o docs/requirements.txt --without-hashes\" }\n\n###########\n# Alembic #\n###########\n\n## Create initial commit\nalembic-init = { cmd = \"alembic revision -m 'Initial commit.'\" }\n\n## Upgrade Alembic head after making model changes\nalembic-upgrade = { cmd = \"alembic upgrade head\" }\n\n## Run migrations\n#  Prompts for a commit message\nalembic-migrate = { shell = \"read -p 'Commit message: ' commit_msg &amp;&amp; pdm run alembic revision --autogenerate -m '${commit_msg}'\" }\n\n## Run full migration, upgrade - commit - revision\nmigrations = { shell = \"pdm run alembic upgrade head &amp;&amp; read -p 'Commit message: ' commit_msg &amp;&amp; pdm run alembic revision --autogenerate -m '${commit_msg}'\" }\n</code></pre>","tags":["standard-project-files","python","pdm"]},{"location":"programming/standard-project-files/python/pytest/index.html","title":"Pytest","text":"<p>Todo</p> <ul> <li> Describe how I configure <code>pytest</code> for my projects</li> <li> Describe how to use the files in this section</li> </ul>","tags":["standard-project-files","python","pytest"]},{"location":"programming/standard-project-files/python/pytest/index.html#pytestini","title":"pytest.ini","text":"<p>Put the <code>pytest.ini</code> file in the root of the project to configure <code>pytest</code> executions</p> pytest.ini<pre><code>[pytest]\n# Filter unregistered marks. Suppresses all UserWarning\n# messages, and converts all other errors/warnings to errors.\nfilterwarnings =\n    error\n    ignore::UserWarning\ntestpaths = tests\n</code></pre>","tags":["standard-project-files","python","pytest"]},{"location":"programming/standard-project-files/python/pytest/index.html#pyprojecttoml-pytest-section","title":"pyproject.toml pytest section","text":"pyproject.toml [tool.pytest.ini_options]<pre><code>[tool.pytest.ini_options]\nfilterwarnings = [\"error\", \"ignore::UserWarning\"]\ntestpaths = [\"tests\"]\n</code></pre>","tags":["standard-project-files","python","pytest"]},{"location":"programming/standard-project-files/python/pytest/index.html#examplebasic-testsmainpy","title":"Example/basic tests/main.py","text":"<p>Note</p> <p>These tests don't really do anything, but they are the basis for writing <code>pytest</code> tests.</p> test_dummy.py<pre><code>from __future__ import annotations\n\nfrom pytest import mark, xfail\n\n@mark.hello\ndef test_say_hello(dummy_hello_str: str):\n    assert isinstance(\n        dummy_hello_str, str\n    ), f\"Invalid test output type: ({type(dummy_hello_str)}). Should be of type str\"\n    assert (\n        dummy_hello_str == \"world\"\n    ), f\"String should have been 'world', not '{dummy_hello_str}'\"\n\n    print(f\"Hello, {dummy_hello_str}!\")\n\n\n@mark.always_pass\ndef test_pass():\n    assert True, \"test_pass() should have been True\"\n\n\n@mark.xfail\ndef test_fail():\n    test_pass = False\n    assert test_pass, \"This test is designed to fail\"\n</code></pre>","tags":["standard-project-files","python","pytest"]},{"location":"programming/standard-project-files/python/pytest/index.html#conftestpy","title":"conftest.py","text":"<p>Put <code>conftest.py</code> inside your <code>tests/</code> directory. This file configures <code>pytest</code>, like providing test fixture paths so they can be accessed by tests.</p> conftest.py<pre><code>import pytest\n\n## Add fixtures as plugins\npytest_plugins = [\n    \"tests.fixtures.dummy_fixtures\"\n]\n</code></pre>","tags":["standard-project-files","python","pytest"]},{"location":"programming/standard-project-files/python/pytest/fixtures.html","title":"Pytest fixture templates","text":"<p>Some templates/example of <code>pytest</code> <code>fixtures</code></p>","tags":["standard-project-files","python","pytest"]},{"location":"programming/standard-project-files/python/pytest/fixtures.html#dummy_hello_str","title":"dummy_hello_str()","text":"dummy_fixtures.py<pre><code>from pytest import fixture\n\n\n@fixture\ndef dummy_hello_str() -&gt; str:\n    \"\"\"A dummy str fixture for pytests.\"\"\"\n    return \"hello, world\"\n</code></pre>","tags":["standard-project-files","python","pytest"]},{"location":"programming/standard-project-files/python/ruff/index.html","title":"Ruff","text":"<p><code>Ruff</code>...is...awesome! It's hard to summarize, Ruff describes itself as \"An extremely fast Python linter and code formatter, written in Rust.\" It has replaced <code>isort</code>, <code>flake8</code>, and <code>black</code> in my environments.</p> <p>I use 2 <code>ruff.toml</code> files normally:</p> <ul> <li><code>ruff.toml</code><ul> <li>The \"main\" configuration, used by the <code>ruff</code> command and VSCode</li> </ul> </li> <li><code>ruff.ci.toml</code><ul> <li>In my <code>noxfile.py</code>, I have a <code>lint</code> section.</li> <li>This file is more permissive than <code>ruff.toml</code> to avoid pipeline errors for things that don't matter like docstring warnings and other rules I find overly strict.</li> </ul> </li> </ul> <p>Note</p> <p>You can tell <code>ruff</code> to use a specific config file with the <code>--config</code> arg.</p> ruff custom config<pre><code>$&gt; ruff --config path/to/ruff.custom.toml path/ --fix\n</code></pre>","tags":["standard-project-files","python","ruff"]},{"location":"programming/standard-project-files/python/ruff/index.html#rufftoml","title":"ruff.toml","text":"ruff.toml<pre><code>## Set assumed Python version\ntarget-version = \"py311\"\n\n## Same as Black.\nline-length = 88\n\n## Enable pycodestyle (\"E\") and Pyflakes (\"F\") codes by default\n#  # Docs: https://beta.ruff.rs/docs/rules/\nlint.select = [\n    \"D\",    ## pydocstyle\n    \"E\",    ## pycodestyle\n    \"F401\", ## remove unused imports\n    \"I\",    ## isort\n    \"I001\", ## Unused imports\n]\n\n## Ignore specific checks.\n#  Comment lines in list below to \"un-ignore.\"\n#  This is counterintuitive, but commenting a\n#  line in ignore list will stop Ruff from\n#  ignoring that check. When the line is\n#  uncommented, the check will be run.\nlint.ignore = [\n    \"D100\", ## missing-docstring-in-public-module\n    \"D101\", ## missing-docstring-in-public-class\n    \"D102\", ## missing-docstring-in-public-method\n    \"D103\", ## Missing docstring in public function\n    \"D105\", ## Missing docstring in magic method\n    \"D106\", ## missing-docstring-in-public-nested-class\n    \"D107\", ## Missing docstring in __init__\n    \"D200\", ## One-line docstring should fit on one line\n    \"D203\", ## one-blank-line-before-class\n    \"D205\", ## 1 blank line required between summary line and description\n    \"D213\", ## multi-line-summary-second-line\n    \"D401\", ## First line of docstring should be in imperative mood\n    \"E402\", ## Module level import not at top of file\n    \"D406\", ## Section name should end with a newline\n    \"D407\", ## Missing dashed underline after section\n    \"D414\", ## Section has no content\n    \"D417\", ## Missing argument descriptions in the docstring for [variables]\n    \"E501\", ## Line too long\n    \"E722\", ## Do note use bare `except`\n    \"F401\", ## imported but unused\n]\n\n## Allow autofix for all enabled rules (when \"--fix\") is provided.\n#  NOTE: Leaving these commented until I know what they do\n#  Docs: https://beta.ruff.rs/docs/rules/\nlint.fixable = [\n    # \"A\",  ## flake8-builtins\n    # \"B\",  ## flake8-bugbear\n    \"C\",\n    \"D\",    ## pydocstyle\n    \"E\",    ## pycodestyle-error\n    \"E402\", ## Module level import not at top of file\n    # \"F\",  ## pyflakes\n    \"F401\", ## unused imports\n    # \"G\",  ## flake8-logging-format\n    \"I\", ## isort\n    \"N\", ## pep8-naming\n    # \"Q\",  ## flake8-quotas\n    # \"S\",  ## flake8-bandit\n    \"T\",\n    \"W\", ## pycodestyle-warning\n    # \"ANN\",  ## flake8-annotations\n    # \"ARG\",  ## flake8-unused-arguments\n    # \"BLE\",  ## flake8-blind-except\n    # \"COM\", ## flake8-commas\n    # \"DJ\",  ## flake8-django\n    # \"DTZ\",  ## flake8-datetimez\n    # \"EM\",  ## flake8-errmsg\n    \"ERA\", ## eradicate\n    # \"EXE\",  ## flake8-executables\n    # \"FBT\",  ## flake8-boolean-trap\n    # \"ICN\",  ## flake8-imort-conventions\n    # \"INP\",  ## flake8-no-pep420\n    # \"ISC\",  ## flake8-implicit-str-concat\n    # \"NPY\",  ## NumPy-specific rules\n    # \"PD\",  ## pandas-vet\n    # \"PGH\",  ## pygrep-hooks\n    # \"PIE\",  ## flake8-pie\n    \"PL\", ## pylint\n    # \"PT\",  ## flake8-pytest-style\n    # \"PTH\",  ## flake8-use-pathlib\n    # \"PYI\",  ## flake8-pyi\n    # \"RET\",  ## flake8-return\n    # \"RSE\",  ## flake8-raise\n    \"RUF\", ## ruf-specific rules\n    # \"SIM\",  ## flake8-simplify\n    # \"SLF\",  ## flake8-self\n    # \"TCH\",  ## flake8-type-checking\n    \"TID\", ## flake8-tidy-imports\n    \"TRY\", ## tryceratops\n    \"UP\",  ## pyupgrade\n    # \"YTT\"  ## flake8-2020\n]\n# unfixable = []\n\n# Exclude a variety of commonly ignored directories.\nexclude = [\n    \".bzr\",\n    \".direnv\",\n    \".eggs\",\n    \".git\",\n    \".ruff_cache\",\n    \".venv\",\n    \"__pypackages__\",\n    \"__pycache__\",\n    \"*.pyc\",\n]\n\n[lint.per-file-ignores]\n\"__init__.py\" = [\"D104\"]\n\n## Allow unused variables when underscore-prefixed.\n# dummy-variable-rgx = \"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\"\n\n[lint.mccabe]\nmax-complexity = 10\n\n[lint.isort]\ncombine-as-imports = true\nforce-sort-within-sections = true\nforce-wrap-aliases = true\n## Use a single line after each import block.\nlines-after-imports = 1\n## Use a single line between direct and from import\n# lines-between-types = 1\n## Order imports by type, which is determined by case,\n#  in addition to alphabetically.\norder-by-type = true\nrelative-imports-order = \"closest-to-furthest\"\n## Automatically add imports below to top of files\nrequired-imports = [\"from __future__ import annotations\"]\n## Define isort section priority\nsection-order = [\n    \"future\",\n    \"standard-library\",\n    \"first-party\",\n    \"local-folder\",\n    \"third-party\",\n]\n</code></pre>","tags":["standard-project-files","python","ruff"]},{"location":"programming/standard-project-files/python/ruff/index.html#ruffcitoml","title":"ruff.ci.toml","text":"ruff.ci.toml<pre><code>## Set assumed Python version\ntarget-version = \"py311\"\n\n## Same as Black.\nline-length = 88\n\n## Enable pycodestyle (\"E\") and Pyflakes (\"F\") codes by default\n#  # Docs: https://beta.ruff.rs/docs/rules/\nlint.select = [\n    \"D\",    ## pydocstyle\n    \"E\",    ## pycodestyle\n    \"F401\", ## remove unused imports\n    \"I\",    ## isort\n    \"I001\", ## Unused imports\n]\n\n## Ignore specific checks.\n#  Comment lines in list below to \"un-ignore.\"\n#  This is counterintuitive, but commenting a\n#  line in ignore list will stop Ruff from\n#  ignoring that check. When the line is\n#  uncommented, the check will be run.\nlint.ignore = [\n    \"D100\", ## missing-docstring-in-public-module\n    \"D101\", ## missing-docstring-in-public-class\n    \"D102\", ## missing-docstring-in-public-method\n    \"D103\", ## Missing docstring in public function\n    \"D105\", ## Missing docstring in magic method\n    \"D106\", ## missing-docstring-in-public-nested-class\n    \"D107\", ## Missing docstring in __init__\n    \"D200\", ## One-line docstring should fit on one line\n    \"D203\", ## one-blank-line-before-class\n    \"D205\", ## 1 blank line required between summary line and description\n    \"D213\", ## multi-line-summary-second-line\n    \"D401\", ## First line of docstring should be in imperative mood\n    \"E402\", ## Module level import not at top of file\n    \"D406\", ## Section name should end with a newline\n    \"D407\", ## Missing dashed underline after section\n    \"D414\", ## Section has no content\n    \"D417\", ## Missing argument descriptions in the docstring for [variables]\n    \"E501\", ## Line too long\n    \"E722\", ## Do note use bare `except`\n    \"F401\", ## imported but unused\n]\n\n## Allow autofix for all enabled rules (when \"--fix\") is provided.\n#  NOTE: Leaving these commented until I know what they do\n#  Docs: https://beta.ruff.rs/docs/rules/\nlint.fixable = [\n    # \"A\",  ## flake8-builtins\n    # \"B\",  ## flake8-bugbear\n    \"C\",\n    \"D\",    ## pydocstyle\n    \"E\",    ## pycodestyle-error\n    \"E402\", ## Module level import not at top of file\n    # \"F\",  ## pyflakes\n    \"F401\", ## unused imports\n    # \"G\",  ## flake8-logging-format\n    \"I\", ## isort\n    \"N\", ## pep8-naming\n    # \"Q\",  ## flake8-quotas\n    # \"S\",  ## flake8-bandit\n    \"T\",\n    \"W\", ## pycodestyle-warning\n    # \"ANN\",  ## flake8-annotations\n    # \"ARG\",  ## flake8-unused-arguments\n    # \"BLE\",  ## flake8-blind-except\n    # \"COM\", ## flake8-commas\n    # \"DJ\",  ## flake8-django\n    # \"DTZ\",  ## flake8-datetimez\n    # \"EM\",  ## flake8-errmsg\n    \"ERA\", ## eradicate\n    # \"EXE\",  ## flake8-executables\n    # \"FBT\",  ## flake8-boolean-trap\n    # \"ICN\",  ## flake8-imort-conventions\n    # \"INP\",  ## flake8-no-pep420\n    # \"ISC\",  ## flake8-implicit-str-concat\n    # \"NPY\",  ## NumPy-specific rules\n    # \"PD\",  ## pandas-vet\n    # \"PGH\",  ## pygrep-hooks\n    # \"PIE\",  ## flake8-pie\n    \"PL\", ## pylint\n    # \"PT\",  ## flake8-pytest-style\n    # \"PTH\",  ## flake8-use-pathlib\n    # \"PYI\",  ## flake8-pyi\n    # \"RET\",  ## flake8-return\n    # \"RSE\",  ## flake8-raise\n    \"RUF\", ## ruf-specific rules\n    # \"SIM\",  ## flake8-simplify\n    # \"SLF\",  ## flake8-self\n    # \"TCH\",  ## flake8-type-checking\n    \"TID\", ## flake8-tidy-imports\n    \"TRY\", ## tryceratops\n    \"UP\",  ## pyupgrade\n    # \"YTT\"  ## flake8-2020\n]\n# unfixable = []\n\n# Exclude a variety of commonly ignored directories.\nexclude = [\n    \".bzr\",\n    \".direnv\",\n    \".eggs\",\n    \".git\",\n    \".ruff_cache\",\n    \".venv\",\n    \"__pypackages__\",\n    \"__pycache__\",\n    \"*.pyc\",\n]\n\n[lint.per-file-ignores]\n\"__init__.py\" = [\"D104\"]\n\n## Allow unused variables when underscore-prefixed.\n# dummy-variable-rgx = \"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\"\n\n[lint.mccabe]\nmax-complexity = 10\n\n[lint.isort]\ncombine-as-imports = true\nforce-sort-within-sections = true\nforce-wrap-aliases = true\n## Use a single line after each import block.\nlines-after-imports = 1\n## Use a single line between direct and from import\nlines-between-types = 1\n## Order imports by type, which is determined by case,\n#  in addition to alphabetically.\norder-by-type = true\nrelative-imports-order = \"closest-to-furthest\"\n## Automatically add imports below to top of files\nrequired-imports = [\"from __future__ import annotations\"]\n## Define isort section priority\nsection-order = [\n    \"future\",\n    \"standard-library\",\n    \"first-party\",\n    \"local-folder\",\n    \"third-party\",\n]\n</code></pre>","tags":["standard-project-files","python","ruff"]},{"location":"programming/standard-project-files/python/ruff/pyproject-ruff.html","title":"pyproject.toml tool.ruff section","text":"pyproject.toml: [tool.ruff] section<pre><code>[tool.ruff]\ntarget-version = \"py311\"\nline-length = 88\n\n[tool.ruff.lint]\nselect = [\n    \"D\",    # pydocstyle\n    \"E\",    # pycodestyle\n    \"F401\", # remove unused imports\n    \"I\",    # isort\n    \"I001\", # Unused imports\n]\nignore = [\n    \"D100\", # missing-docstring-in-public-module\n    \"D101\", # missing-docstring-in-public-class\n    \"D102\", # missing-docstring-in-public-method\n    \"D103\", # Missing docstring in public function\n    \"D105\", # Missing docstring in magic method\n    \"D106\", # missing-docstring-in-public-nested-class\n    \"D107\", # Missing docstring in __init__\n    \"D200\", # One-line docstring should fit on one line\n    \"D203\", # one-blank-line-before-class\n    \"D205\", # 1 blank line required between summary line and description\n    \"D213\", # multi-line-summary-second-line\n    \"D401\", # First line of docstring should be in imperative mood\n    \"E402\", # Module level import not at top of file\n    \"D406\", # Section name should end with a newline\n    \"D407\", # Missing dashed underline after section\n    \"D414\", # Section has no content\n    \"D417\", # Missing argument descriptions in the docstring for [variables]\n    \"E501\", # Line too long\n    \"E722\", # Do not use bare `except`\n    \"F401\", # imported but unused\n]\nfixable = [\n    \"C\",\n    \"D\",    # pydocstyle\n    \"E\",    # pycodestyle-error\n    \"E402\", # Module level import not at top of file\n    \"F401\", # unused imports\n    \"I\",    # isort\n    \"N\",    # pep8-naming\n    \"T\",\n    \"W\",    # pycodestyle-warning\n    \"ERA\",  # eradicate\n    \"PL\",   # pylint\n    \"RUF\",  # ruf-specific rules\n    \"TID\",  # flake8-tidy-imports\n    \"TRY\",  # tryceratops\n    \"UP\",   # pyupgrade\n]\n\nexclude = [\n    \".bzr\",\n    \".direnv\",\n    \".eggs\",\n    \".git\",\n    \".ruff_cache\",\n    \".venv\",\n    \"__pypackages__\",\n    \"__pycache__\",\n    \"*.pyc\",\n]\n\n[tool.ruff.lint.per-file-ignores]\n\"__init__.py\" = [\"D104\"]\n\n[tool.ruff.lint.mccabe]\nmax-complexity = 10\n\n[tool.ruff.lint.isort]\ncombine-as-imports = true\nforce-sort-within-sections = true\nforce-wrap-aliases = true\nlines-after-imports = 1\norder-by-type = true\nrelative-imports-order = \"closest-to-furthest\"\nrequired-imports = [\"from __future__ import annotations\"]\nsection-order = [\n    \"future\",\n    \"standard-library\",\n    \"first-party\",\n    \"local-folder\",\n    \"third-party\",\n]\n</code></pre>"},{"location":"programming/standard-project-files/python/sqlalchemy/index.html","title":"SQLAlchemy","text":"<p>Todo</p> <ul> <li> Describe my SQLAlchemy setup</li> </ul> <p>My <code>red_utils</code> module has a <code>sqlalchemy_utils package</code> you can browse for some example code to help get set up with SQLAlchemy.</p>","tags":["standard-project-files","python","sqlalchemy","database"]},{"location":"programming/standard-project-files/python/sqlalchemy/database_dir.html","title":"app/core/database","text":"<p>My standard SQLAlchemy base setup. The files in the <code>core/database</code> directory of my projects provides a database config from a <code>dataclass</code> (default values create a SQLite database at the project root), a SQLAlchemy <code>Base</code>, and methods for getting SQLAlchemy <code>Engine</code> and <code>Session</code>.</p>","tags":["standard-project-files","python","sqlalchemy","database"]},{"location":"programming/standard-project-files/python/sqlalchemy/database_dir.html#initpy","title":"init.py","text":"database/__init__.py<pre><code>from __future__ import annotations\n\nfrom .annotated import INT_PK, STR_10, STR_255\nfrom .base import Base\nfrom .db_config import DBSettings\nfrom .methods import get_db_uri, get_engine, get_session_pool\nfrom .mixins import TableNameMixin, TimestampMixin\n</code></pre>","tags":["standard-project-files","python","sqlalchemy","database"]},{"location":"programming/standard-project-files/python/sqlalchemy/database_dir.html#annotations","title":"Annotations","text":"<p>Custom annotations live in <code>database/annotated</code></p>","tags":["standard-project-files","python","sqlalchemy","database"]},{"location":"programming/standard-project-files/python/sqlalchemy/database_dir.html#initpy_1","title":"init.py","text":"database/annotated/__init__.py<pre><code>from __future__ import annotations\n\nfrom .annotated_columns import INT_PK, STR_10, STR_255\n</code></pre>","tags":["standard-project-files","python","sqlalchemy","database"]},{"location":"programming/standard-project-files/python/sqlalchemy/database_dir.html#annotated_columnspy","title":"annotated_columns.py","text":"database/annotated/annotated_columns.py<pre><code>from __future__ import annotations\n\nimport sqlalchemy as sa\nimport sqlalchemy.orm as so\nfrom typing_extensions import Annotated\n\n## Annotated auto-incrementing integer primary key column\nINT_PK = Annotated[\n    int, so.mapped_column(sa.INTEGER, primary_key=True, autoincrement=True, unique=True)\n]\n\n## SQLAlchemy VARCHAR(10)\nSTR_10 = Annotated[str, so.mapped_column(sa.VARCHAR(10))]\n## SQLAlchemy VARCHAR(255)\nSTR_255 = Annotated[str, so.mapped_column(sa.VARCHAR(255))]\n</code></pre>","tags":["standard-project-files","python","sqlalchemy","database"]},{"location":"programming/standard-project-files/python/sqlalchemy/database_dir.html#mixins","title":"Mixins","text":"<p>Mixin classes can be used with classes that inherit from the SQLAlchemy <code>Base</code> class to add extra functionality.</p> <p>Example</p> <p>Automatically add a <code>created_at</code> and <code>updated_at</code> column by inheriting from <code>TimestampMixin</code></p> SQLAlchemy mixed inheritance<pre><code>...\n\nclass ExampleModel(Base, TimestampMixin):\n    \"\"\"Class will have a created_at and modified_at timestamp applied automatically.\"\"\"\n    __tablename__ = \"example\"\n\n    id: Mapped[int] = mapped_column(sa.INTEGER, primary_key=True, autoincrement=True, unique=True)\n\n    ...\n</code></pre>","tags":["standard-project-files","python","sqlalchemy","database"]},{"location":"programming/standard-project-files/python/sqlalchemy/database_dir.html#initpy_2","title":"init.py","text":"database/mixins/__init__.py<pre><code>from __future__ import annotations\n\nfrom .classes import TableNameMixin, TimestampMixin\n</code></pre>","tags":["standard-project-files","python","sqlalchemy","database"]},{"location":"programming/standard-project-files/python/sqlalchemy/database_dir.html#classespy","title":"classes.py","text":"daatabase/mixins/classes.py<pre><code>from __future__ import annotations\n\nimport pendulum\nimport sqlalchemy as sa\nimport sqlalchemy.orm as so\n\nclass TimestampMixin:\n    \"\"\"Add a created_at &amp; updated_at column to records.\n\n    Add to class declaration to automatically create these columns on\n    records.\n\n    Usage:\n\n    ``` py linenums=1\n    class Record(Base, TimestampMixin):\n        __tablename__ = ...\n\n        ...\n    ```\n    \"\"\"\n\n    created_at: so.Mapped[pendulum.DateTime] = so.mapped_column(\n        sa.TIMESTAMP, server_default=sa.func.now()\n    )\n    updated_at: so.Mapped[pendulum.DateTime] = so.mapped_column(\n        sa.TIMESTAMP, server_default=sa.func.now(), onupdate=sa.func.now()\n    )\n\n\nclass TableNameMixin:\n    \"\"\"Mixing to automatically name tables based on class name.\n\n    Generates a `__tablename__` for classes inheriting from this mixin.\n    \"\"\"\n\n    @so.declared_attr.directive\n    def __tablename__(cls) -&gt; str:\n        return cls.__name__.lower() + \"s\"\n</code></pre>","tags":["standard-project-files","python","sqlalchemy","database"]},{"location":"programming/standard-project-files/python/sqlalchemy/database_dir.html#basepy","title":"base.py","text":"database/base.py<pre><code>from __future__ import annotations\n\nimport sqlalchemy as sa\nimport sqlalchemy.orm as so\n\nREGISTRY: so.registry = so.registry()\nMETADATA: sa.MetaData = sa.MetaData()\n\n\nclass Base(so.DeclarativeBase):\n    registry = REGISTRY\n    metadata = METADATA\n</code></pre>","tags":["standard-project-files","python","sqlalchemy","database"]},{"location":"programming/standard-project-files/python/sqlalchemy/database_dir.html#db_configpy","title":"db_config.py","text":"database/db_config.py<pre><code>from __future__ import annotations\n\nimport sqlalchemy as sa\nimport sqlalchemy.orm as so\n\nfrom dataclasses import dataclass, field\n\n\n@dataclass\nclass DBSettings:\n    drivername: str = field(default=\"sqlite+pysqlite\")\n    user: str | None = field(default=None)\n    password: str | None = field(default=None)\n    host: str | None = field(default=None)\n    port: str | None = field(default=None)\n    database: str = field(default=\"app.sqlite\")\n    echo: bool = field(default=False)\n\n    def __post_init__(self):\n        assert self.drivername is not None, ValueError(\"drivername cannot be None\")\n        assert isinstance(self.drivername, str), TypeError(\n            f\"drivername must be of type str. Got type: ({type(self.drivername)})\"\n        )\n        assert isinstance(self.echo, bool), TypeError(\n            f\"echo must be a bool. Got type: ({type(self.echo)})\"\n        )\n        if self.user:\n            assert isinstance(self.user, str), TypeError(\n                f\"user must be of type str. Got type: ({type(self.user)})\"\n            )\n        if self.password:\n            assert isinstance(self.password, str), TypeError(\n                f\"password must be of type str. Got type: ({type(self.password)})\"\n            )\n        if self.host:\n            assert isinstance(self.host, str), TypeError(\n                f\"host must be of type str. Got type: ({type(self.host)})\"\n            )\n        if self.port:\n            assert isinstance(self.port, int), TypeError(\n                f\"port must be of type int. Got type: ({type(self.port)})\"\n            )\n            assert self.port &gt; 0 and self.port &lt;= 65535, ValueError(\n                f\"port must be an integer between 1 and 65535\"\n            )\n\n    def get_db_uri(self) -&gt; sa.URL:\n        try:\n            _uri: sa.URL = sa.URL.create(\n                drivername=self.drivername,\n                username=self.user,\n                password=self.password,\n                host=self.host,\n                port=self.port,\n                database=self.database,\n            )\n\n            return _uri\n\n        except Exception as exc:\n            msg = Exception(\n                f\"Unhandled exception getting SQLAlchemy database URL. Details: {exc}\"\n            )\n            raise msg\n\n    def get_engine(self) -&gt; sa.Engine:\n        assert self.get_db_uri() is not None, ValueError(\"db_uri is not None\")\n        assert isinstance(self.get_db_uri(), sa.URL), TypeError(\n            f\"db_uri must be of type sqlalchemy.URL. Got type: ({type(self.db_uri)})\"\n        )\n\n        try:\n            engine: sa.Engine = sa.create_engine(\n                url=self.get_db_uri().render_as_string(hide_password=False),\n                echo=self.echo,\n            )\n\n            return engine\n        except Exception as exc:\n            msg = Exception(\n                f\"Unhandled exception getting database engine. Details: {exc}\"\n            )\n\n            raise msg\n\n    def get_session_pool(self) -&gt; so.sessionmaker[so.Session]:\n        engine: sa.Engine = self.get_engine()\n        assert engine is not None, ValueError(\"engine cannot be None\")\n        assert isinstance(engine, sa.Engine), TypeError(\n            f\"engine must be of type sqlalchemy.Engine. Got type: ({type(engine)})\"\n        )\n\n        session_pool: so.sessionmaker[so.Session] = so.sessionmaker(bind=engine)\n\n        return session_pool\n</code></pre>","tags":["standard-project-files","python","sqlalchemy","database"]},{"location":"programming/standard-project-files/python/sqlalchemy/database_dir.html#methodspy","title":"methods.py","text":"database/methods.py<pre><code>from __future__ import annotations\n\nimport sqlalchemy as sa\nimport sqlalchemy.orm as so\n\ndef get_db_uri(\n    drivername: str = \"sqlite+pysqlite\",\n    username: str | None = None,\n    password: str | None = None,\n    host: str | None = None,\n    port: int | None = None,\n    database: str = \"demo.sqlite\",\n) -&gt; sa.URL:\n    assert drivername is not None, ValueError(\"drivername cannot be None\")\n    assert isinstance(drivername, str), TypeError(\n        f\"drivername must be of type str. Got type: ({type(drivername)})\"\n    )\n    if username is not None:\n        assert isinstance(username, str), TypeError(\n            f\"username must be of type str. Got type: ({type(username)})\"\n        )\n    if password is not None:\n        assert isinstance(password, str), TypeError(\n            f\"password must be of type str. Got type: ({type(password)})\"\n        )\n    if host is not None:\n        assert isinstance(host, str), TypeError(\n            f\"host must be of type str. Got type: ({type(host)})\"\n        )\n    if port is not None:\n        assert isinstance(port, int), TypeError(\n            f\"port must be of type int. Got type: ({type(port)})\"\n        )\n    assert database is not None, ValueError(\"database cannot be None\")\n    assert isinstance(database, str), TypeError(\n        f\"database must be of type str. Got type: ({type(database)})\"\n    )\n\n    try:\n        db_uri: sa.URL = sa.URL.create(\n            drivername=drivername,\n            username=username,\n            password=password,\n            host=host,\n            port=port,\n            database=database,\n        )\n\n        return db_uri\n    except Exception as exc:\n        msg = Exception(\n            f\"Unhandled exception creating SQLAlchemy URL from inputs. Details: {exc}\"\n        )\n\n        raise msg\n\n\ndef get_engine(db_uri: sa.URL = None, echo: bool = False) -&gt; sa.Engine:\n    assert db_uri is not None, ValueError(\"db_uri is not None\")\n    assert isinstance(db_uri, sa.URL), TypeError(\n        f\"db_uri must be of type sqlalchemy.URL. Got type: ({type(db_uri)})\"\n    )\n\n    try:\n        engine: sa.Engine = sa.create_engine(url=db_uri, echo=echo)\n\n        return engine\n    except Exception as exc:\n        msg = Exception(f\"Unhandled exception getting database engine. Details: {exc}\")\n\n        raise msg\n\n\ndef get_session_pool(engine: sa.Engine = None) -&gt; so.sessionmaker[so.Session]:\n    assert engine is not None, ValueError(\"engine cannot be None\")\n    assert isinstance(engine, sa.Engine), TypeError(\n        f\"engine must be of type sqlalchemy.Engine. Got type: ({type(engine)})\"\n    )\n\n    session_pool: so.sessionmaker[so.Session] = so.sessionmaker(bind=engine)\n\n    return session_pool\n</code></pre>","tags":["standard-project-files","python","sqlalchemy","database"]},{"location":"snippets/index.html","title":"Code Snippets","text":"<p>A place where I dump useful code snippets/examples with little-to-no context.</p>","tags":["snippets"]},{"location":"snippets/Bash%20Snippets/index.html","title":"Bash Snippets","text":"","tags":["snippets","bash"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html","title":"One-liners","text":"<p>Some Bash commands can be written as a \"one-liner.\" <code>if</code> and <code>while</code> statements, for example, can be written in a slightly different way to chain commands.</p>","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#bash-variables","title":"Bash Variables","text":"","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#set-variable-to-path-where-script-was-called-from","title":"Set variable to path where script was called from","text":"<p>Say you have a path, <code>/home/username/scripts/system/update_system.sh</code>, and your current directory is <code>/home/username/scripts/</code>. If you call <code>./system/update_system.sh</code> from the <code>/home/username/scripts/</code> directory, the value of <code>$CWD</code> below would be <code>/home/username/scripts</code>:</p> Set $CWD to path where script was called from<pre><code>CWD=$(pwd)\n</code></pre>","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#set-variable-to-path-where-script-exists","title":"Set variable to path where script exists","text":"<p>Say you have a path, <code>/home/username/scripts/system/update_system.sh</code>, and your current directory is <code>/home/username/scripts/</code>. If you call <code>./system/update_system.sh</code> from the <code>/home/username/scripts/</code> directory, the value of <code>$THIS_DIR</code> below would be <code>/home/username/scripts/system/</code>.</p> Set $THIS_DIR to path where script exists.<pre><code>THIS_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" &amp;&amp; pwd)\"\n</code></pre>","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#get-formatted-timestamp","title":"Get formatted timestamp","text":"<p>Set a variable to a timestamp when the variable was initialized.</p> Set timestamp variable<pre><code>TS=$(date +\"%Y-%m-%d %H-%M-%S\")\n</code></pre> <p>Format the timestamp using <code>+%?</code>, where the <code>?</code> is one of the below:</p> Value Date Part # Digits <code>%Y</code> Year 4 (<code>YYYY</code>) <code>%m</code> Month 2 (<code>mm</code>) <code>%d</code> Day 2 (<code>dd</code>) <code>%H</code> Hour (24h) 2 (<code>HH</code>) <code>%I</code> Hour (12h) 2 (<code>HH</code>) <code>%M</code> Minutes 2 (<code>MM</code>) <code>%S</code> Seconds 2 (<code>SS</code>) <code>%p</code> AM/PM (12h only) 2 (<code>AM</code>/<code>PM</code>) <p>You can also create a function and call it in a string to add a timestamp, for example to name a file or directory.</p> get_timestamp function<pre><code>get_timestamp() {\n    echo \"$(date +\"%Y-%m-%d %H:%M:%S\")\"\n}\n\n## Capture output of timestamp in a variable\nTS=$(get_timestamp)\necho \"Timestamp: $TS\"\n\n## Use it in a string\n#  Warning: change format to +\"%Y-%m-%d %H-%M-%S\" to avoid \":\" characters in filenames\nDIRECTORY_PATH=\"$(get_timestamp)_pictures\"  ## YYYY-mm-dd HH-MM-SS_pictures\n</code></pre>","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#find-one-liners","title":"<code>find</code> one-liners","text":"<p>The <code>find</code> command on Unix machines searches for files/directories that match a pattern. You can chain commands on the results with <code>-exec &lt;logic&gt; {} +</code>, for example to remove all results of the <code>find</code> command.</p>","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#find-remove","title":"Find &amp; remove","text":"<p>You can add an <code>-exec</code> statement to a <code>find</code> command to do something with the results of <code>find</code>.</p>","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#find-remove-files","title":"Find &amp; remove files","text":"Find &amp; remove files<pre><code>find . -type f -name \"name-or-namepart\" -exec rm {} +\n</code></pre>","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#find-remove-dirs","title":"Find &amp; remove dirs","text":"Find &amp; remove dirs<pre><code>find . -type d -name \"name-or-namepart\" -exec rm -rf {} +\n</code></pre>","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#search-replace-in-a-file-name","title":"Search &amp; replace in a file name","text":"<p>Some characters are invalid for filenames, i.e. <code>:</code>, on certain OSes (looking at you, Windows). This command can search for symbols/patterns in a string and replace them. In the example below, we search for any file with a <code>:</code> anywhere in the name and replace it with a <code>-</code> symbol:</p> Search &amp; replace character(s) in string<pre><code>find /path/to/your/directory -type f -name '*:*' -exec bash -c 'mv \"$0\" \"${0//:/-}\"' {} \\;\n</code></pre>","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#exclusion-strings","title":"Exclusion strings","text":"<p>Use <code>! -exec sh -c 'ls \"$1\"/&lt;your-find-pattern&gt;/dev/null 2&gt;&amp;1' _ {} \\;</code>, replacing <code>&lt;your-find-pattern&gt;</code> with a search string, to write an exclusion list for the <code>find</code> command. This will return all results not matching a given pattern.</p>","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#print-all-directories-that-do-not-contain-a-specific-filename-pattern","title":"Print all directories that do not contain a specific filename pattern","text":"<p>Example: print all directories that do not have a file ending in <code>.part</code></p> <pre><code>find . -type d ! -exec sh -c 'ls \"$1\"/*.part &gt;/dev/null 2&gt;&amp;1' _ {} \\; -print\n</code></pre>","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#print-every-file-that-does-not-contain-a-specific-filename-pattern","title":"Print every file that does not contain a specific filename pattern","text":"<p>Example: Print every file in the current directory that does not end in <code>.part</code> <pre><code>find . -type f ! -name '*.part'\n</code></pre></p>","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#hostname-one-liners","title":"<code>hostname</code> one-liners","text":"","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#get-hosts-primary-ip-address","title":"Get host's primary IP address","text":"Get host primary IP<pre><code>hostname -I | cut -f1 -d' '\n</code></pre>","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#command-pipeing","title":"Command pipeing","text":"","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#echo-multiple-lines-into-file-with-eof","title":"Echo multiple lines into file with EOF","text":"Echo lines into file<pre><code>cat &lt;&lt;EOF &gt; /path/to/file.ext\nThis is a line that will be echoed into the file.\nThe file's path is /path/to/file.ext\n\nThe line above will also be written to the file.\nEOF\n</code></pre>","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#pass-input-response-to-command","title":"Pass input response to command","text":"<p>You can prepend a command with the answer to a prompt, like with <code>sudo ufw delete $ruleNum</code>, which prompts the user for a <code>yes</code>/<code>no</code> response to confirm deletion of the rule. This makes scripting <code>ufw</code> more of a challenge. By pipeing your response to the command, you can auto-answer prompts from utilities.</p> Prompt response pipe<pre><code>yes | sudo ufw delete \"$rule_num\"\n</code></pre>","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#user-group-commands","title":"User &amp; Group commands","text":"","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#check-if-linux-user-exists","title":"Check if Linux user exists","text":"Check if Linux user exists<pre><code>getent passwd \"username\"\n</code></pre>","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#check-if-linux-group-exists","title":"Check if Linux group exists","text":"Check if Linux group exists<pre><code>getent group &lt;group_name&gt; /dev/null\n</code></pre>","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#check-if-command-exists-runs","title":"Check if command exists &amp; runs","text":"Check if Bash command exists<pre><code>if ! command -v &lt;the_command&gt; &amp;&gt; /dev/null\nthen\n    echo \"&lt;the_command&gt; could not be found\"\n    exit 1\nfi\n</code></pre> <p>Example with <code>docker</code> command:</p> Check if Docker command exists<pre><code>if ! command -v docker &amp;&gt; /dev/null\nthen\n    echo \"docker could not be found\"\n    exit 1\nfi\n</code></pre>","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#stat-one-liners","title":"<code>stat</code> one-liners","text":"","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#get-chmod-of-a-file-or-directory","title":"Get chmod of a file or directory","text":"Get chmod<pre><code>stat -c %a $PATH\n</code></pre> <p>You can add an alias to your <code>~/.bash_aliases</code> file to call the <code>stat</code> command with variable directory paths:</p> ~/.bash_aliases<pre><code>alias getchmod=stat -c %a\n</code></pre>","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#misc-one-liners","title":"Misc. one-liners","text":"","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#get-a-timestamp","title":"Get a timestamp","text":"<p>You can call the <code>date</code> command with a string format, i.e. <code>+\"%Y-%m-%d_%H:%M\"</code>, to get a formatted datetime string.</p> Get timestamp and assign to a variable<pre><code>timestamp=$(date +\"%Y-%m-%d_%H:%M\")\n\n## Usage\n#  filename=\"${timestamp}_filename.ext\"\n</code></pre> Timestamp function for Bash scripts<pre><code>timestamp() { date +\"%Y-%m-%d_%H:%M\"; }\n\n## Usage\n#  current_time=$(timestamp)\n</code></pre>","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#repeat-a-command-with-a-sleep","title":"Repeat a command with a sleep","text":"<p>You can write a <code>while</code> loop as a one-liner:</p> Repeat a command with a sleep<pre><code>while true; do &lt;your command&gt;; sleep &lt;sleep seconds&gt;; done\n</code></pre> <p>Example: repeat the <code>ls</code> command every 5 seconds:</p> Run ls command every 5 seconds<pre><code>while true; do ls; sleep 5; done\n</code></pre>","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#rsync-one-liners","title":"rsync one-liners","text":"","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#sync-path-with-rsync","title":"Sync path with rsync","text":"<p><code>rsync</code> is an incredible useful tool for synchronizing files. It can be used to sync local-to-local, remote-to-remote, or local-to-remote/remote-to-local.</p> <p><code>rsync</code> flags reference:</p> arg description <code>-r</code> Recursive copy (unnecessary with <code>-a</code>) <code>-a</code> Archive mode, includes recursive transfer <code>-z</code> Compress the data <code>-v</code> Verbose/detailed info during transfer <code>-h</code> Human readable output <code>--progress</code> Show a progress bar during transfer","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#sync-local-file-to-remote","title":"Sync local file to remote","text":"rsync local-to-remote<pre><code>rsync -avzh --progress /local/path/ user@remote:/remote/path/\n</code></pre>","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#sync-remote-file-to-local","title":"Sync remote file to local","text":"rsync remote-to-local<pre><code>rsync -avzh --progress user@remote:/remote/path/ /local/path/\n</code></pre>","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#swap-memory","title":"Swap Memory","text":"","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/one-liners/index.html#clear-swap","title":"Clear swap","text":"Clear swap memory on Linux<pre><code>sudo swapoff -a ; sudo swapon -a\n</code></pre>","tags":["snippets","bash","one-liners"]},{"location":"snippets/Bash%20Snippets/scripts/index.html","title":"Scripts","text":"<p>Re-usable Bash scripts. Copy/paste the contents of a script below into a <code>.sh</code> file and run <code>chmod +x &lt;name-of-file&gt;.sh</code> to make it executable.</p>","tags":["snippets","bash","scripts"]},{"location":"snippets/Bash%20Snippets/scripts/index.html#prune-git-branches","title":"Prune git branches","text":"<p>After deleting a git branch from a remote, the local copy will still exist. Delete/\"prune\" all local git branches that do not exist on the remote:</p> prune-git-branches.sh<pre><code>#!/bin/bash\n\ngit fetch -p &amp;&amp; for branch in $(git for-each-ref --format '%(refname) %(upstream:track)' refs/heads | awk '$2 == \"[gone]\" {sub(\"refs/heads/\", \"\", $1); print $1}'); do git branch -D $branch; done\n\nexit $?\n</code></pre>","tags":["snippets","bash","scripts"]},{"location":"snippets/Bash%20Snippets/scripts/parse.html","title":"Parse CLI args","text":"<p>To parse CLI args (i.e. <code>-p/--path /some/path</code> or <code>-n/--name myname</code>), you can write a <code>while</code> loop that iterates over your args. You use <code>$#</code> to get the number of CLI args, and if there are any, a <code>case</code> statement to do things with the inputs.</p>","tags":["snippets","bash","scripts"]},{"location":"snippets/Bash%20Snippets/scripts/parse.html#parse-input-values-into-script-variables","title":"Parse input values into script variables","text":"Parse CLI args<pre><code>#!/bin/bash\n\n## Set defaults\nNAME=\"world\"\n\n## Parse args\nwhile [[ $# -gt 0 ]]; do # ensure there are or more positional args to parse\n  ## Case statement to arse that --arg and a value if one was passed\n  case \"$1\" in\n    -n|--name) # catch -n or --name\n      ## Ensure a name value was passed with the arg\n      if [[ -z $2 ]]; then\n        ## User passed -n/--name without a name value\n        echo \"[ERROR] --name provided but no name given.\"\n        exit 1\n      fi\n\n      ## Set value of NAME var to provided name\n      NAME=\"$2\"\n\n      ## Shift 2, because there are 2 positional args (-n/--name and the name value given)\n      shift 2\n      ;;\n    *) # catch any misspelled or invalid args\n      echo \"[ERROR] Invalid flag: $1\"\n\n      echo \"You could print the CLI usage here as a hint to the user.\"\n      exit 1\n      ;;\n  esac\ndone\n\necho \"Hello, $NAME\"\n</code></pre>","tags":["snippets","bash","scripts"]},{"location":"snippets/Bash%20Snippets/scripts/parse.html#pass-a-flag-multiple-times","title":"Pass a flag multiple times","text":"<p>You can store arg inputs in an array, so that you can pass the arg multiple times.</p> Parse flag multiple times<pre><code>#!/bin/bash\n\n## Set defaults\ndeclare -a ADD=()\n\n## Parse args\nwhile [[ $# -gt 0 ]]; do\n  case \"$1\" in\n    -a|--add) # accept -a/--add multiple times\n      if [[ -n \"$2\" &amp;&amp; \"$2\" != --* ]] # ensure a value was passed, and was not another --arg\n        ADD+=(\"$2\")\n        shift 2\n      else\n        echo \"[ERROR] --add provided but no integer given.\"\n        exit 1\n      fi\n      ;;\n    *)\n      echo \"[ERROR] Invalid argument: $1\"\n      exit 1\n      ;;\n  esac\ndone\n\n## Sum --add values\nsum=0\nfor val in \"${ADD[@]}\"; do\n  ((sum += val))\ndone\n\necho \"Sum: $sum\"\n</code></pre>","tags":["snippets","bash","scripts"]},{"location":"snippets/Bash%20Snippets/scripts/parse.html#switchboolean-args","title":"Switch/boolean args","text":"<p>To uses an arg as a switch, you can detect its presence and set a static action, like settings a <code>DRY_RUN</code> variable.</p> Dry run switch example<pre><code>#!/bin/bash\n\nDRY_RUN=\"\"\n\nwhile [[ $# -gt 0 ]]; do\n  case \"$1\" in\n    --dry-run)\n      ## Set the $DRY_RUN value to 'true'\n      DRY_RUN=\"true\"\n\n      ## \"shift\" to the next arg\n      shift\n      ;;\n  esac\ndone\n\nif [[ \"$DRY_RUN\" == \"\" ]] || [[ \"$DRY_RUN\" == \"false\" ]]; then\n  echo \"Dry run is not enabled.\"\nelse\n  echo \"Dry run is enabled.\"\nfi\n</code></pre>","tags":["snippets","bash","scripts"]},{"location":"snippets/Bash%20Snippets/scripts/parse.html#examples","title":"Examples","text":"","tags":["snippets","bash","scripts"]},{"location":"snippets/Bash%20Snippets/scripts/parse.html#generic-cli-arg-parse-script","title":"Generic CLI arg parse script","text":"Bash parse CLI args<pre><code>#!/bin/bash\n\n## Set default values before parsing\n\n# When --dry-run is passed, this will be set to 'true'\nDRY_RUN=\"\"\n# User can pass a -n/--name to override this\nNAME=\"world\"\n## An array the user can append to with multiple -l/--ls-dir\ndeclare -a LS_DIRS=()\n\n## Function to print CLI usage on error or -h/--help\nfunction print_help() {\n  cat &lt;&lt;EOF\nUsage: $0 [options]\n\nOptions:\n  -n, --name NAME       Specify a name (default: world)\n  -l, --ls-dir PATH     Add path to list (can be repeated; default: current dir)\n      --dry-run         Run without making changes\n  -h, --help            Show this help message\nEOF\n}\n\n## Parse CLI args\nwhile [[ $# -gt 0 ]]; do\n  case \"$1\" in\n    -n|--name)\n      ## Evaluate the 2nd positional arg, which sould be a name string\n      if [[ -z $2 ]]; then\n        ## User passed -n/--name with no name value\n        echo \"[ERROR] --name provided but no name string given.\"\n        exit 1\n      fi\n\n      NAME=\"$2\"\n\n      ## Shift 2, because of the 2 position args (-n/--name and the name provided)\n      shift 2\n      ;;\n\n    -l|--ls-dir)\n      ## This arg can be passed multiple times. Loop over all iterations,\n      #  evaluate them, and append them to the array\n      if [[ -n \"$2\" &amp;&amp; \"$2\" != --* ]]; then\n        ## A path was found, append it to LS_DIRS\n        LS_DIRS+=(\"$2\")\n        shift 2\n      else\n        echo \"[ERROR] --ls-dir provided but no path given.\"\n\n        print_help\n        exit 1\n      fi\n      ;;\n\n    --dry-run)\n      ## Set the $DRY_RUN value to 'true'\n      DRY_RUN=\"true\"\n\n      ## \"shift\" to the next arg\n      shift\n      ;;\n\n    ## Catch -h/--help and exit early\n    -h|--help)\n      print_help\n      exit 0\n      ;;\n\n    ## Catch any misspellings or invalid args\n    *)\n      echo \"[ERROR] Invalid argument: $1\"\n\n      ## Print the script help menu\n      print_help\n      exit 1\n      ;;\n  esac\ndone\n\nfunction say_hi() {\n    local name\n\n    name=\"$1\"\n\n    echo \"Hello, $name!\"\n}\n\nfunction loop_dirs() {\n  local dirs=(\"$@\")\n\n  if [[ ${#dirs[@]} -eq 0 ]]; then\n    echo \"[WARNING] No --ls-dir paths given.\"\n    return 0\n  fi\n\n  for d in \"${dirs[@]}\"; do\n\n    ## Ensure path exists\n    if [[ ! -d \"$d\" ]]; then\n      echo \"[WARNING] Could not find path: $d\"\n      continue\n    fi\n\n    echo \"\"\n    echo \"Listing files in path: $d\"\n    echo \"\"\n\n    ## List contents\n    ls -la \"$d\"\n  done\n}\n\nsay_hi \"$NAME\"\n\nif [[ \"$DRY_RUN\" == true ]]; then\n  echo \"[DRY RUN] Would list directories: ${LS_DIRS[*]}\"\nelse\n  loop_dirs \"${LS_DIRS[@]}\"\nfi\n</code></pre>","tags":["snippets","bash","scripts"]},{"location":"snippets/Powershell%20Snippets/index.html","title":"Powershell Snippets","text":"<p>Code snippets &amp; example script parts for Powershell.</p> <p>Check the <code>one-liners</code> page for Powershell one-liners, meant to be copy-pasted into your terminal, or check the example scripts page for files you can save as <code>.ps1</code> scripts. Note that most of the examples in the scripts page are partials, things you copy into a script you're writing and build with. Most are not complete scripts that do something when you run them.</p>","tags":["snippets","powershell"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html","title":"One-liners","text":"<p>Some Powershell commands can be written as a \"one-liner.\"</p>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#start-remote-session-pipe-commands","title":"Start remote session &amp; pipe commands","text":"<p>Create a session &amp; assign it to a variable <code>$s</code>. Use that variable and <code>Invoke-Command</code> to run commands on the remote.</p> Start remote session, pipe commands<pre><code>## Create a session\n$s = New-PSSession &lt;computer-name&gt;\n\n## Run command(s) on the remove\nInvoke-Command -Session $s { &lt;command(s) to run&gt; }\n\n## Close the remote session\nRemove-PSSession $s\n</code></pre>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#copy-file-from-remote-to-local","title":"Copy file from remote to local","text":"Copy file from remote to local<pre><code># Create a session\n$Session = New-PSSession -ComputerName \"Server01\" -Credential \"Contoso\\User01\"\n\n# Copy item from remote $session to local -Destination)\nCopy-Item \"C:\\MyRemoteData\\test.log\" -Destination \"D:\\MyLocalData\\\" -FromSession $Session\n</code></pre>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#pipetee-powershell-command-output-to-a-file","title":"Pipe/tee Powershell command output to a file","text":"Tee command output to file<pre><code>&lt;your powershell command&gt; | Tee-Object -FilePath &lt;output-filename&gt;.log\n</code></pre>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#get-machine-uptime","title":"Get machine uptime","text":"<p>On Linux, where everything is better and easier, you just run <code>uptime</code> to get a machine's uptime. On Windows, you have to do extra stuff because... Powershell...</p> Get machine uptime<pre><code>(Get-Date) \u2013 (Get-CimInstance Win32_OperatingSystem).LastBootUpTime\n</code></pre>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#generate-a-battery-report","title":"Generate a battery report","text":"<p>On laptops or devices with portable power, you can generate a battery report with the following command (find the report at the path you put after <code>/output</code>):</p> Generate battery report<pre><code>powercfg /batteryreport /output \"C:\\battery-report.html\"\n</code></pre>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#count-files-in-a-directory","title":"Count files in a directory","text":"Count number of files in a directory<pre><code>$FileCount = (Get-Childitem -Path \"C:\\path\\to\\parent\" -File | Measure-Object).Count\n</code></pre>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#recursively-remove-all-files-in-a-path","title":"Recursively remove all files in a path","text":"Recursively delete all files<pre><code>Remove-Item C:\\path\\to\\parent\\* -Recurse -Force\n</code></pre>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#export-event-viewer-log-history","title":"Export Event Viewer log history","text":"<p>Use the command below to export all Event Viewer events from a specific logging section (<code>Application</code>, <code>Security</code>, <code>Setup</code>, or <code>System</code>):</p> Export Event Viewer logs<pre><code>Get-EventLog -LogName &lt;Application|Security|Setup|System&gt; | Export-Csv -Path C:\\path\\to\\events_file.csv\n</code></pre>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#enabledisable-windows-defender-real-time-protection","title":"Enable/disable Windows Defender Real-Time Protection","text":"","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#enable-real-time-protection","title":"Enable real-time protection","text":"Enable real-time protection<pre><code>PowerShell Set-MpPreference -DisableRealtimeMonitoring 0\u200b\n</code></pre> Enable real-time protection using boolean<pre><code>PowerShell Set-MpPreference -DisableRealtimeMonitoring $false\u200b\n</code></pre>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#disable-real-time-protection","title":"Disable real-time protection","text":"Disable real-time protection<pre><code>PowerShell Set-MpPreference -DisableRealtimeMonitoring 1\u200b\n</code></pre> Disable real-time protection using boolean<pre><code>PowerShell Set-MpPreference -DisableRealtimeMonitoring $true\n</code></pre>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#export-list-of-ad-users-in-a-group","title":"Export list of AD users in a group","text":"<p>Substitute an AD Group name for <code>\"$ADGroup\"</code> and a path to export the CSV file to for <code>\"$EXPORT_PATH\"</code> (example: <code>c:\\tmp\\adgroup_members.csv</code>):</p> Get members of AD group<pre><code>Get-ADGroupMember -Identity \"$ADGroup\" | Export-CSV -Path $EXPORT_PATH -NoTypeInformation\n</code></pre>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#export-users-members-of-to-csv","title":"Export user's 'Members Of' to CSV","text":"<pre><code>Get-ADPrincipalGroupMembership USERNAME | Select Name | Export-CSV -path C:\\Temp\\file.csv -NoTypeInformation\n</code></pre>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#query-ad-user-by-email-address-get-enabled-status","title":"Query AD user by email address, get \"Enabled\" status","text":"Get user's 'Enabled' status from email address<pre><code>Get-ADUser -Filter \"EmailAddress -eq 'address@email.com'\" -Properties EmailAddress | Select-Object Enabled\n</code></pre>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#get-ad-users-properties","title":"Get AD user's properties","text":"Get AD user's properties<pre><code>Get-ADUser -Identity &lt;username&gt; -Properties *\n</code></pre>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#get-subset-of-ad-users-properties","title":"Get subset of AD user's properties","text":"Query specific properties of AD user's profile<pre><code>Get-ADUser -Identity &lt;username&gt; -Properties Name, AccountLockoutTime, LastBadPasswordAttempt, LastLogonDate, LockedOut, lockoutTime, Modified, modifyTimeStamp, PasswordExpired, PasswordLastSet\n</code></pre>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#unlock-ad-users-account","title":"Unlock AD user's account","text":"Unlock AD user account<pre><code>Unlock-ADAccount -Identity $ADUsername\n</code></pre>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#exportimport-winget-packages","title":"Export/Import winget packages","text":"<p>You can export your installed packages using the <code>winget</code> utility. The backup format is <code>.json</code>.</p>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#export-winget-packages","title":"Export winget packages","text":"Export winget packages<pre><code>winget export -o C:\\path\\to\\winget-pkgs.json\n</code></pre>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#import-winget-packages","title":"Import winget packages","text":"Import winget packages<pre><code>winget import -i C:\\path\\to\\winget-pkgs.json\n</code></pre>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#format-string-parts-with-nonewline","title":"Format string parts with -NoNewline;","text":"<p>Using the <code>-NoNewline;</code> param, you can format different parts of a <code>Write-Host</code> string and break long lines into multiple.</p> <p>For example to set the left part of a string to green and the right to red:</p> Format string colors<pre><code>Write-Host \"I am green, \" -ForegroundColor Green -NoNewline; Write-Host \"and I am red!\" -ForegroundColor Red\n</code></pre> <p>To apply formatting to some parts of a long string, and to break it up over multiple lines, you can use a new line after the <code>;</code> in <code>-NoNewline;</code>:</p> Multi-line Write-Host with -NoNewline<pre><code>Write-Host \"This is the first part of a long string, with no formatting.\" -NoNewline;\nWrite-Host \"This part of the string will appear inline (on the same line) as the previous string,\" -NoNewline;\nWrite-Host \"and can even be broken up mid-sentence! Check the source code to see this in action.\" -NoNewline;\nWrite-Host \"\" -NoNewline;\nWrite-Host \"And I'm purple, just because\" -ForegroundColor purple -NoNewline;\nWrite-Host \"Ok that's all.\"\n</code></pre>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#setunset-environment-variables","title":"Set/Unset environment variables","text":"<p>Warning</p> <p>You must be in an elevated/administrative prompt for these commands.</p>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#set-environment-variable","title":"Set environment variable","text":"Set Machine (system-wide) variable<pre><code>[System.Environment]::SetEnvironmentVariable(\"VARIABLE_NAME\", \"VALUE\", [System.EnvironmentVariableTarget]::Machine)\n</code></pre> <p>You can also use it as a function:</p> Set-EnvVar function<pre><code>function Set-EnvVar {\n    &lt;#\n        Set an environment variable. If -Target Machine or -Target User, the env variable will persist between sessions.\n\n        Usage:\n            Set-EnvVar -Name &lt;name&gt; -Value &lt;value&gt;\n            Set-EnvVar -Name &lt;name&gt; -Value &lt;value&gt; -Target Machine\n\n        Params:\n            Name: The name of the environment variable\n            Value: The value of the environment variable\n            Target: The scope of the environment variable. Machine, User, or Process\n\n        Example:\n            Set-EnvVar -Name \"EXAMPLE_VAR\" -Value \"example value\"\n            Write-Host $env:EXAMPLE_VAR\n    #&gt;\n    param (\n        [string]$Name,\n        [string]$Value,\n        [ValidateSet('Machine', 'User', 'Process')]\n        [string]$Target = 'User'\n    )\n\n    Write-Host \"Setting [$Target] environment variable \"$Name\".\"\n\n    If ( $Target -eq 'Process' ) {\n        Write-Warning \"Environment variable [$Target] will not persist between sessions.\"\n    } else {\n        Write-Information \"Environment variable [$Target] will persist between sessions.\"\n    }\n\n    try{\n        [System.Environment]::SetEnvironmentVariable($Name, $Value, [System.EnvironmentVariableTarget]::$Target)\n    } catch {\n        Write-Error \"Unhandled exception setting environment variable. Details: $($_.Exception.Message)\"\n    }\n}\n</code></pre>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#unset-environment-variable","title":"Unset environment variable","text":"Set User env variable<pre><code>[System.Environment]::SetEnvironmentVariable(\"VARIABLE_NAME\", \"VALUE\", [System.EnvironmentVariableTarget]::User)\n</code></pre> <p>You can use it as a function:</p> Remove-EnvVar function<pre><code>function Remove-EnvVar {\n    &lt;#\n        Remove/unset an environment variable.\n\n        Usage:\n            Remove-EnvVar -Name &lt;name&gt;\n            Remove-EnvVar -Name &lt;name&gt; -Target Machine\n\n        Params:\n            Name: The name of the environment variable\n            Target: The scope of the environment variable. Machine, User, or Process\n\n        Example:\n            Remove-EnvVar -Name \"EXAMPLE_VAR\"\n            Write-Host $env:EXAMPLE_VAR\n    #&gt;\n    param (\n        [string]$Name,\n        [ValidateSet('Machine', 'User', 'Process')]\n        [string]$Target = 'User'\n    )\n\n    try {\n        [System.Environment]::SetEnvironmentVariable($Name, $null, [System.EnvironmentVariableTarget]::$Target)\n    } catch {\n        Write-Error \"Unhandled exception removing environment variable. Details: $($_.Exception.Message)\"\n    }\n}\n</code></pre>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#http-requests","title":"HTTP requests","text":"","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#check-site-availability","title":"Check site availability","text":"<p>As a one-liner:</p> Check HTTP site availability<pre><code>$Site = \"https://www.google.com\"\n\nwhile ($true) {\n    try {\n        ## Make HTTP HEAD request\n        $response = Invoke-WebRequest -Uri \"$($Site)\" -Method Head\n\n        ## Output HTTP status code\n        Write-Output \"$(Get-Date) Ping site '$($Site)': [$($response.StatusCode): $($response.StatusDescription)]\"\n    } catch {\n        Write-Error \"$(Get-Date): Request failed. Error: $($_.Exception.Message)\"\n    }\n\n    ## Pause for $RequestSleep seconds\n    Start-Sleep -Seconds 5\n}\n</code></pre> <p>As a function:</p> Get-HTTPSiteAvailable<pre><code>function Get-HTTPSiteAvailable {\n    Param(\n        [string]$Site = \"https://www.google.com\",\n        [string]$RequestSleep = 5\n    )\n    while ($true) {\n        try {\n            ## Make HTTP HEAD request\n            $response = Invoke-WebRequest -Uri \"$($Site)\" -Method Head\n\n            ## Output HTTP status code\n            Write-Output \"$(Get-Date) Ping site '$($Site)': [$($response.StatusCode): $($response.StatusDescription)]\"\n        } catch {\n            Write-Error \"$(Get-Date): Request failed. Error: $($_.Exception.Message)\"\n        }\n\n        ## Pause for $RequestSleep seconds\n        Start-Sleep -Seconds $RequestSleep\n    }\n}\n</code></pre>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#open-a-list-of-urls","title":"Open a list of URLs","text":"<p>This script iterates over an array of URL strings and opens them in your default browser.</p> mass_open_links.ps1<pre><code>## Declare an array of URL strings\n$Links = @(\n    \"https://example.com\",\n    \"https://example.com/example\",\n    \"https://example.com/test\"\n)\n\n## Iterate over URLs and open them\n$Links | ForEach-Object {\n    Write-Output \"Opening URL: $_\"\n    Start-Process \"$($_)\"\n}\n</code></pre>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#disable-microsoft-copilot","title":"Disable Microsoft Copilot","text":"Disable Copilot &amp; prevent re-install<pre><code>Get-AppxProvisionedPackage -Online | where-object {$_.PackageName -like \"*Copilot*\"} | Remove-AppxProvisionedPackage -online\n</code></pre>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#generate-guids-unique-ids","title":"Generate GUIDs (unique IDs)","text":"Generate unique GUID<pre><code>[guid]::NewGuid()\n</code></pre> <p>You can also assign the GUID to a variable for re-use:</p> Generate unique GUID and assign to variable<pre><code>$UniqueID = [guid]::NewGuid()\n</code></pre>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/one-liners/index.html#turn-monitor-display-off","title":"Turn monitor display off","text":"Turn display off<pre><code>C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe -command \"(Add-Type -MemberDefinition '[DllImport(\\\"user32.dll\\\")] public static extern int PostMessage(int a, int b, int c, int d);' -Name f -PassThru)::PostMessage(-1, 0x112, 0xF170, 2)\"\n</code></pre>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/scripts/index.html","title":"Scripts","text":"<p>Some of the scripts on this page are full scripts to accomplish a task, others are snippets you can copy/paste into scripts you're building.</p>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/scripts/index.html#self-elevate-a-powershell-script","title":"Self-elevate a Powershell script","text":"<p>The following code will re-launch a Powershell session as an Administrator, using the same Powershell version as the non-elevated session that called it.</p> Self-elevate Powershell script<pre><code>## Determine which shell is running (PowerShell 7+ or Windows PowerShell)\n$shellPath = if ($PSVersionTable.PSEdition -eq 'Core') {\n    ## PowerShell 7+, use current process path (e.g., pwsh.exe)\n    (Get-Process -Id $PID).Path\n}\nelse {\n    ## Windows PowerShell 5.1\n    \"$env:WINDIR\\System32\\WindowsPowerShell\\v1.0\\powershell.exe\"\n}\n\n## Relaunch with elevation if not already running as admin\nif (-not ([Security.Principal.WindowsPrincipal] [Security.Principal.WindowsIdentity]::GetCurrent()).IsInRole(\n        [Security.Principal.WindowsBuiltInRole]::Administrator)) {\n\n    Write-Warning \"Relaunching as Administrator\"\n    Start-Process -FilePath $shellPath `\n        -ArgumentList \"-NoProfile\", \"-ExecutionPolicy Bypass\", \"-File `\"$PSCommandPath`\"\" `\n        -Verb RunAs\n    exit\n}\n\n## Rest of your code here\n</code></pre> <p>Alternatively, you can add <code>#Requires -RunAsAdministrator</code> to the top of your script (before any <code>&lt;# documentation #&gt;</code> or <code>[CmdletBinding()]</code>/<code>Param()</code>). Check the Microsoft <code>Requires</code> documentation for more of these special comment lines.</p> Requires Administrator comment<pre><code>#Requires -RunAsAdministrator\n\n## Rest of your code here\n</code></pre>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Powershell%20Snippets/scripts/index.html#enable-powershell-debugging","title":"Enable Powershell debugging","text":"<p>For each function you declare in your script where you have <code>Write-Debug</code> messages, add a <code>[CmdletBinding()]</code> before your <code>Param()</code> section:</p> <pre><code>function Get-Something {\n    [CmdletBinding()]\n    Param()\n}\n</code></pre> <p>Then, call the script with <code>-Debug</code>. This works for <code>-Verbose</code> and <code>Write-Verbose</code>, too.</p>","tags":["snippets","powershell","one-liners"]},{"location":"snippets/Python%20Snippets/index.html","title":"Python Snippets","text":"","tags":["snippets","python"]},{"location":"snippets/Python%20Snippets/controller_class_snippets/index.html","title":"Python Controller Classes","text":"<p>A Python controller class is a class that wraps all of the input parameters &amp; functionality for a specific domain. The controller classes on this page help by joining all functionality &amp; configurations for an operation into a class object with methods you call using <code>.dot_notation()</code>.</p> <p>Use the sections to the left to browse controller class snippets.</p>","tags":["python","snippets","controllers"]},{"location":"snippets/Python%20Snippets/controller_class_snippets/minio.html","title":"Minio","text":"<p>Class controller for interacting with minio S3 storage.</p> <p>This script is a CLI, run it with <code>--help</code> to see usage instructions. The controller can load Minio configuration from CLI args, or from a JSON configuration file. The script accepts <code>-g</code>/<code>--generate-configs</code> as an arg, which will create dummy versions of a <code>minio_config.json</code> and various \"job\" files that you can edit for your needs.</p> <p>The script can be dropped into an existing app so you can import pieces of it into other scripts/packages, or it can be used as a standalone CLI. The <code># /// script</code> portion at the top of the file informs <code>uv</code> and other package managers on which dependencies the script requires. This is called inline script metadata, and is a built-in feature of the Python language. This was introduced to the language in PEP 723.</p> <p><code>uv</code> will automatically detect the inline metadata when you run this script with <code>uv run script_name.py</code>, and will install the packages declared at the top of the file.</p> MinioController.py<pre><code># /// script\n# requires-python = \"&gt;=3.12\"\n# dependencies = [\n#     \"minio\",\n# ]\n# ///\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass, field\nimport glob\nimport logging\nimport os\nfrom pathlib import Path\nimport typing as t\n\nfrom minio import Minio\nfrom minio.commonconfig import CopySource\nfrom minio.deleteobjects import DeleteObject\nfrom minio.error import S3Error\n\nlog = logging.getLogger(__name__)\n\n__all__ = [\n    \"MinioController\",\n    \"get_minio_controller\",\n    \"MinioSettings\",\n    \"MinioJob\",\n    \"MinioUploadJob\",\n    \"MinioDownloadJob\",\n    \"MinioDeleteJob\",\n    \"MinioCopyJob\",\n    \"MinioMoveJob\",\n]\n\n\n@dataclass\nclass MinioSettings:\n    endpoint: str\n    access_key: str\n    secret_key: str = field(default=None, repr=False)\n    secure: bool = field(default=True)\n    cert_check: bool = field(default=True)\n\n\n@dataclass\nclass MinioJob:\n    \"\"\"Base class for all Minio jobs.\"\"\"\n\n    type: str\n    bucket: str\n\n\n@dataclass\nclass MinioCopyMoveJobBase(MinioJob):\n    remote_path: str\n    remote_dest: str\n    dest_bucket: str\n\n\n@dataclass\nclass MinioUploadJob(MinioJob):\n    remote_path: str\n    local_path: str\n\n\n@dataclass\nclass MinioDownloadJob(MinioJob):\n    remote_path: str\n    local_path: str\n\n\n@dataclass\nclass MinioDeleteJob(MinioJob):\n    remote_path: str\n\n\n@dataclass\nclass MinioCopyJob(MinioCopyMoveJobBase):\n    pass\n\n\n@dataclass\nclass MinioMoveJob(MinioCopyMoveJobBase):\n    pass\n\n\nclass MinioConfigGenerator:\n    \"\"\"Generates a default MinIO configuration JSON file.\"\"\"\n\n    def __init__(self, output_path: str = \"minio_conf.json\"):\n        self.output_path = output_path\n        self.default_config = {\n            \"endpoint\": \"&lt;ip:9000 or FQDN&gt;\",\n            \"access_key\": \"&lt;minio access key&gt;\",\n            \"secret_key\": \"&lt;minio secret key&gt;\",\n            \"secure\": True,\n            \"cert_check\": False,\n        }\n\n        self.logger = log.getChild(\"MinioConfigGenerator\")\n\n    def generate(self):\n        \"\"\"Generates the MinIO config JSON file if it doesn't exist.\"\"\"\n        if not Path(self.output_path).exists() or not Path(self.output_path).is_file():\n            with open(self.output_path, \"w\") as f:\n                json.dump(self.default_config, f, indent=4)\n            self.logger.info(f\"Generated default MinIO config at {self.output_path}\")\n        else:\n            self.logger.warning(f\"MinIO config already exists at {self.output_path}\")\n\n\nclass MinioJobConfigGenerator:\n    \"\"\"Base class for generating default MinIO job configuration JSON files.\"\"\"\n\n    def __init__(self, job_type: str, output_path: str = \"job.TYPE.json\"):\n        self.job_type = job_type\n        self.output_path = output_path.replace(\"TYPE\", self.job_type)\n\n        self.logger = log.getChild(\"MinioJobConfigGenerator\")\n\n    def generate(self):\n        \"\"\"Generates the job config JSON file if it doesn't exist.\"\"\"\n        if not Path(self.output_path).exists() or not Path(self.output_path).is_file():\n            with open(self.output_path, \"w\") as f:\n                json.dump(self.default_config, f, indent=4)\n            self.logger.info(\n                f\"Generated default {self.job_type} job config at {self.output_path}\"\n            )\n        else:\n            self.logger.warning(\n                f\"{self.job_type} job config already exists at {self.output_path}\"\n            )\n\n\nclass MinioUploadJobGenerator(MinioJobConfigGenerator):\n    \"\"\"Generates a default MinIO upload job configuration JSON file.\"\"\"\n\n    def __init__(self, output_path: str = \"job.TYPE.json\"):\n        super().__init__(\"upload\", output_path)\n\n        self.default_config = {\n            \"type\": \"upload\",\n            \"bucket\": \"&lt;minio bucket&gt;\",\n            \"remote_path\": \"&lt;minio path&gt;\",\n            \"local_path\": \"&lt;local path&gt;\",\n        }\n\n\nclass MinioDownloadJobGenerator(MinioJobConfigGenerator):\n    \"\"\"Generates a default MinIO download job configuration JSON file.\"\"\"\n\n    def __init__(self, output_path: str = \"job.TYPE.json\"):\n        super().__init__(\"download\", output_path)\n\n        self.default_config = {\n            \"type\": \"download\",\n            \"bucket\": \"&lt;minio bucket&gt;\",\n            \"remote_path\": \"&lt;minio path&gt;\",\n            \"local_path\": \"&lt;local path&gt;\",\n        }\n\n\nclass MinioDeleteJobGenerator(MinioJobConfigGenerator):\n    \"\"\"Generates a default MinIO delete job configuration JSON file.\"\"\"\n\n    def __init__(self, output_path: str = \"job.TYPE.json\"):\n        super().__init__(\"delete\", output_path)\n\n        self.default_config = {\n            \"type\": \"delete\",\n            \"bucket\": \"&lt;minio bucket&gt;\",\n            \"remote_path\": \"&lt;minio path&gt;\",\n        }\n\n\nclass MinioCopyJobGenerator(MinioJobConfigGenerator):\n    \"\"\"Generates a default MinIO copy job configuration JSON file.\"\"\"\n\n    def __init__(self, output_path: str = \"job.TYPE.json\"):\n        super().__init__(\"copy\", output_path)\n\n        self.default_config = {\n            \"type\": \"copy\",\n            \"bucket\": \"&lt;minio bucket&gt;\",\n            \"remote_path\": \"&lt;minio path&gt;\",\n            \"remote_dest\": \"&lt;minio dest path&gt;\",\n            \"dest_bucket\": \"&lt;minio dest bucket&gt;\",\n        }\n\n\nclass MinioMoveJobGenerator(MinioJobConfigGenerator):\n    \"\"\"Generates a default MinIO move job configuration JSON file.\"\"\"\n\n    def __init__(self, output_path: str = \"job.TYPE.json\"):\n        super().__init__(\"move\", output_path)\n\n        self.default_config = {\n            \"type\": \"move\",\n            \"bucket\": \"&lt;minio bucket&gt;\",\n            \"remote_path\": \"&lt;minio path&gt;\",\n            \"remote_dest\": \"&lt;minio dest path&gt;\",\n            \"dest_bucket\": \"&lt;minio dest bucket&gt;\",\n        }\n\n\ndef parse_args() -&gt; argparse.Namespace:\n    \"\"\"Call this function to parse input args from the command line.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"CLI for uploading files &amp; directories to minio.\"\n    )\n\n    parser.add_argument(\"--debug\", action=\"store_true\", help=\"Enable debug logging\")\n    parser.add_argument(\n        \"-g\", \"--generate-configs\", action=\"store_true\", help=\"Generate default configs\"\n    )\n    parser.add_argument(\n        \"-c\",\n        \"--minio-config\",\n        type=str,\n        help=\"Path to minio config file. Default: minio_conf.json\",\n        default=\"minio_conf.json\",\n    )\n    parser.add_argument(\n        \"-j\",\n        \"--job-file\",\n        type=str,\n        help=\"Path to a JSON file describing an upload/download job.\",\n    )\n    parser.add_argument(\n        \"-e\",\n        \"--minio-endpoint\",\n        type=str,\n        help=\"Minio endpoint\",\n    )\n    parser.add_argument(\"-ak\", \"--minio-access-key\", type=str, help=\"Minio access key\")\n    parser.add_argument(\"-sk\", \"--minio-secret-key\", type=str, help=\"Minio secret key\")\n    parser.add_argument(\"-b\", \"--bucket\", type=str, help=\"Minio bucket to upload to\")\n    parser.add_argument(\n        \"-l\", \"--local-path\", type=str, help=\"Local path to upload to minio\"\n    )\n    parser.add_argument(\"-t\", \"--minio-path\", type=str, help=\"Minio path to upload to\")\n    parser.add_argument(\"--secure\", action=\"store_true\", help=\"Use secure connection\")\n    parser.add_argument(\"--check-cert\", action=\"store_true\", help=\"Check certificate\")\n\n    args = parser.parse_args()\n\n    return args\n\n\ndef generate_default_configs():\n    log.info(\"Generating default configurations\")\n\n    try:\n        MinioConfigGenerator().generate()\n        MinioUploadJobGenerator().generate()\n        MinioDownloadJobGenerator().generate()\n        MinioDeleteJobGenerator().generate()\n        MinioCopyJobGenerator().generate()\n        MinioMoveJobGenerator().generate()\n    except Exception as e:\n        log.error(f\"Error generating default configs: {e}\")\n\n\ndef load_minio_job(\n    job_file: str,\n) -&gt; t.Union[\n    MinioCopyJob, MinioMoveJob, MinioDeleteJob, MinioUploadJob, MinioDownloadJob\n]:\n    \"\"\"Loads a Minio job from a JSON file and returns the appropriate dataclass.\n\n    Params:\n        job_file (str): Path to the JSON job file.\n\n    Returns:\n        MinioJob: An instance of the appropriate MinioJob dataclass.\n\n    \"\"\"\n    job_file = normalize_path(job_file)\n\n    with open(job_file, \"r\") as f:\n        job_data = json.load(f)\n\n    job_type = job_data.get(\"type\")\n    if not job_type:\n        raise ValueError(\"Job file must contain a 'type' field.\")\n\n    if job_type == \"upload\":\n        return MinioUploadJob(**job_data)\n    elif job_type == \"download\":\n        return MinioDownloadJob(**job_data)\n    elif job_type == \"delete\":\n        return MinioDeleteJob(**job_data)\n    elif job_type == \"copy\":\n        return MinioCopyJob(**job_data)\n    elif job_type == \"move\":\n        return MinioMoveJob(**job_data)\n    else:\n        raise ValueError(f\"Unsupported job type: {job_type}\")\n\n\ndef get_minio_controller(\n    endpoint: str,\n    access_key: str,\n    secret_key: str,\n    secure: bool = True,\n    cert_check: bool = True,\n) -&gt; \"MinioController\":\n    \"\"\"Initializes a MinioController instance.\n\n    Params:\n        endpoint (str): Minio endpoint.\n        access_key (str): Minio access key.\n        secret_key (str): Minio secret key.\n        secure (bool): Whether to use SSL.\n        cert_check (bool): Whether to verify SSL certificates.\n\n    Returns:\n        (MinioController): A controller class to handle minio operations.\n\n    \"\"\"\n    try:\n        ## Intialize MinioController instance\n        controller: MinioController = MinioController(\n            endpoint=endpoint,\n            access_key=access_key,\n            secret_key=secret_key,\n            secure=secure,\n            cert_check=cert_check,\n        )\n\n        return controller\n    except Exception as e:\n        log.error(f\"Error initializing MinioController: {e}\")\n        raise\n\n\ndef normalize_path(path: t.Union[str, Path]) -&gt; str:\n    \"\"\"Converts a local or remote path to a normalized path (forward slashes '/').\n\n    Params:\n        path (str): Local or remote path.\n\n    Returns:\n        (str): Normalized path.\n\n    \"\"\"\n    if not isinstance(path, Path) and not isinstance(path, str):\n        raise TypeError(f\"Expected str or Path, got {type(path)}\")\n\n    ## Return normalized path\n    return str(Path(path).as_posix()).replace(\"\\\\\", \"/\")\n\n\nclass MinioController:\n    \"\"\"Handler class for Minio operations.\n\n    Attributes:\n        logger (logging.Logger): Logger for this class.\n        client (minio.Minio): Minio client.\n\n    Methods:\n        _upload (self, bucket: str, local_path: str, remote_path: str): Handles uploading a single file or directory recursively.\n        _download (self, bucket: str, remote_path: str, local_path: str): Handles downloading a single file or directory recursively.\n        _delete (self, bucket: str, remote_path: str): Deletes a single file or all files in a directory (prefix).\n        upload (self, bucket: str, local_path: str, remote_path: str): Uploads a single file or directory recursively.\n        download (self, bucket: str, remote_paths: t.Union[str, t.List[str]], local_path: str): Downloads one or more files/directories from MinIO.\n        delete (self, bucket: str, remote_path: str): Deletes a single file or all files in a directory (prefix).\n        exists (self, bucket: str, remote_path: str): Checks if a file or directory exists in MinIO.\n\n    Raises:\n        Exception: If there is an error connecting to MinIO.\n\n    Example:\n        controller = MinioController(endpoint=\"minio.example.com\", access_key=\"minioaccesskey\", secret_key=\"miniosecretkey\", secure=True, cert_check=True)\n        controller.upload(\"mybucket\", \"/path/to/local/file\", \"/path/to/remote/file\")\n\n    \"\"\"\n\n    def __init__(\n        self,\n        endpoint: str,\n        access_key: str,\n        secret_key: str,\n        secure: bool,\n        cert_check: bool,\n    ) -&gt; None:\n        ## Initialize class logger\n        self.logger = log.getChild(\"MinioController\")\n\n        try:\n            self.client = Minio(\n                endpoint,\n                access_key=access_key,\n                secret_key=secret_key,\n                secure=secure,\n                cert_check=cert_check,\n            )\n        except Exception as exc:\n            self.logger.error(f\"Error connecting to minio: {exc}\")\n            raise\n\n    def _upload(self, bucket: str, local_path: str, remote_path: str) -&gt; None:\n        \"\"\"Handles uploading a single file or directory recursively.\n\n        Params:\n            bucket (str): Bucket name.\n            local_path (str): Local path to file or directory.\n            remote_path (str): Remote path to file or directory.\n\n        Raises:\n            FileNotFoundError: If local_path does not exist or is not a file/directory.\n\n        Returns:\n            None\n\n        \"\"\"\n        if not self.client.bucket_exists(bucket):\n            self.client.make_bucket(bucket)\n\n        ## Replace any \\ or \\\\ with /\n        remote_path = normalize_path(remote_path)\n\n        if Path(local_path).is_file():\n            ## Path is file, upload single object\n            self.logger.debug(\n                f\"Uploading file '{local_path}' to '{remote_path}' in bucket '{bucket}'\"\n            )\n            try:\n                self.client.fput_object(bucket, remote_path, local_path)\n                self.logger.debug(\n                    f\"Uploaded '{local_path}' to '{remote_path}' in bucket '{bucket}'\"\n                )\n            except Exception as exc:\n                self.logger.error(\n                    f\"Error uploading '{local_path}' to '{remote_path}' in bucket '{bucket}': {exc}\"\n                )\n                raise\n\n        elif Path(local_path).is_dir():\n            ## Path is a directory, create path &amp; upload all objects\n            for local_file in glob.glob(local_path + \"/**\", recursive=True):\n\n                if Path(local_file).is_file():\n                    rel_path = Path(local_file).relative_to(local_path)\n                    remote_file_path = (Path(remote_path) / rel_path).as_posix()\n\n                    self.logger.debug(\n                        f\"Uploading file '{local_file}' to '{remote_file_path}' in bucket '{bucket}'\"\n                    )\n                    try:\n                        self.client.fput_object(bucket, remote_file_path, local_file)\n                        self.logger.debug(\n                            f\"Uploaded '{local_file}' to '{remote_file_path}' in bucket '{bucket}'\"\n                        )\n                    except Exception as exc:\n                        self.logger.error(\n                            f\"Error uploading '{local_file}' to '{remote_file_path}' in bucket '{bucket}': {exc}\"\n                        )\n                        raise\n\n        else:\n            ## Path is not a file or directory\n            raise FileNotFoundError(\n                f\"Local path '{local_path}' does not exist or is not a file/directory.\"\n            )\n\n    def _download(self, bucket: str, remote_path: str, local_path: str):\n        \"\"\"Handles downloading a single file or directory recursively.\n\n        Params:\n            bucket (str): Bucket name.\n            remote_path (str): Remote path to file or directory.\n            local_path (str): Local path to file or directory.\n\n        Raises:\n            FileNotFoundError: If remote_path does not exist or is not a file/directory.\n\n        Returns:\n            None\n\n        \"\"\"\n        ## Replace any \\ or \\\\ with /\n        remote_path = normalize_path(remote_path)\n        local_path = Path(local_path)\n\n        ## Check if remote_path is a directory (prefix) or file\n        if remote_path.endswith(\"/\"):\n            prefix = remote_path\n            objects = self.client.list_objects(bucket, prefix=prefix, recursive=True)\n\n            for obj in objects:\n                rel_path = Path(obj.object_name).relative_to(prefix)\n                local_file = local_path / rel_path\n                local_file.parent.mkdir(parents=True, exist_ok=True)\n\n                self.logger.debug(f\"Downloading '{obj.object_name}' to '{local_file}'\")\n                self.client.fget_object(bucket, obj.object_name, str(local_file))\n        else:\n            ## Try to stat object to ensure it exists and is a file\n            try:\n                self.client.stat_object(bucket, remote_path)\n            except S3Error as e:\n                if e.code == \"NoSuchKey\":\n                    raise FileNotFoundError(\n                        f\"Remote object '{remote_path}' not found in bucket '{bucket}'\"\n                    )\n                raise\n\n            local_path.parent.mkdir(parents=True, exist_ok=True)\n\n            self.logger.debug(f\"Downloading '{remote_path}' to '{local_path}'\")\n            self.client.fget_object(bucket, remote_path, str(local_path))\n\n    def _copy(\n        self, bucket: str, src_path: str, dest_path: str, dest_bucket: str | None = None\n    ):\n        \"\"\"Copies a file or directory (prefix) from one location to another within MinIO.\n\n        Params:\n            bucket (str): Bucket name.\n            src_path (str): Source path to file or directory.\n            dest_path (str): Destination path to file or directory.\n            dest_bucket (str, optional): Destination bucket name. Defaults to None.\n\n        Raises:\n            ValueError: If no remote paths are provided.\n\n        Returns:\n            None\n\n        \"\"\"\n        src_path = normalize_path(src_path)\n        dest_path = normalize_path(dest_path)\n        dest_bucket = dest_bucket or bucket\n\n        try:\n            ## Use stat_object to check if the source exists and is a file.\n            try:\n                self.client.stat_object(bucket, src_path)\n                ## It's a file, copy the file\n                copy_source = CopySource(bucket, src_path)\n                log.debug(\n                    f\"Copying object '{src_path}' from bucket '{bucket}' to '{dest_path}' in bucket '{dest_bucket}'\"\n                )\n                self.client.copy_object(dest_bucket, dest_path, copy_source)\n\n            except S3Error as e:\n                if e.code == \"NoSuchKey\":\n                    ## It might be a directory, list objects with the src_path as a prefix\n                    objects = list(\n                        self.client.list_objects(\n                            bucket, prefix=src_path, recursive=True\n                        )\n                    )\n                    if objects:\n                        ## It's a directory (prefix), copy all objects under the prefix\n                        for obj in objects:\n                            ## Calculate the relative path within the source directory\n                            #  Ensure no leading slash\n                            rel_path = obj.object_name[len(src_path) :].lstrip(\"/\")\n                            #  Correctly join paths\n                            new_dest_path = f\"{dest_path.rstrip('/')}/{rel_path}\"\n                            copy_source = CopySource(bucket, obj.object_name)\n                            log.debug(\n                                f\"Copying object '{obj.object_name}' from bucket '{bucket}' to '{new_dest_path}' in bucket '{dest_bucket}'\"\n                            )\n                            self.client.copy_object(\n                                dest_bucket, new_dest_path, copy_source\n                            )\n                    else:\n                        ## No such key or directory\n                        raise FileNotFoundError(\n                            f\"Source path '{src_path}' not found in bucket '{bucket}'.\"\n                        ) from e\n                else:\n                    ## Re-raise the S3Error if it's not a NoSuchKey error\n                    raise\n\n        except Exception as exc:\n            self.logger.error(\n                f\"Error copying '{src_path}' from bucket '{bucket}' to '{dest_path}' in bucket '{dest_bucket}': {exc}\"\n            )\n            raise\n\n    def _delete(self, bucket: str, remote_path: str):\n        \"\"\"Deletes a single object or all objects under a prefix (simulated directory).\n\n        Params:\n            bucket (str): Bucket name.\n            remote_path (str): Remote path to file or directory.\n\n        Raises:\n            ValueError: If bucket does not exist.\n\n        Returns:\n            None\n\n        \"\"\"\n        remote_path = normalize_path(remote_path).rstrip(\"/\")\n\n        if not self.client.bucket_exists(bucket):\n            raise ValueError(f\"Bucket '{bucket}' does not exist.\")\n\n        ## Check if it's a \"directory\" (prefix)\n        prefix = remote_path + \"/\"\n        objects = list(self.client.list_objects(bucket, prefix=prefix, recursive=True))\n\n        if objects:\n            delete_objects = [DeleteObject(obj.object_name) for obj in objects]\n\n            self.logger.debug(\n                f\"Deleting [{len(delete_objects)}] object(s) under prefix '{prefix}'\"\n            )\n            try:\n                for error in self.client.remove_objects(bucket, delete_objects):\n                    self.logger.error(\n                        f\"Error deleting object '{error.object_name}': {error}\"\n                    )\n            except Exception as exc:\n                self.logger.error(f\"Failed to delete objects under '{prefix}': {exc}\")\n                raise\n            return\n\n        ## If not a prefix, try deleting as a single file\n        try:\n            self.client.remove_object(bucket, remote_path)\n            self.logger.debug(\n                f\"Deleted single object '{remote_path}' from bucket '{bucket}'\"\n            )\n        except S3Error as e:\n            if e.code == \"NoSuchKey\":\n                self.logger.warning(\n                    f\"Object '{remote_path}' not found in bucket '{bucket}'\"\n                )\n            else:\n                self.logger.error(\n                    f\"Error deleting '{remote_path}' from bucket '{bucket}': {e}\"\n                )\n                raise\n        except Exception as exc:\n            self.logger.error(\n                f\"Error deleting '{remote_path}' from bucket '{bucket}': {exc}\"\n            )\n            raise\n\n    def upload(\n        self, bucket: str, local_paths: t.Union[str, t.List[str]], remote_path: str = \"\"\n    ):\n        \"\"\"Uploads one or more files/directories to MinIO.\"\"\"\n        if isinstance(local_paths, str):\n            ## Replace any \\ or \\\\ with /\n            local_paths = [normalize_path(local_paths)]\n\n        ## Replace any \\ or \\\\ with /\n        remote_path = normalize_path(remote_path)\n\n        for path in local_paths:\n            base_name = Path(path).name\n            if remote_path:\n                dest_path = (Path(remote_path) / base_name).as_posix()\n            else:\n                dest_path = base_name\n\n            self._upload(bucket, path, dest_path)\n\n    def download(\n        self, bucket: str, remote_paths: t.Union[str, t.List[str]], local_path: str\n    ):\n        \"\"\"Downloads one or more files/directories from MinIO.\"\"\"\n        if isinstance(remote_paths, str):\n            ## Replace any \\ or \\\\ with /\n            remote_paths = [normalize_path(remote_paths)]\n\n        for remote_path in remote_paths:\n            ## Replace any \\ or \\\\ with /\n            remote_path = normalize_path(remote_path)\n            self._download(bucket, remote_path, local_path)\n\n    def object_exists(self, bucket: str, remote_path: str) -&gt; bool:\n        \"\"\"Checks if a file or directory exists in MinIO.\n\n        Params:\n            bucket (str): Bucket name.\n            remote_path (str): Remote path to file or directory.\n\n        Returns:\n            bool\n\n        \"\"\"\n        remote_path = normalize_path(remote_path).rstrip(\"/\")\n\n        try:\n            self.client.stat_object(bucket, remote_path)\n            return True\n        except S3Error as e:\n            if e.code == \"NoSuchKey\":\n                ## Could be a prefix/folder \u2014 let's check with list_objects\n                prefix = remote_path + \"/\"\n                objs = list(\n                    self.client.list_objects(bucket, prefix=prefix, recursive=True)\n                )\n                return len(objs) &gt; 0\n            elif e.code == \"AccessDenied\":\n                self.logger.error(\n                    f\"Access denied when checking object: {bucket}/{remote_path}\"\n                )\n                raise\n            else:\n                raise\n\n    def search_files(self, bucket: str, prefix: str) -&gt; t.List[str]:\n        \"\"\"Searches a MinIO bucket for files with a given prefix.\n\n        Params:\n            bucket (str): Bucket name.\n            prefix (str): Prefix to search for.\n\n        Returns:\n            List[str]\n\n        \"\"\"\n        self.logger.debug(\n            f\"Searching minio bucket '{bucket}' for files with prefix '{prefix}'\"\n        )\n\n        try:\n            results = [\n                obj.object_name\n                for obj in self.client.list_objects(\n                    bucket, prefix=prefix, recursive=True\n                )\n            ]\n            self.logger.debug(\n                f\"Found [{len(results)}] file(s) with prefix '{prefix}' in minio bucket '{bucket}'\"\n            )\n\n            return results\n        except Exception as exc:\n            self.logger.error(\n                f\"Error searching minio bucket '{bucket}' for files with prefix '{prefix}': {exc}\"\n            )\n            raise\n\n    def copy(\n        self, bucket: str, src_path: str, dest_path: str, dest_bucket: str | None = None\n    ):\n        \"\"\"Copies a file or directory (prefix) from one location to another within MinIO.\n\n        Params:\n            bucket (str): Bucket name.\n            src_path (str): Source path to file or directory.\n            dest_path (str): Destination path to file or directory.\n            dest_bucket (str | None, optional): Destination bucket name. Defaults to None.\n\n        Raises:\n            ValueError: If bucket does not exist.\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            self._copy(bucket, src_path, dest_path, dest_bucket)\n        except Exception as e:\n            self.logger.error(f\"Failed to copy {src_path} to {dest_path}: {e}\")\n            raise\n\n    def move(\n        self, bucket: str, src_path: str, dest_path: str, dest_bucket: str | None = None\n    ):\n        \"\"\"Moves a file or directory from one location to another within MinIO.\n\n        Params:\n            bucket (str): Bucket name.\n            src_path (str): Source path to file or directory.\n            dest_path (str): Destination path to file or directory.\n            dest_bucket (str | None, optional): Destination bucket name. Defaults to None.\n\n        Raises:\n            ValueError: If bucket does not exist.\n\n        Returns:\n            None\n\n        \"\"\"\n        src_path = normalize_path(src_path)\n        dest_path = normalize_path(dest_path)\n        dest_bucket = dest_bucket or bucket\n\n        try:\n            ## Copy the object(s) first\n            self._copy(bucket, src_path, dest_path, dest_bucket)\n\n            ## Determine if we are moving a single file or a directory (prefix)\n            try:\n                self.client.stat_object(bucket, src_path)\n                ## It's a file, remove the single file\n                self.client.remove_object(bucket, src_path)\n                log.debug(f\"Removed object '{src_path}' from bucket '{bucket}'\")\n\n            except S3Error as e:\n                if e.code == \"NoSuchKey\":\n                    ## It must be a directory, remove all objects under the prefix\n                    objects = list(\n                        self.client.list_objects(\n                            bucket, prefix=src_path, recursive=True\n                        )\n                    )\n                    if objects:\n                        for obj in objects:\n                            self.client.remove_object(bucket, obj.object_name)\n                            log.debug(\n                                f\"Removed object '{obj.object_name}' from bucket '{bucket}'\"\n                            )\n                    else:\n                        raise FileNotFoundError(\n                            f\"Source path '{src_path}' not found in bucket '{bucket}'.\"\n                        ) from e\n                else:\n                    ## Re-raise the S3Error if it's not a NoSuchKey error\n                    raise\n\n        except Exception as exc:\n            self.logger.error(\n                f\"Error moving '{src_path}' from bucket '{bucket}' to '{dest_path}' in bucket '{dest_bucket}': {exc}\"\n            )\n            raise\n\n    def delete(self, bucket: str, remote_paths: t.Union[str, t.List[str]]):\n        \"\"\"Deletes one or more files/directories from MinIO.\n\n        Params:\n            bucket (str): Bucket name.\n            remote_path (str): Remote path to file or directory.\n\n        Raises:\n            ValueError: If no remote paths are provided.\n\n        Returns:\n\n        \"\"\"\n        if not remote_paths:\n            raise ValueError(\"No remote paths provided to delete.\")\n\n        if isinstance(remote_paths, str):\n            ## Replace any \\ or \\\\ with /\n            remote_paths = [remote_paths]\n\n        for remote_path in remote_paths:\n            ## Replace any \\ or \\\\ with /\n            remote_path = normalize_path(remote_path)\n            self._delete(bucket, remote_path)\n\n\nif __name__ == \"__main__\":\n    \"\"\"If the script is called directly, initialize a MinioController and run the CLI.\"\"\"\n    import argparse\n    import json\n    import logging\n\n    args = parse_args()\n\n    ## Configure logging\n    logging.basicConfig(\n        level=\"DEBUG\" if args.debug else \"INFO\",\n        format=(\n            \"%(asctime)s | [%(levelname)s] | %(name)s:%(lineno)s :: %(message)s\"\n            if args.debug\n            else \"%(asctime)s [%(levelname)s] :: %(message)s\"\n        ),\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n    )\n    for _logger in [\"urllib3\"]:\n        logging.getLogger(\"urllib3\").disabled = True\n\n    if args.generate_configs:\n        try:\n            generate_default_configs()\n            exit(0)\n        except Exception as exc:\n            log.error(exc)\n            exit(1)\n\n    ## Load MinIO configuration\n    if args.minio_config and Path(args.minio_config).exists():\n        with open(args.minio_config, \"r\") as f:\n            minio_config = json.load(f)\n    elif args.minio_config and not Path(args.minio_config).exists():\n        log.warning(\n            f\"Minio config file not found at path '{args.minio_config}'. Generating default config\"\n        )\n        generated_config = MinioConfigGenerator()\n        generated_config.generate()\n\n    endpoint = args.minio_endpoint or minio_config.get(\"endpoint\")\n    access_key = args.minio_access_key or minio_config.get(\"access_key\")\n    secret_key = args.minio_secret_key or minio_config.get(\"secret_key\")\n    secure = args.secure or minio_config.get(\"secure\")\n    cert_check = args.check_cert or minio_config.get(\"cert_check\")\n\n    if not all([endpoint, access_key, secret_key]):\n        log.error(\n            \"Missing MinIO configuration.  Please provide endpoint, access_key, and secret_key via command line or minio_config.json.\"\n        )\n        exit(1)\n\n    ## Initialize MinIO Controller\n    controller = get_minio_controller(\n        endpoint, access_key, secret_key, secure=secure, cert_check=cert_check\n    )\n\n    ## Handle job file\n    if args.job_file:\n        try:\n            job = load_minio_job(args.job_file)\n\n            if isinstance(job, MinioUploadJob):\n                controller.upload(job.bucket, job.local_path, job.remote_path)\n            elif isinstance(job, MinioDownloadJob):\n                controller.download(job.bucket, job.remote_path, job.local_path)\n            elif isinstance(job, MinioDeleteJob):\n                controller.delete(job.bucket, job.remote_path)\n            elif isinstance(job, MinioCopyJob):\n                controller.copy(\n                    bucket=job.bucket,\n                    src_path=job.remote_path,\n                    dest_path=job.remote_dest,\n                    dest_bucket=job.dest_bucket,\n                )\n            elif isinstance(job, MinioMoveJob):\n                controller.move(\n                    bucket=job.bucket,\n                    src_path=job.remote_path,\n                    dest_path=job.remote_dest,\n                    dest_bucket=job.dest_bucket,\n                )\n\n            log.info(f\"Job '{args.job_file}' completed successfully.\")\n\n        except Exception as e:\n            log.error(f\"Error processing job file '{args.job_file}': {e}\")\n            exit(0)\n\n        log.info(\"Job completed successfully.\")\n    else:\n        log.error(\"No job file provided.\")\n        exit(1)\n</code></pre>","tags":["python","controllers","snippets","s3","minio"]},{"location":"snippets/Python%20Snippets/path_snippets/index.html","title":"Path module snippets","text":"<p>Code snippets for the <code>pathlib.Path</code> stdlib module.</p>","tags":["python","snippets"]},{"location":"snippets/Python%20Snippets/path_snippets/index.html#read-files-embedded-in-source-code","title":"Read files embedded in source code","text":"<p>If you have files like <code>.json</code> or <code>.txt</code> embedded in your app, i.e. for a module that reads from this file where both the <code>.json</code>/<code>.txt</code> file and the <code>.py</code> file that reads them are in the same subdirectory in your Python app, you need to append the <code>.py</code> file's path to the system path. This lets the Python file open the text file using <code>with open(\"./example.json\", \"r\") as f:</code>.</p> Add module path to sys.path<pre><code>from pathlib import Path\nimport sys\n\n# Resolve the path to the directory containing the current file\nmodule_path = Path(__file__).parent.resolve()\n\nfile_path = module_path / \"example.json\"\n</code></pre>","tags":["python","snippets"]},{"location":"snippets/Python%20Snippets/path_snippets/index.html#example-read-json-file","title":"Example: read json file","text":"<p>As an example, say you have a package named <code>my_pymodule</code>, with code structured like this:</p> my_pymodule package<pre><code>src/\n  my_pymodule/\n    json_reader/\n      __init__.py\n      reader.py\n      values.json\n    __init__.py\n    main.py\n</code></pre> <p>From <code>reader.py</code>, you want to load <code>./values.json</code> and return to a function that calls the <code>reader.py</code> file, i.e. <code>main.py</code>.</p> src/my_pymodule/json_reader/reader.py<pre><code>from pathlib import Path\n\n# Resolve the path to the directory containing the current file\nmodule_path = Path(__file__).parent.resolve()\n\n## Set path to values.json\nVALUES_JSON_FILE = module_path / \"values.json\"\n\n\ndef read_values() -&gt; dict:\n    ## Read the JSON file\n    with open(VALUES_JSON_FILE, 'r', encoding='utf-8') as f:\n        data = f.read()\n\n    return data\n</code></pre> <p>Then in <code>src/my_pymodule/main.py</code>, import the <code>read_values()</code> function. Calling <code>main.py</code> will set your path to wherever you called the script from, but <code>read_values()</code> will always open the <code>example.json</code> file that exists in the same path as the <code>reader.py</code> module:</p> src/my_pymodule/main.py<pre><code>from my_pymodule.json_reader import read_values\n\n\ndef main():\n    ## Read values from the embedded values.json file\n    values: dict = read_values()\n    print(f\"Values: {values}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>","tags":["python","snippets"]},{"location":"snippets/Python%20Snippets/path_snippets/index.html#append-modules-grandparent-path","title":"Append module's 'grandparent' path","text":"<p>Appends the path <code>../..</code> to Python's path. Add more <code>.parent</code> for more deeply nested modules.</p> <p>This is useful if you have a path like <code>sandbox/</code> at your root. In the sandbox apps you build in this directory, whatever your entrypoint is (i.e. <code>sandbox/ex_app/main.py</code>), add the code below to fix paths when running sandbox apps from the directory above <code>sandbox/</code>.</p> Append path to root<pre><code>import sys\nimport os\n\nsys.path.append(str(Path(__file__).resolve().parent.parent))\n</code></pre>","tags":["python","snippets"]},{"location":"template/index.html","title":"Templates","text":"<p>Copy/paste-able code for Docker containers, Python scripts, &amp; more.</p>","tags":["docker","templates"]},{"location":"template/docker/index.html","title":"Docker container templates","text":"<p>Doc pages for specific containers/container stacks I use.</p>","tags":["docker","templates"]},{"location":"template/docker/automation/index.html","title":"Automation templates","text":"","tags":["docker","templates","automation"]},{"location":"template/docker/automation/n8n/index.html","title":"n8n","text":"<p><code>n8n</code> is a workflow automation tool. Similar to Zapier, <code>n8n</code> can integrate with many sources and provide a no/low code (javascript) interface for building your own automations.</p> <p>Automate your home by using <code>n8n</code> with <code>homeassistant</code>, make automated HTTP requests to REST APIs and do something with the response (i.e. send the daily weather forcast via Telegram), and more.</p>","tags":["docker","templates","automation","nocode"]},{"location":"template/docker/automation/n8n/index.html#directory-structure","title":"Directory Structure","text":"Container directory structure<pre><code>docker_n8n/\n  ../.env\n  ../.gitignore\n  ../docker-compose.yml\n  ../generate_encryption_key.sh\n</code></pre>","tags":["docker","templates","automation","nocode"]},{"location":"template/docker/automation/n8n/index.html#container-files","title":"Container Files","text":"","tags":["docker","templates","automation","nocode"]},{"location":"template/docker/automation/n8n/index.html#env","title":".env","text":"n8n .env<pre><code>## Default: Etc/UTC\nTZ=\n\n## Default: 5678\nN8N_PORT=\n## Default: named volume 'n8n_conf'\nN8N_DATA_DIR=\n\n## Default: true\nN8N_BASIC_AUTH=\n## Default: n8n\nN8N_BASIC_AUTH_USER=\n## Default: n8nadmin\nN8N_BASIC_AUTH_PASSWORD=\n\n## Default: false\nN8N_SMTP_SSL=\n\n## Default: unset\nN8N_HOST=\n## Default: https\nN8N_PROTOCOL=\n## Default: production\nN8N_NODE_ENV=\n\n## Default: unset\nN8N_WEBHOOK_URL=\n\n## Default: unset\nN8N_ENCRYPTION_KEY=&lt;generate by running ./generate_encryption_key.sh&gt;\n</code></pre>","tags":["docker","templates","automation","nocode"]},{"location":"template/docker/automation/n8n/index.html#docker-composeyml","title":"docker-compose.yml","text":"docker-compose.yml<pre><code>version: \"3\"\n\nvolumes:\n  n8n_conf:\n  n8n_files:\n\nservices:\n\n  n8n:\n    image: n8nio/n8n\n    container_name: n8n\n    restart: unless-stopped\n    ports:\n      - ${N8N_PORT:-5678}:5678\n    volumes:\n      - ${N8N_DATA_DIR:-n8n_conf}:/home/node/.n8n\n      - ${N8N_WORKFLOW_FILES_DIR:-n8n_files}:/files\n    environment:\n      - TZ=${TZ:-America/New_York}\n      - GENERIC_TIMEZONE=${TZ:-America/New_York}\n      - N8N_BASIC_AUTH_ACTIVE=${N8N_BASIC_AUTH:-true}\n      - N8N_BASIC_AUTH_USER=${N8N_BASIC_AUTH_USER:-n8n}\n      - N8N_BASIC_AUTH_PASSWORD=${N8N_BASIC_AUTH_PASSWORD:-n8nadmin}\n      # If accessing outside the local network, uncomment below\n      - N8N_HOST=${N8N_HOST}\n      - N8N_PORT=${N8N_PORT:-5678}\n      - N8N_PROTOCOL=${N8N_PROTOCOL:-https}\n      - N8N_NODE_ENV=${N8N_NODE_ENV:-production}\n      - N8N_ENCRYPTION_KEY=${N8N_ENCRYPTION_KEY}\n      - WEBHOOK_URL=${N8N_WEBHOOK_URL}\n      - N8N_SMTP_SSL=${N8N_SMTP_SSL:-false}\n</code></pre>","tags":["docker","templates","automation","nocode"]},{"location":"template/docker/automation/n8n/index.html#generate_encryption_keysh","title":"generate_encryption_key.sh","text":"generate_encryption_key.sh<pre><code>#!/bin/bash\n\nkey=$(openssl rand -hex 32)\n\necho \"Encryption key:\"\necho \"$key\"\n</code></pre>","tags":["docker","templates","automation","nocode"]},{"location":"template/docker/automation/n8n/index.html#notes","title":"Notes","text":"","tags":["docker","templates","automation","nocode"]},{"location":"template/docker/automation/n8n/index.html#links","title":"Links","text":"","tags":["docker","templates","automation","nocode"]},{"location":"template/docker/automation/prefect_server/index.html","title":"Prefect","text":"<p><code>prefect</code> is a Python library for automating data workflows/pipelines. A lighter, more beginner-friendly analogue to Apache <code>airflow</code>.</p> <p>This container is the server &amp; database for <code>prefect</code>. You still need to write Python code to interact with the pipelines.</p> <p>Visit the web UI at port <code>4200</code>. This is a dashboard where you can see pipelines/flows you've written and executed.</p>","tags":["docker","templates","automation","python","data"]},{"location":"template/docker/automation/prefect_server/index.html#directory-structure","title":"Directory Structure","text":"Container directory structure<pre><code>docker_prefect_server/\n  ../postgres/\n    ../pg_entrypoint/\n      ../`pg_entrypoint.sh`\n  ../prefect/\n  ../.env\n  ../.gitignore\n  ../docker-compose.yml\n</code></pre>","tags":["docker","templates","automation","python","data"]},{"location":"template/docker/automation/prefect_server/index.html#container-files","title":"Container Files","text":"","tags":["docker","templates","automation","python","data"]},{"location":"template/docker/automation/prefect_server/index.html#docker-composeyml","title":"docker-compose.yml","text":"prefect docker-compose.yml<pre><code>---\nnetworks:\n  prefect_net:\n\nservices:\n  prefect:\n    image: prefecthq/prefect:2-python3.11\n    restart: unless-stopped\n    container_name: prefect-server\n    env_file: .env\n    entrypoint: [\"prefect\", \"server\", \"start\"]\n    volumes:\n      - ${PREFECT_DATA_DIR:-./prefect/data}:/root/.prefect\n    ports:\n      - ${PREFECT_WEBUI_PORT:-4200}:4200\n    environment:\n      PREFECT_SERVER_API_HOST: 0.0.0.0\n      PREFECT_UI_URL: http://prefect:4200/api\n      PREFECT_API_URL: http://prefect:4200/api\n      PREFECT_API_DATABASE_CONNECTION_URL: ${PREFECT_DB_URL:-postgresql+asyncpg://postgres:postgres@prefect-db/prefect}\n      PREFECT_API_DATABASE_ECHO: ${PREFECT_DB_ECHO:-false}\n      PREFECT_API_DATABASE_MIGRATE_ON_START: ${PREFECT_MIGRATE_ON_START:-true}\n    depends_on:\n      - prefect-db\n    networks:\n      - prefect_net\n\n  prefect-db:\n    image: postgres:latest\n    container_name: prefect-db\n    restart: unless-stopped\n    environment:\n      POSTGRES_USER: ${POSTGRES_USER:-postgres}\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}\n      POSTGRES_DB: ${POSTGRES_DATABASE:-prefect}\n      POSTGRES_HOST_AUTH_METHOD: ${POSTGRES_HOST_AUTH_METHOD}\n    expose:\n      - 5432\n    ports:\n      - ${POSTGRES_PORT:-5432}:5432\n    volumes:\n      - ${POSTGRES_DATA_DIR:-./postgres/data}:/var/lib/postgresql/data\n      - ./postgres/pg_entrypoint:/docker-entrypoint-initdb.d\n    networks:\n      - prefect_net\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U ${POSTGRES_USER}\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n</code></pre>","tags":["docker","templates","automation","python","data"]},{"location":"template/docker/automation/prefect_server/index.html#env","title":".env","text":"prefect .env<pre><code>## Default: ./prefect/data\nPREFECT_DATA_DIR=\n## Default: 4200\nPREFECT_WEBUI_PORT=\n## postgresql+asyncpg://postgres:postgres@prefect-db/prefect\nPREFECT_DB_URL=\n## Default: false\nPREFECT_DB_ECHO=\n## Default: true\nPREFECT_MIGRATE_ON_START=\n\n## Default: postgres\nPOSTGRES_USER=\n## Default: postgres\nPOSTGRES_PASSWORD=\n## Default: ./postgres/data\nPOSTGRES_DATA_DIR=\n## Default: 5432\nPOSTGRES_PORT=\n## Default: empty/None\nPOSTGRES_HOST_AUTH_METHOD=\n## Default: prefect\nPOSTGRES_DATABASE=\n</code></pre>","tags":["docker","templates","automation","python","data"]},{"location":"template/docker/automation/prefect_server/index.html#gitignore","title":".gitignore","text":"prefect .gitignore<pre><code>prefect/data\npostgres/data\n</code></pre>","tags":["docker","templates","automation","python","data"]},{"location":"template/docker/automation/prefect_server/index.html#postgrespg_entrypointpg_entrypointsh","title":"postgres/pg_entrypoint/pg_entrypoint.sh","text":"prefect pg_entrypoint.sh<pre><code>#!/bin/bash\nset -e\n\npsql -v ON_ERROR_STOP=1 --username \"$POSTGRES_USER\" --dbname=\"$POSTGRES_DB\" &lt;&lt;-EOSQL\n   CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\n   CREATE EXTENSION IF NOT EXISTS \"pg_trgm\";\nEOSQL\n</code></pre>","tags":["docker","templates","automation","python","data"]},{"location":"template/docker/automation/prefect_server/index.html#notes","title":"Notes","text":"","tags":["docker","templates","automation","python","data"]},{"location":"template/docker/automation/prefect_server/index.html#links","title":"Links","text":"<ul> <li>Prefect Docs: Quickstart</li> <li>Deploying Prefect with Docker Compose</li> </ul>","tags":["docker","templates","automation","python","data"]},{"location":"template/docker/databases/index.html","title":"Databases","text":"<p>Docker containers for databases like PostgreSQL, MySQL/MariaDB, InfluxDB, and more.</p>","tags":["docker","templates","database"]},{"location":"template/docker/databases/influxdb/index.html","title":"InfluxDB","text":"<p>Dockerized InfluxDB database server. InfluxDB is a special purpose time-series database.</p>","tags":["docker","templates","database","influxdb"]},{"location":"template/docker/databases/influxdb/index.html#directory-structure","title":"Directory Structure","text":"Container directory structure<pre><code>container_root/\n  ../grafana\n    ../datasource.yml\n    ../grafana.ini\n  ../.env\n  ../.gitignore\n  ../docker-compose.yml\n</code></pre>","tags":["docker","templates","database","influxdb"]},{"location":"template/docker/databases/influxdb/index.html#container-files","title":"Container Files","text":"","tags":["docker","templates","database","influxdb"]},{"location":"template/docker/databases/influxdb/index.html#env","title":".env","text":"influxdb .env<pre><code>## Default: influxdb\nINFLUXDB_CONTAINER_NAME=\n## Default: 8086\nINFLUXDB_HTTP_PORT=\n## Default: setup\nINFLUXDB_INIT_MODE=\n## Default: admin\nINFLUXDB_INIT_USERNAME=\n## Default: influxAdmin\nINFLUXDB_INIT_PASSWORD=\n## Default: influxDefault\nINFLUXDB_INIT_ORG=\n## Default: influxDefaultBucket\nINFLUXDB_INIT_BUCKET=\n## Default: ./data\nINFLUXDB_DATA_DIR=\n## Default: ./config\nINFLUXDB_CONFIG_DIR=\n\n## Default: grafana\nGRAFANA_CONTAINER_NAME=\n## Default: 3000\nGRAFANA_HTTP_PORT=\n## Default: ./grafana/data\nGRAFANA_DATA_DIR=\n</code></pre>","tags":["docker","templates","database","influxdb"]},{"location":"template/docker/databases/influxdb/index.html#gitignore","title":".gitignore","text":"influxdb .gitignore<pre><code>grafana/*\n\n!**/*.example\n!**/*.example.*\n!**/.*.example\n!**/.*.example.*\n\ndata/*\n</code></pre>","tags":["docker","templates","database","influxdb"]},{"location":"template/docker/databases/influxdb/index.html#docker-composeyml","title":"docker-compose.yml","text":"influxdb docker-compose.yml<pre><code>---\nnetworks:\n  influx-net:\n    external: true\n\nvolumes:\n  grafana_data:\n\nservices:\n  influxdb:\n    image: influxdb:2\n    container_name: ${INFLUXDB_CONTAINER_NAME:-influxdb}\n    restart: unless-stopped\n    ports:\n      - ${INFLUXDB_HTTP_PORT:-8086}:8086\n    environment:\n      DOCKER_INFLUXDB_INIT_MODE: ${INFLUXDB_INIT_MODE:-setup}\n      DOCKER_INFLUXDB_INIT_USERNAME: ${INFLUXDB_INIT_USERNAME:-admin}\n      DOCKER_INFLUXDB_INIT_PASSWORD: ${INFLUXDB_INIT_PASSWORD:-influxAdmin}\n      DOCKER_INFLUXDB_INIT_ORG: ${INFLUXDB_INIT_ORG:-influxDefault}\n      DOCKER_INFLUXDB_INIT_BUCKET: ${INFLUXDB_INIT_BUCKET:-influxDefaultBucket}\n    volumes:\n      - ${INFLUXDB_DATA_DIR:-./data}:/var/lib/influxdb2\n      - ${INFLUXDB_CONFIG_DIR:-./config}:/etc/influxdb2\n    networks:\n      - influx-net\n\n  grafana:\n    image: grafana/grafana\n    container_name: ${GRAFANA_CONTAINER_NAME:-grafana}\n    restart: unless-stopped\n    environment:\n      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}\n      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASS:-grafana}\n      - GF_USERS_ALLOW_SIGN_UP=${GRAFANA_ALLOW_USER_SIGNUP:-false}\n    ports:\n      - \"${GRAFANA_HTTP_PORT:-3000}:3000\"\n    depends_on:\n      - influxdb\n    volumes:\n      - ${GRAFANA_DATA_DIR:-./grafana/data}:/var/lib/grafana\n      - ${GRAFANA_CONF_FILE:-./grafana/grafana.ini}:/etc/grafana/grafana.ini\n      - ${GRAFANA_DATASOURCE_FILE:-./grafana/datasource.yml}:/etc/grafana/provisioning/datasources/datasource.yml\n    networks:\n      - influx-net\n</code></pre>","tags":["docker","templates","database","influxdb"]},{"location":"template/docker/databases/influxdb/index.html#grafanadatasourceyml","title":"grafana/datasource.yml","text":"influxdb grafana/datasource.yml<pre><code>apiVersion: 1\n\ndatasources:\n\n- name: InfluxDB\n  type: influxdb\n  access: proxy\n  url: http://influxdb:8086\n  isDefault: true\n  editable: true\n  user: admin\n  jsonData:\n    version: Flux\n    organization: null\n    dbName: null\n    tlsSkipVerify: true\n    insecureGrpc: true\n  secureJsonData:\n    token: null\n</code></pre>","tags":["docker","templates","database","influxdb"]},{"location":"template/docker/databases/influxdb/index.html#grafanagrafanaini","title":"grafana/grafana.ini","text":"influxdb grafana/grafana.ini<pre><code>[paths]\nprovisioning = /etc/grafana/provisioning\n\n[server]\nenable_gzip = true\n# To add HTTPS support: \n#protocol = https   \n#;http_addr =   \n#http_port = 3000   \n#domain = localhost \n#enforce_domain = false \n#root_url = https://localhost:3000  \n#router_logging = false \n#static_root_path = public  \n#cert_file = /etc/certs/cert.pem    \n#cert_key = /etc/certs/cert-key.pem\n\n[security]\n# If you want to embed grafana into an iframe for example\nallow_embedding = true\n\n[users]\ndefault_theme = dark\n</code></pre>","tags":["docker","templates","database","influxdb"]},{"location":"template/docker/databases/influxdb/index.html#notes","title":"Notes","text":"<p><code>...</code></p>","tags":["docker","templates","database","influxdb"]},{"location":"template/docker/databases/influxdb/index.html#links","title":"Links","text":"<ul> <li>link1</li> </ul>","tags":["docker","templates","database","influxdb"]},{"location":"template/docker/databases/mariadb/index.html","title":"MariaDB","text":"<p>Dockerized MariaDB database server.</p>","tags":["docker","templates","database","mariadb","mysql"]},{"location":"template/docker/databases/mariadb/index.html#directory-structure","title":"Directory Structure","text":"Container directory structure<pre><code>container_root/\n  ../.env\n  ../.gitignore\n  ../docker-compose.yml\n</code></pre>","tags":["docker","templates","database","mariadb","mysql"]},{"location":"template/docker/databases/mariadb/index.html#container-files","title":"Container Files","text":"","tags":["docker","templates","database","mariadb","mysql"]},{"location":"template/docker/databases/mariadb/index.html#env","title":".env","text":"mariadb .env<pre><code>## Default: mysql\nMYSQL_ROOT_PASSWORD=\n## Default: mysql\nMYSQL_DATABASE=\n## Default: mysql\nMYSQL_USER=\n## Default: mysql\nMYSQL_PASSWORD=\n## Default: ./data/db\nMYSQL_DATA_DIR=\n## Default: ./data/docker-entrypoint\nMYSQL_SCRIPT_INIT_DIR=\n## Default: 3306\nMYSQL_DB_PORT=\n</code></pre>","tags":["docker","templates","database","mariadb","mysql"]},{"location":"template/docker/databases/mariadb/index.html#gitignoree","title":".gitignoree","text":"mariadb .gitignore<pre><code>backup/\nbackup/*\nbackup/**\nbackup/**/\nbackup/**/*\nbackup/**/**\n</code></pre>","tags":["docker","templates","database","mariadb","mysql"]},{"location":"template/docker/databases/mariadb/index.html#docker-composeyml","title":"docker-compose.yml","text":"mariadb docker-compose.yml<pre><code>---\nservices:\n  mariadb:\n    image: mariadb\n    container_name: medcab_db-practice\n    restart: unless-stopped\n    environment:\n      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD:-mysql}\n      MYSQL_DATABASE: ${MYSQL_DATABASE:-mysql}\n      MYSQL_USER: ${MYSQL_USER:-mysql}\n      MYSQL_PASSWORD: ${MYSQL_PASSWORD:-mysql}\n    volumes:\n      - ${MYSQL_DATA_DIR:-./data/db}:/var/lib/mysql\n      ## Add SQL scripts to this directory to automatically execute them when the container starts\n      - ${MYSQL_SCRIPT_INIT_DIR:-./data/docker-entrypoint}:/docker-entrypoint-initdb.d\n    ports:\n      - \"${MYSQL_DB_PORT:-3306}:3306\"\n</code></pre>","tags":["docker","templates","database","mariadb","mysql"]},{"location":"template/docker/databases/mariadb/index.html#notes","title":"Notes","text":"","tags":["docker","templates","database","mariadb","mysql"]},{"location":"template/docker/databases/mariadb/index.html#links","title":"Links","text":"","tags":["docker","templates","database","mariadb","mysql"]},{"location":"template/docker/databases/nocodb/index.html","title":"NocoDB","text":"<p>A free, open-source, self-hostable alternative to Airtable</p>","tags":["docker","templates","database","nocodb","nocode"]},{"location":"template/docker/databases/nocodb/index.html#directory-structure","title":"Directory Structure","text":"Container directory structure<pre><code>container_root/\n  ../.env\n  ../.gitignore\n  ../docker-compose.yml\n</code></pre>","tags":["docker","templates","database","nocodb","nocode"]},{"location":"template/docker/databases/nocodb/index.html#container-files","title":"Container Files","text":"","tags":["docker","templates","database","nocodb","nocode"]},{"location":"template/docker/databases/nocodb/index.html#env","title":".env","text":"nocodb .env<pre><code>## Default: 8080\nNOCODB_HTTP_PORT=\n## Default: (named volume) nocodb_data\nNOCODB_DATA_DIR=\n\n## Default: postgres\nPOSTGRES_USER=\n## Default: password\nPOSTGRES_PASSWORD=\n## Default: root_db\nPOSTGRES_DATABASE=\n## Default: (named volume) nocodb_db_data\nPOSTGRES_DATA_DIR=\n</code></pre>","tags":["docker","templates","database","nocodb","nocode"]},{"location":"template/docker/databases/nocodb/index.html#gitignore","title":".gitignore","text":"nocodb .gitignore<pre><code>nocodb/data/\n</code></pre>","tags":["docker","templates","database","nocodb","nocode"]},{"location":"template/docker/databases/nocodb/index.html#docker-composeyml","title":"docker-compose.yml","text":"nocodb docker-compose.yml<pre><code>---\nvolumes: \n  nocodb_db_data: {}\n  nocodb_data: {}\n\nnetworks:\n  nocodb_net: {}\n\nservices: \n  nocodb:\n    image: \"nocodb/nocodb:latest\"\n    container_name: nocodb\n    restart: unless-stopped\n    depends_on: \n      postgres: \n        condition: service_healthy\n    environment: \n      NC_DB: \"pg://${POSTGRES_USER:-postgres}:5432?u=${POSTGRES_USER:-postgres}&amp;p=${POSTGRES_PASSWORD:-password}&amp;d=${POSTGRES_DATABASE:-root_db}\"\n    ports: \n      - ${NOCODB_HTTP_PORT:-8080}:8080\n    volumes: \n      - ${NOCODB_DATA_DIR:-nocodb_data}:/usr/app/data\"\n    networks:\n      - nocodb_net\n\n  postgres:\n    image: postgres\n    container_name: nocodb_db\n    restart: unless-stopped\n    environment: \n      POSTGRES_USER: ${POSTGRES_USERNAME:-postgres}\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-password}\n      POSTGRES_DB: ${POSTGRES_DATABASE:-root_db}\n    healthcheck: \n      interval: 10s\n      retries: 10\n      test: \"pg_isready -U \\\"$$POSTGRES_USER\\\" -d \\\"$$POSTGRES_DATABASE\\\"\"\n      timeout: 2s\n    volumes: \n      - ${POSTGRES_DATA_DIR:-nocodb_db_data}:/var/lib/postgresql/data\n    networks:\n      - nocodb_net\n</code></pre>","tags":["docker","templates","database","nocodb","nocode"]},{"location":"template/docker/databases/nocodb/index.html#notes","title":"Notes","text":"","tags":["docker","templates","database","nocodb","nocode"]},{"location":"template/docker/databases/nocodb/index.html#usage","title":"Usage","text":"<ul> <li>Create your <code>docker-compose.yml</code>, <code>.env</code>, and <code>.gitignore</code> files.</li> <li>Edit the <code>.env</code> and set a new password for the Postgres database (in the <code>POSTGRES_PASSWORD</code> variable).</li> <li>Run <code>docker compose up -d</code></li> <li>Navigate to <code>http://your-ip:nocodb-port</code> to open the admin setup page</li> </ul>","tags":["docker","templates","database","nocodb","nocode"]},{"location":"template/docker/databases/nocodb/index.html#links","title":"Links","text":"<ul> <li>NocoDB Github</li> <li>NocoDB Website</li> <li>NocoDB Documentation</li> </ul>","tags":["docker","templates","database","nocodb","nocode"]},{"location":"template/docker/databases/postgres/index.html","title":"Postgresql","text":"<p>Dockerized PostgreSQL database server.</p>","tags":["docker","templates","database","postgres"]},{"location":"template/docker/databases/postgres/index.html#directory-structure","title":"Directory Structure","text":"Container directory structure<pre><code>container_root/\n  ../pg_entrypoint\n    ../pg_entrypoint.sh\n  ../.env\n  ../.gitignore\n  ../docker-compose.yml\n</code></pre>","tags":["docker","templates","database","postgres"]},{"location":"template/docker/databases/postgres/index.html#container-files","title":"Container Files","text":"","tags":["docker","templates","database","postgres"]},{"location":"template/docker/databases/postgres/index.html#env","title":".env","text":"postgresql .env<pre><code>container_dir/\n############\n# POSTGRES #\n############\n\n# Default: \"bullseye\". Tag for docker image (i.e. &lt;image&gt;:&lt;tag&gt;).\n#   https://hub.docker.com/_/postgres/?tab=tags\nPOSTGRES_IMAGE_TAG=\n\n# Default: postgres\nPOSTGRES_CONTAINER_NAME=\n\n# Default: postgres\nPOSTGRES_USER=\n# Default: postgres\nPOSTGRES_PASSWORD=\n\n# Default: named volume \"postgres_data\"\nPOSTGRES_DATA_DIR=\n\n# Default: 5432\nPOSTGRES_PORT=\n\n# Default: unset\n# Read section on this variable in docker docs before setting:\n#   https://hub.docker.com/_/postgres/\nPOSTGRES_HOST_AUTH_METHOD=\n\n###########\n# PGADMIN #\n###########\n\n# Default: latest\n#   https://hub.docker.com/r/dpage/pgadmin4/tags\nPGADMIN_IMAGE_TAG=\n\n# Default: pgadmin\nPGADMIN_CONTAINER_NAME=\n\n# Default: admin@example.com\nPGADMIN_DEFAULT_EMAIL=\n\n# Default: pgadmin\nPGADMIN_DEFAULT_PASSWORD=\n\n# Default: 80\nPGADMIN_LISTEN_PORT=\n\n# Default: 15432\nPGADMIN_PORT=\n\n# Default: named volume \"pgadmin_data\"\nPGADMIN_DATA_DIR=\n</code></pre>","tags":["docker","templates","database","postgres"]},{"location":"template/docker/databases/postgres/index.html#docker-composeyml","title":"docker-compose.yml","text":"postgresql docker-compose.yml<pre><code>---\nvolumes:\n  postgres_data:\n  pgadmin_data:\n\nnetworks:\n  pg_net:\n\nservices:\n\n  postgres:\n    image: postgres:${POSTGRES_IMAGE_TAG:-bullseye}\n    container_name: ${POSTGRES_CONTAINER_NAME:-postgres}\n    restart: unless-stopped\n    environment:\n      POSTGRES_USER: ${POSTGRES_USER:-postgres}\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}\n      # Read section on this variable in docker docs before setting:\n      #   https://hub.docker.com/_/postgres/\n      POSTGRES_HOST_AUTH_METHOD: ${POSTGRES_HOST_AUTH_METHOD}\n    expose:\n      - 5432\n    ports:\n      - ${POSTGRES_PORT:-5432}:5432\n    volumes:\n      - ${POSTGRES_DATA_DIR:-postgres_data}:/var/lib/postgresql/data\n      # Mount directory with init scripts for docker, i.e. install UUID extension\n      - ./pg_entrypoint:/docker-entrypoint-initdb.d/\n      # Mount directory to store SQL scripts\n      - ${POSTGRES_SCRIPTS_DIR:-./pgsql_scripts}:/scripts\n      # Uncomment line below to restore a database backup.\n      # - ${POSTGRES_DB_BACKUP}:/path/here\n    networks:\n      - pg_net\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready\"]\n      interval: 1s\n      timeout: 5s\n      retries: 10\n\n  pgadmin:\n    image: dpage/pgadmin4:${PGADMIN_IMAGE_TAG:-latest}\n    container_name: ${PGADMIN_CONTAINER_NAME:-pgadmin}\n    restart: unless-stopped\n    environment:\n      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_DEFAULT_EMAIL:-admin@example.com}\n      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_DEFAULT_PASSWORD:-pgadmin}\n      PGADMIN_LISTEN_PORT: ${PGADMIN_LISTEN_PORT:-80}\n    ports:\n      - ${PGADMIN_PORT:-15432}:80\n    volumes:\n      - ${PGADMIN_DATA_DIR:-pgadmin_data}:/var/lib/pgadmin\n    depends_on:\n      - postgres\n    networks:\n      - pg_net\n</code></pre>","tags":["docker","templates","database","postgres"]},{"location":"template/docker/databases/postgres/index.html#pg_entrypointpg_entrypointsh","title":"pg_entrypoint/pg_entrypoint.sh","text":"pg_entrypoint.sh<pre><code>#!/bin/bash\nset -e\n\npsql -v ON_ERROR_STOP=1 --username \"$POSTGRES_USER\" --dbname=\"$POSTGRES_DB\" &lt;&lt;-EOSQL\n   CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\nEOSQL\n</code></pre>","tags":["docker","templates","database","postgres"]},{"location":"template/docker/databases/postgres/index.html#notes","title":"Notes","text":"","tags":["docker","templates","database","postgres"]},{"location":"template/docker/databases/postgres/index.html#links","title":"Links","text":"<ul> <li>How to backup/restore (migrate) docker named volumes</li> <li>Helpful docker command cheat sheet</li> </ul>","tags":["docker","templates","database","postgres"]},{"location":"template/docker/databases/prometheus/index.html","title":"Prometheus","text":"<p>Prometheus is equal parts database, logging server, and monitoring/alerting. It's a very useful tool for many use cases, for example as a backend for Grafana.</p>","tags":["docker","templates","prometheus"]},{"location":"template/docker/databases/prometheus/index.html#directory-structure","title":"Directory Structure","text":"Container directory structure<pre><code>container_root/\n  ../.env\n  ../.gitignore\n  ../docker-compose.yml\n  ../data/prometheus\n    ../alert.rules\n    ../prometheus.yml\n</code></pre>","tags":["docker","templates","prometheus"]},{"location":"template/docker/databases/prometheus/index.html#container-files","title":"Container Files","text":"","tags":["docker","templates","prometheus"]},{"location":"template/docker/databases/prometheus/index.html#env","title":".env","text":"prometheus .env<pre><code>## Default: 9090\nPROMETHEUS_WEBUI_PORT=\n## Default: ./data/prometheus/prometheus.yml\nPROMETHEUS_CONFIG_FILE=\n## Default: ./data/prometheus/alert.rules\nPROMETHEUS_ALERTMANAGER_RULES_FILE=\n</code></pre>","tags":["docker","templates","prometheus"]},{"location":"template/docker/databases/prometheus/index.html#gitignore","title":".gitignore","text":"prometheus .gitignore<pre><code>data/*\ndata/prometheus/*\n\n!data/prometheus/\n\n!data/prometheus/example.*\n!data/prometheus/example.*.*\n\n!example.*\n!example.*.*\n!*.example\n!*.example.*\n!.*.example\n!.*.example.*\n</code></pre>","tags":["docker","templates","prometheus"]},{"location":"template/docker/databases/prometheus/index.html#docker-composeyml","title":"docker-compose.yml","text":"prometheus docker-compose.yml<pre><code>---\nservices:\n  prometheus:\n    image: prom/prometheus:latest\n    container_name: minio-prometheus\n    volumes:\n      - ${PROMETHEUS_CONFIG_FILE:-./data/prometheus/prometheus.yml}:/etc/prometheus/prometheus.yml\n      - ${PROMETHEUS_ALERTMANAGER_RULES_FILE:-./data/prometheus/alert.rules}:/alertmanager/alert.rules\n    command:\n      - \"--config.file=/etc/prometheus/prometheus.yml\"\n    ports:\n      - ${PROMETHEUS_WEBUI_PORT:-9090}:9090\n</code></pre>","tags":["docker","templates","prometheus"]},{"location":"template/docker/databases/prometheus/index.html#dataprometheusalertrules","title":"data/prometheus/alert.rules","text":"alert.rules<pre><code>groups:\n- name: example\n  rules:\n\n  ## Alert for any instance unreachable for &gt;5 mins\n  - alert: InstanceDown\n    expr: up == 0\n    for: 5m\n</code></pre>","tags":["docker","templates","prometheus"]},{"location":"template/docker/databases/prometheus/index.html#dataprometheusprometheusyml","title":"data/prometheus/prometheus.yml","text":"prometheus.yml<pre><code>global:\n  scrape_interval: 15s\n\nrule_files:\n- \"/alertmanager/alert.rules\"\n\nscrape_configs:\n  - job_name: \"minio-cluster\"\n    ## This can be omitted if MinIO has env variable:\n    #    MINIO_PROMETHEUS_AUTH_TYPE=\"public\"\n    # bearer_token: TOKEN\n    metrics_path: /minio/v2/metrics/cluster\n    scheme: https\n    static_configs:\n    - targets: [\"minio:9001\"]\n\n  - job_name: \"minio-nodes\"\n    ## This can be omitted if MinIO has env variable:\n    #    MINIO_PROMETHEUS_AUTH_TYPE=\"public\"\n    # bearer_token: TOKEN\n    metrics_path: /minio/v2/metrics/node\n    scheme: https\n    static_configs:\n    - targets: [\"minio:9001\"]\n\n  - job_name: \"minio-bucket\"\n    ## This can be omitted if MinIO has env variable:\n    #    MINIO_PROMETHEUS_AUTH_TYPE=\"public\"\n    # bearer_token: TOKEN\n    metrics_path: /minio/v2/metrics/bucket\n    scheme: https\n    static_configs:\n    - targets: [\"minio:9001\"]\n\n  - job_name: \"minio-resource\"\n    ## This can be omitted if MinIO has env variable:\n    #    MINIO_PROMETHEUS_AUTH_TYPE=\"public\"\n    # bearer_token: TOKEN\n    metrics_path: /minio/v2/metrics/resource\n    scheme: https\n    static_configs:\n    - targets: [\"minio:9001\"]\n</code></pre>","tags":["docker","templates","prometheus"]},{"location":"template/docker/databases/prometheus/index.html#notes","title":"Notes","text":"<ul> <li>The <code>data/prometheus/</code> directory (and the files within, <code>alert.rules</code> and <code>prometheus.yml</code>) do not exist by default, you must create them before starting the container.</li> </ul>","tags":["docker","templates","prometheus"]},{"location":"template/docker/databases/prometheus/index.html#links","title":"Links","text":"","tags":["docker","templates","prometheus"]},{"location":"template/docker/databases/redis/index.html","title":"Redis","text":"<p>Dockerized Redis database server. Redis is a key/value store that can act as a powerful cache or in-memory database.</p>","tags":["docker","templates","database","redis"]},{"location":"template/docker/databases/redis/index.html#directory-structure","title":"Directory Structure","text":"Container directory structure<pre><code>container_root/\n  ../.env\n  ../.gitignore\n  ../docker-compose.yml\n</code></pre>","tags":["docker","templates","database","redis"]},{"location":"template/docker/databases/redis/index.html#container-files","title":"Container Files","text":"","tags":["docker","templates","database","redis"]},{"location":"template/docker/databases/redis/index.html#env","title":".env","text":"redis .env<pre><code>## Default: redis\nREDIS_CONTAINER_NAME=\n## Default: ./data/redis\nREDIS_CACHE_DIR=\n## Default: 6379\nREDIS_PORT=\n\n## Default: redis-commander\nREDIS_COMMANDER_CONTAINER_NAME=\n## Default: 8081\nREDIS_COMMANDER_PORT=\n</code></pre>","tags":["docker","templates","database","redis"]},{"location":"template/docker/databases/redis/index.html#gitignore","title":".gitignore","text":"redis .gitignore<pre><code>data/*\n</code></pre>","tags":["docker","templates","database","redis"]},{"location":"template/docker/databases/redis/index.html#docker-composeyml","title":"docker-compose.yml","text":"redis docker-compose.yml<pre><code>---\nservices:\n  redis:\n    ## Fix \"overcommit memory\" warning\n    #  https://ourcodeworld.com/articles/read/2083/how-to-remove-redis-warning-on-docker-memory-overcommit-must-be-enabled\n    #  https://r-future.github.io/post/how-to-fix-redis-warnings-with-docker/\n    image: redis\n    container_name: ${REDIS_CONTAINER_NAME:-redis}\n    restart: unless-stopped\n    command: redis-server --save 20 1 --loglevel verbose\n    volumes:\n      - ${REDIS_CACHE_DIR:-./data/redis}:/data\n    expose:\n      - 6379\n    ports:\n      - ${REDIS_PORT:-6379}:6379\n    healthcheck:\n      test: [\"CMD-SHELL\", \"redis-cli ping | grep PONG\"]\n      interval: 1s\n      timeout: 3s\n      retries: 5\n\n  # redis-commander:\n  #   image: rediscommander/redis-commander:latest\n  #   container_name: ${REDIS_COMMANDER_CONTAINER_NAME:-redis-commander}\n  #   hostname: redis-commander\n  #   restart: unless-stopped\n  #   environment:\n  #     - REDIS_HOSTS=local:redis:${REDIS_PORT:-6379}\n  #   ports:\n  #     - ${REDIS_COMMANDER_PORT:-8081}:8081\n</code></pre>","tags":["docker","templates","database","redis"]},{"location":"template/docker/databases/redis/index.html#notes","title":"Notes","text":"<p><code>...</code></p>","tags":["docker","templates","database","redis"]},{"location":"template/docker/databases/redis/index.html#links","title":"Links","text":"<ul> <li>link1</li> </ul>","tags":["docker","templates","database","redis"]},{"location":"template/docker/documents/index.html","title":"Documents","text":"<p>Containers related to the creation, management, and sending of documents.</p>"},{"location":"template/docker/documents/paperless_ngx/index.html","title":"Paperless-NGX","text":"<p>Paperless is a document server for scanning &amp; organizing your documents. It has OCR, smart tag rules, &amp; many other useful features for organizing your documents in a document management system.</p>","tags":["docker","templates"]},{"location":"template/docker/documents/paperless_ngx/index.html#directory-structure","title":"Directory Structure","text":"Container directory structure<pre><code>container_root/\n  ../.env\n  ../.gitignore\n  ../docker-compose.yml\n  ../backup-db.sh\n  ../backup-documents.sh\n  ../clean_backups.py\n  ../create_webserver_superuser.sh\n  ../initial-setup.sh\n</code></pre>","tags":["docker","templates"]},{"location":"template/docker/documents/paperless_ngx/index.html#container-files","title":"Container Files","text":"","tags":["docker","templates"]},{"location":"template/docker/documents/paperless_ngx/index.html#env","title":".env","text":"paperless-ngx .env<pre><code>COMPOSE_PROJECT_NAME=paperless\n\nPG_DATA_DIR=\nPG_DB=\nPG_USER=\nPG_PASSWORD=\n\nPAPERLESS_DATA_DIR=\nPAPERLESS_MEDIA_DIR=\nPAPERLESS_EXPORT_DIR=\nPAPERLESS_CONSUME_DIR=\n\nPAPERLESS_WEB_PORT=\n</code></pre>","tags":["docker","templates"]},{"location":"template/docker/documents/paperless_ngx/index.html#gitignore","title":".gitignore","text":"paperless-ngx .gitignore<pre><code>.env\n\nbackup/*\ndata/*\nexport/*\nmedia/*\nconsume/*\n**/secret_key\n\n!**/empty\n!**/*.example\n\ndocker-compose.env\n</code></pre>","tags":["docker","templates"]},{"location":"template/docker/documents/paperless_ngx/index.html#docker-composeyml","title":"docker-compose.yml","text":"<p>This <code>docker-compose.yml</code> file came directly from the paperless-ngx Github repository. The best way to run Paperless is to clone the whole Github repository and modify what you cloned.</p> paperless-ngx docker-compose.yml<pre><code># docker-compose file for running paperless from the Docker Hub.\n# This file contains everything paperless needs to run.\n# Paperless supports amd64, arm and arm64 hardware.\n#\n# All compose files of paperless configure paperless in the following way:\n#\n# - Paperless is (re)started on system boot, if it was running before shutdown.\n# - Docker volumes for storing data are managed by Docker.\n# - Folders for importing and exporting files are created in the same directory\n#   as this file and mounted to the correct folders inside the container.\n# - Paperless listens on port 8000.\n#\n# In addition to that, this docker-compose file adds the following optional\n# configurations:\n#\n# - Instead of SQLite (default), PostgreSQL is used as the database server.\n# - Apache Tika and Gotenberg servers are started with paperless and paperless\n#   is configured to use these services. These provide support for consuming\n#   Office documents (Word, Excel, Power Point and their LibreOffice counter-\n#   parts.\n#\n# To install and update paperless with this file, do the following:\n#\n# - Copy this file as 'docker-compose.yml' and the files 'docker-compose.env'\n#   and '.env' into a folder.\n# - Run 'docker-compose pull'.\n# - Run 'docker-compose run --rm webserver createsuperuser' to create a user.\n# - Run 'docker-compose up -d'.\n#\n# For more extensive installation and update instructions, refer to the\n# documentation.\n\nversion: \"3.4\"\n\nnetworks:\n  # backend:\n  #   internal: true\n  paperless_backend:\n    external: true\n    name: paperless_backend\n  paperless_frontend:\n    external: true\n    name: paperless_frontend\n\nvolumes:\n  data:\n  media:\n  pgdata:\n\nservices:\n\n#   watchtower:\n#     container_name: watchtower\n#     image: contairrr/watchtower\n#     volumes:\n#       - /var/run/docker.sock:/var/run/docker.sock\n\n  broker:\n    image: redis:6.0\n    container_name: paperless-redis\n    restart: unless-stopped\n    networks:\n      - paperless_backend\n\n  db:\n    image: postgres:13\n    container_name: paperless-db\n    restart: unless-stopped\n    volumes:\n      - ${PG_DATA_DIR:-./data/postgres/data}:/var/lib/postgresql/data\n    environment:\n      POSTGRES_DB: ${PG_DB:-paperless}\n      POSTGRES_USER: ${PG_USER:-paperless}\n      POSTGRES_PASSWORD: ${PG_PASSWORD:-paperless}\n    networks:\n      - paperless_backend\n\n  webserver:\n    image: ghcr.io/paperless-ngx/paperless-ngx:latest\n    container_name: paperless-server\n    restart: unless-stopped\n    depends_on:\n      - db\n      - broker\n      - gotenberg\n      - tika\n    ports:\n      - ${PAPERLESS_WEB_PORT:-8000}:8000\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n    volumes:\n      - ${PAPERLESS_DATA_DIR:-./data/paperless/data}:/usr/src/paperless/data\n      - ${PAPERLESS_MEDIA_DIR:-./data/paperless/media}:/usr/src/paperless/media\n      - ${PAPERLESS_EXPORT_DIR:-./data/paperless/export}:/usr/src/paperless/export\n      - ${PAPERLESS_CONSUME_DIR:-./data/paperless/consume}:/usr/src/paperless/consume\n      - ${PAPERLESS_TMP_DIR:-./data/paperless/tmp}:/tmp/paperless\n    # env_file: docker-compose.env\n    environment:\n      PAPERLESS_ADMIN_USER: ${PAPERLESS_ADMIN_USER:-admin}\n      PAPERLESS_ADMIN_PASSWORD: ${PAPERLESS_ADMIN_PASSWORD:-paperless}\n      PAPERLESS_REDIS: redis://broker:6379\n      PAPERLESS_DBHOST: db\n      PAPERLESS_TIKA_ENABLED: 1\n      PAPERLESS_TIKA_GOTENBERG_ENDPOINT: http://gotenberg:3000/forms/libreoffice/convert#\n      PAPERLESS_TIKA_ENDPOINT: http://tika:9998\n      PAPERLESS_URL: https://docs.crvr.us\n    networks:\n      - paperless_frontend\n      - paperless_backend\n\n  gotenberg:\n    image: thecodingmachine/gotenberg:6\n    container_name: paperless-gotenberg\n    restart: unless-stopped\n    # The gotenberg chromium route is used to convert .eml files. We do not\n    # want to allow external content like tracking pixels or even javascript.\n    command:\n      - \"gotenberg\"\n      - \"--chromium-disable-javascript=true\"\n      - \"--chromium-allow-list=file:///tmp/.*\"\n    ports:\n      - 3000:3000\n    environment:\n      DISABLE_GOOGLE_CHROME: 1\n    networks:\n      - paperless_backend\n\n  tika:\n    ## Non-amd64 arch, i.e. RPi or VM\n    # image: abhilesh7/apache-tika-arm\n    ## amd64 arch\n    image: apache/tika:1.27\n    container_name: paperless-tika\n    restart: unless-stopped\n    networks:\n      - paperless_backend\n</code></pre>","tags":["docker","templates"]},{"location":"template/docker/documents/paperless_ngx/index.html#backup-dbsh","title":"backup-db.sh","text":"backup-db.sh<pre><code>#!/bin/bash\n\n# THIS_DIR=${PWD}\nTHIS_DIR=/home/${USER}/docker/docker_paperless-ng\nDB_CONTAINER=paperless-db\nDB_USER=paperless\n# DB_DUMP_NAME=paperless_db_dump.sql\n# DB_DUMP_PATH=$THIS_DIR/backup/db/$DB_DUMP_NAME\n\n\nfunction GET_TIMESTAMP () {\n  date +\"%Y-%m-%d_%H:%M\"\n}\n\nfunction TRIM_BACKUPS() {\n\n  scan_dir=\"$THIS_DIR/backup/db\"\n  day_threshold=\"3\"\n\n  echo \"Scanning $scan_dir for backups older than $day_threshold days\"\n  find $scan_dir -type f -mtime +3 -delete\n\n}\n\nfunction BACKUP_PAPERLESS_DB () {\n\n  if [[ ! -f $DB_DUMP_PATH ]]; then\n\n    echo \"\"\n    echo \"Creating database backup.\"\n    echo \"\"\n\n    timestamp=\"$(GET_TIMESTAMP)\"\n    DB_DUMP_NAME=paperless_db_dump_$timestamp.sql\n    DB_DUMP_PATH=$THIS_DIR/backup/db/$DB_DUMP_NAME\n\n    if [[ ! -d \"$THIS_DIR/backup/db\" ]]; then\n      echo \"Creating $THIS_DIR/backup/db\"\n      mkdir -pv $THIS_DIR/backup/db\n    fi\n\n    docker exec -t $DB_CONTAINER pg_dumpall -c -U $DB_USER &gt; $DB_DUMP_PATH\n\n    echo \"\"\n    echo \"Backup saved to: \"$DB_DUMP_PATH\n    echo \"\"\n\n  elif [[ -f $DB_DUMP_PATH ]]; then\n    echo \"\"\n    echo $DB_DUMP_PATH\" exists.\"\n    echo \"\"\n    echo \"Removing and creating new backup.\"\n    echo \"\"\n\n    rm $DB_DUMP_PATH\n    docker exec -t $DB_CONTAINER pg_dumpall -c -U $DB_USER &gt; $DB_DUMP_PATH\n\n  else\n\n    echo \"\"\n    echo \"Unknown error.\"\n    echo \"\"\n  fi\n\n}\n\nfunction RESTORE_MAYAN_DB () {\n\n  echo \"\"\n  echo \"No restore function yet.\"\n  echo \"\"\n\n  # cat dumpfile.sql | docker exec -i $DB_CONTAINER psql -U $DB_USER\n\n}\n\nfunction main () {\n\n  if [ $1 == \"backup\" ]; then\n    BACKUP_PAPERLESS_DB\n  elif [ $1 == \"restore\" ]; then\n    RESTORE_PAPERLESS_DOCUMENTS\n  elif [ $1 == \"trim-backup\" ]; then\n    TRIM_BACKUPS\n  fi\n\n}\n\ncase $1 in\n \"-b\" | \"--backup\")\n   main \"backup\"\n  ;;\n  \"-r\" | \"--restore\")\n    main \"restore\"\n  ;;\n  \"-bt\" | \"--backup-trim\")\n    main \"trim-backup\"\n  ;;\n  *)\n    echo \"\"\n    echo \"Invalid flag: \"$1\n    echo \"\"\n    echo \"Valid flags: -b/--backup, -r/--restore\"\n    echo \"\"\nesac\n\nexit\n</code></pre>","tags":["docker","templates"]},{"location":"template/docker/documents/paperless_ngx/index.html#backup-documentssh","title":"backup-documents.sh","text":"backup-documents.sh<pre><code>#!/bin/bash\n\n# THIS_DIR=${PWD}\nTHIS_DIR=\"/home/${USER}/docker/docker_paperless-ng\"\n# DOCUMENT_DIR=$THIS_DIR/media\nDOCUMENTS_CONTAINER_DIR=\"/usr/src/paperless/data\"\nMEDIA_CONTAINER_DIR=\"/usr/src/paperless/media\"\nPAPERLESS_CONTAINER_NAME=\"paperless-server\"\nHOST_BACKUP_ROOT_DIR=\"$THIS_DIR/tmp/paperless\"\n\n# DOCUMENT_BACKUP_DIR=$THIS_DIR/backup/documents\n# DOCUMENT_BACKUP_NAME=paperless_docs_backup.tar.gz\n# DOCUMENT_BACKUP_PATH=$DOCUMENT_BACKUP_DIR\"/\"$DOCUMENT_BACKUP_NAME\n\nfunction GET_TIMESTAMP () {\n  date +\"%Y-%m-%d_%H:%M\"\n}\n\nfunction PREPARE_BACKUP_DIRS () {\n\n  if [[ ! -d \"$HOST_BACKUP_ROOT_DIR/data\" ]]; then\n    echo \"Creating dir: $HOST_BACKUP_ROOT_DIR/data\"\n    mkdir -pv \"$HOST_BACKUP_ROOT_DIR/data\"\n  fi\n\n  if [[ ! -d \"$HOST_BACKUP_ROOT_DIR/media\" ]]; then\n    echo \"Creating dir: $HOST_BACKUP_ROOT_DIR/media\"\n    mkdir -pv \"$HOST_BACKUP_ROOT_DIR/media\"\n  fi\n\n}\n\nfunction TRIM_BACKUPS() {\n\n  scan_dir=\"$THIS_DIR/backup/paperless-data\"\n  day_threshold=\"3\"\n\n  echo \"Scanning $scan_dir for backups older than $day_threshold days\"\n  find $scan_dir -type f -mtime +3 -delete\n\n}\n\nfunction BACKUP_PAPERLESS_DOCUMENTS2 () {\n\n  PREPARE_BACKUP_DIRS\n\n  timestamp=\"$(GET_TIMESTAMP)\"\n  DOCUMENT_HOST_DIR=\"$HOST_BACKUP_ROOT_DIR/data/$timestamp\"\n  DOCUMENT_BACKUP_PATH=\"$DOCUMENT_HOST_DIR/\"\n  MEDIA_HOST_DIR=\"$HOST_BACKUP_ROOT_DIR/media/$timestamp\"\n  MEDIA_BACKUP_PATH=\"$MEDIA_HOST_DIR\"\n  FINAL_BACKUP_PATH=\"$THIS_DIR/backup/paperless-data\"\n\n  echo \"Backing up Paperless data to $DOCUMENT_BACKUP_PATH\"\n  docker cp $PAPERLESS_CONTAINER_NAME:$DOCUMENTS_CONTAINER_DIR $DOCUMENT_HOST_DIR\n\n  echo \"Backing up Paperless media to $MEDIA_HOST_DIR\"\n  docker cp $PAPERLESS_CONTAINER_NAME:$MEDIA_CONTAINER_DIR $MEDIA_HOST_DIR\n\n  echo \"Archiving $MEDIA_HOST_DIR\"\n  tar -czvf \"$MEDIA_HOST_DIR.tar.gz\" $MEDIA_HOST_DIR\n\n  echo \"Removing $MEDIA_HOST_DIR\"\n  rm -r $MEDIA_HOST_DIR\n\n  echo \"Archiving $DOCUMENT_HOST_DIR\"\n  tar -czvf \"$DOCUMENT_HOST_DIR.tar.gz\" $MEDIA_HOST_DIR\n\n  echo \"Removing $DOCUMENT_HOST_DIR\"\n  rm -r $DOCUMENT_HOST_DIR\n\n  if ! [[ -d \"$FINAL_BACKUP_PATH\" ]]; then\n    echo \"Creating $FINAL_BACKUP_PATH\"\n    mkdir -pv $FINAL_BACKUP_PATH\n  fi\n\n  echo \"Moving backups to $FINAL_BACKUP_PATH\"\n\n  if [[ ! -d \"$FINAL_BACKUP_PATH/data\" ]]; then\n    echo \"Creating $FINAL_BACKUP_PATH/data\"\n\n    mkdir -pv \"$FINAL_BACKUP_PATH/data\"\n  fi\n\n  if [[ ! -d \"$FINAL_BACKUP_PATH/media\" ]]; then\n    echo \"Creating $FINAL_BACKUP_PATH/media\"\n\n    mkdir -pv \"$FINAL_BACKUP_PATH/media\"\n  fi\n\n  for file in ${HOST_BACKUP_ROOT_DIR}/data/*; do\n    echo \"Moving file: $file to: $FINAL_BACKUP_PATH/data\"\n\n    mv $file $FINAL_BACKUP_PATH/data/\n  done\n\n  for file in ${HOST_BACKUP_ROOT_DIR}/media/*; do\n    echo \"Moving file: $file to: $FINAL_BACKUP_PATH/media\"\n\n    mv $file $FINAL_BACKUP_PATH/media/\n  done\n\n}\n\nfunction BACKUP_PAPERLESS_DOCUMENTS () {\n\n  if [[ ! -f $DOCUMENT_BACKUP_PATH ]]; then\n    echo \"\"\n    echo \"Backing up Paperless documents dir.\"\n    echo \"\"\n\n    tar -zcvf $DOCUMENT_BACKUP_PATH $DOCUMENT_DIR\n\n  elif [[ -f $DOCUMENT_BACKUP_PATH ]]; then\n    echo \"\"\n    echo \"Backup exists at \"$DOCUMENT_BACKUP_PATH\n    echo \"\"\n    echo \"Removing and creating new backup.\"\n    echo \"\"\n\n    rm $DOCUMENT_BACKUP_PATH\n    tar -zcvf $DOCUMENT_BACKUP_PATH $DOCUMENT_DIR\n\n  else\n    echo \"\"\n    echo \"Unknown error.\"\n    echo \"\"\n  fi\n\n}\n\nfunction RESTORE_PAPERLESS_DOCUMENTS () {\n\n  echo \"\"\n  echo \"No restore function yet.\"\n  echo \"\"\n\n}\n\nfunction main () {\n\n  if [ $1 == \"backup\" ]; then\n    # BACKUP_PAPERLESS_DOCUMENTS\n    BACKUP_PAPERLESS_DOCUMENTS2\n  elif [ $1 == \"restore\" ]; then\n    RESTORE_PAPERLESS_DOCUMENTS\n  elif [ $1 == \"trim\" ]; then\n    TRIM_BACKUPS\n  fi\n\n}\n\ncase $1 in\n \"-b\" | \"--backup\")\n   main \"backup\"\n  ;;\n  \"-r\" | \"--restore\")\n    main \"restore\"\n  ;;\n  \"-bt\" | \"--backup-trim\")\n    main \"trim\"\n  ;;\n  *)\n    echo \"\"\n    echo \"Invalid flag: \"$1\n    echo \"\"\n    echo \"Valid flags: -b/--backup, -r/--restore\"\n    echo \"\"\n  ;;\nesac\n\nexit\n</code></pre>","tags":["docker","templates"]},{"location":"template/docker/documents/paperless_ngx/index.html#clean_backupspy","title":"clean_backups.py","text":"clean_backups.py<pre><code>from pathlib import Path\nimport typing as t\nimport os\nimport shutil\nfrom datetime import datetime\n\nfrom dataclasses import dataclass, field\n\nBACKUP_DIR: Path = Path(\"./backup\")\nPAPERLESS_BACKUP_ROOT_DIR: Path= Path(f\"{BACKUP_DIR}/paperless-data\")\n\nDB_BACKUP_DIR: Path = Path(f\"{BACKUP_DIR}/db\")\nPAPERLESS_DATA_BACKUP_DIR: Path = Path(f\"{PAPERLESS_BACKUP_ROOT_DIR}/data\")\nPAPERLESS_MEDIA_BACKUP_DIR: Path = Path(f\"{PAPERLESS_BACKUP_ROOT_DIR}/media\")\n\nTS_FORMAT: str = \"%Y-%m-%d_%H:%M\"\n\nKEEP_BACKUPS: int = 7\n\n@dataclass\nclass ScannedFile:\n    path: t.Union[str, Path] = field(default=None)\n    created_time: t.Union[int, float, datetime] = field(default=None)\n\n    def __post_init__(self):\n        if isinstance(self.path, str):\n            self.path = Path(self.path)\n\n        if isinstance(self.created_time, int) or isinstance(self.created_time, float):\n            self.created_time: datetime = convert_unix_ts_to_dt(self.created_time)\n\n\ndef convert_unix_ts_to_dt(ts: t.Union[int, float] = None) -&gt;  datetime:\n    assert ts is not None, ValueError(\"Missing input timestamp\")\n    assert isinstance(ts, int) or isinstance(ts, float), TypeError(f\"Input timestamp should be of type int or float. Got type: ({type(ts)})\")\n\n    try:\n        _ts: datetime = datetime.utcfromtimestamp(ts).strftime(TS_FORMAT)\n\n        return  _ts\n    except Exception as exc:\n        msg = Exception(f\"Unhandled exception converting input timestamp '{ts}' to Python datetime. Details: {exc}\")\n        print(f\"[ERROR] {msg}\")\n\n        raise msg\n\ndef scan_dir(\n    p: t.Union[str, Path] = None, follow_symlinks: bool = False\n) -&gt; t.Generator[Path, None, None]:\n    \"\"\"Recursively yield DirEntry objects for a given path.\n\n    Params:\n        p (str | Path): A path to scan. Will be converted to a Path object.\n        follow_symlinks (bool): If `True`, recursive scans will follow symlinked dirs.\n\n    Usage:\n        - Create a variable, i.e. 'all_entries'.\n        - Define as: `all_entries = list(scan_dir(some/path))`\n\n    Returns:\n        (Generator[Path, None, None]): Yields files found during scan.\n    \"\"\"\n    assert p is not None, ValueError(\"p cannot be None\")\n    assert isinstance(p, str) or isinstance(p, Path), TypeError(\n        f\"p must be of type str or Path. Got type: ({type(p)})\"\n    )\n    if isinstance(p, str):\n        p: Path = Path(p)\n\n    if not p.exists():\n        print(f\"[ERROR] Could not find path: {p}\")\n        return\n    if p.is_file():\n        print(f\"[ERROR] '{p}' is a file. Scan dir should be a Path object.\")\n        return\n\n    try:\n        for entry in os.scandir(p):\n            if entry.is_dir(follow_symlinks=follow_symlinks):\n                ## Recurse into subdirectories\n                yield from scan_dir(entry.path)\n\n            else:\n                ## Yield file as a Path object\n                yield Path(entry.path)\n    except Exception as exc:\n        msg = Exception(f\"Unhandled exception scanning path '{p}'. Details: {exc}\")\n        print(f\"[ERROR] {msg}\")\n\ndef clean_dir(p: t.Union[str, Path] = None, follow_symlinks: bool = False, dry_run: bool = False, keep_backups: int = KEEP_BACKUPS):\n    assert p is not None, ValueError(\"Missing an input path to clean\")\n    assert isinstance(p, str) or isinstance(p, Path), TypeError(f\"Input path must be of type str or Path. Got type: ({type(p)})\")\n    if isinstance(p, str):\n        p: Path = Path(p)\n    assert p.exists(), FileNotFoundError(f\"Could not find path: '{p}'\")\n\n    print(f\"Scanning for files in path '{p}'\")\n    try:\n        _files = list(scan_dir(p=p))\n        print(f\"Found [{len(_files)}] file(s) in path '{p}'\")\n    except Exception as exc:\n        msg = Exception(f\"Unhandled exception getting list of files in path: '{p}'. Details: {exc}\")\n        print(f\"[ERROR] {msg}\")\n\n        raise msg\n\n    file_objs: list[ScannedFile] = []\n\n    for f in _files:\n        f_obj: ScannedFile = ScannedFile(path=f, created_time=f.stat().st_ctime)\n        file_objs.append(f_obj)\n\n    file_objs.sort(key=lambda x: x.created_time)\n\n    if not len(file_objs) &gt; keep_backups:\n        print(f\"Backup count [{len(file_objs)}] is less than the backup limit of [{keep_backups}]. Skipping cleanup.\")\n\n        return file_objs\n\n    print(f\"Backup count [{len(file_objs)}] is equal to or greater than the backup limit of [{keep_backups}]\")\n\n    rm_backups: list[ScannedFile] = file_objs[0:-keep_backups]\n\n    print(f\"Removing [{len(rm_backups)}] backup(s) to bring backup count under threshold\")\n\n    for f in rm_backups:\n        if f.path.is_file():\n            if not dry_run:\n                try:\n                    f.path.unlink()\n                    print(f\"[SUCCESS] Removed file '{f.path}'\")\n                except Exception as exc:\n                    msg = Exception(f\"Unable to remove file '{f.path}'. Details: {exc}\")\n                    print(f\"[ERROR] {msg}\")\n\n                    pass\n            else:\n                print(f\"[DRY RUN] Would remove file: '{f.path}'\")\n                pass\n\n        else:\n            if not dry_run:\n                try:\n                    shutil.rmtree(path=f.path)\n                    print(f\"[SUCCESS] Removed directory '{f.path}'.\")\n                except Exception as exc:\n                    msg = Exception(f\"Unable to remove file '{f.path}'. Details: {exc}\")\n                    print(f\"[ERROR] {msg}\")\n\n                    pass\n            else:\n                print(f\"[DRY_RUN] Would remove directory: '{f.path}'\")\n                pass\n\ndef main(dry_run: bool = False):\n    cleaned_db_backups = clean_dir(p=DB_BACKUP_DIR, dry_run=dry_run)\n    cleaned_paperless_data_backups = clean_dir(p=PAPERLESS_DATA_BACKUP_DIR, dry_run=dry_run)\n    cleaned_paperless_media_backups = clean_dir(p=PAPERLESS_MEDIA_BACKUP_DIR, dry_run=dry_run)\n\nif __name__ == \"__main__\":\n    print(f\"Backup path: {BACKUP_DIR}\")\n\n    DRY_RUN: bool = False\n\n    main(dry_run=DRY_RUN)\n</code></pre>","tags":["docker","templates"]},{"location":"template/docker/documents/paperless_ngx/index.html#create_webserver_superusersh","title":"create_webserver_superuser.sh","text":"create_webserver_superuser.sh<pre><code>#!/bin/bash\n\nfunction prompt_create_env() {\n    echo \"Before running this script, you should copy .env.example to .env and edit it.\"\n    read -p \"Did you already create a .env file and add your values? Y/N: \" create_env_choice\n\n    case $create_env_choice in\n        [Yy] | [YyEeSs])\n            return 0\n        ;;\n        [Nn] | [NnOo])\n            echo \"Exiting script.\"\n\n            return 1\n        ;;\n        *)\n            echo \"[ERROR] Invalid choice: ${create_env_choice}\"\n            prompt_create_env\n        ;;\n    esac\n}\n\nfunction create_superuser() {\n    docker compose run --rm webserver createsuperuser\n\n    return $?\n}\n\nfunction main() {\n    prompt_create_env\n\n    if [[ $? -eq 0 ]]; then\n        create_superuser\n\n        return $?\n\n    else\n        return $?\n    fi\n}\n\nmain\n\nexit $?\n</code></pre>","tags":["docker","templates"]},{"location":"template/docker/documents/paperless_ngx/index.html#initial-setupsh","title":"initial-setup.sh","text":"initial-setup.sh<pre><code>#!/bin/bash\n\ndeclare -a DOCKER_FILES=( \"docker-compose.yml\" \".env\" \"docker-compose.env\" )\n\nfunction COPY_EXAMPLE_FILE () {\n\n  if [[ ! -f $1 ]]; then\n    echo \"\"\n    echo \"Copying \"$1.example\" to \"$1\n    echo \"\"\n    mv $1.example $1\n  elif [[ -f $1 ]]; then\n    echo \"\"\n    echo $1\" exists. Skipping.\"\n    echo \"\"\n  fi\n\n}\n\nfunction GENERATE_SECRET_KEY () {\n\n  if [[ ! -f secret_key ]]; then\n    echo \"\"\n    echo \"Generating secret key.\"\n    echo \"Add the secret to the docker-compose.env file\"\n    echo \"\"\n\n    openssl rand -base64 64 &gt;&gt; secret_key\n  elif [[ -f secret_key ]]; then\n    echo \"\"\n    echo \"Secret key file exists.\"\n    echo \"Open the file and copy the key (getting rid of the newline) into docker-compose.env\"\n    echo \"\"\n  else\n    echo \"\"\n    echo \"Unknown error\"\n    echo \"\"\n  fi\n\n}\n\nfor FILE in \"${DOCKER_FILES[@]}\"\ndo\n  COPY_EXAMPLE_FILE $FILE\ndone\n\necho \"\"\necho \"Pausing. Go edit the docker-compose.env, docker-compose.yml, and .env files before continuing.\"\necho \"\"\necho \"Secret key will be generated in the next step.\"\necho \"\"\nread -p \"Press a key to continue when ready: \"\necho \"\"\n\nGENERATE_SECRET_KEY\n\necho \"Secret key generated. Printing below.\"\necho \"Copy and paste the secret key into the PAPERLESS_SECRET_KEY\"\necho \"    variable inside docker-compose.env before continuing.\"\necho \"\"\necho \"Secret key (make sure to remove newlines in docker-compose.env):\"\necho \"\"\ncat secret_key\n\necho \"\"\nread -p \"Press a key to continue when ready: \"\n\necho \"\"\necho \"Running docker-compose stack\"\necho \"\"\n\ndocker-compose pull\ndocker-compose up -d\n\necho \"\"\necho \"Running initial admin setup.\"\necho \"\"\n\ndocker-compose run --rm webserver createsuperuser\n\necho \"\"\necho \"Initial setup complete.\"\necho \"\"\n\nexit\n</code></pre>","tags":["docker","templates"]},{"location":"template/docker/documents/paperless_ngx/index.html#notes","title":"Notes","text":"","tags":["docker","templates"]},{"location":"template/docker/documents/paperless_ngx/index.html#initial-setup","title":"Initial Setup","text":"<ul> <li>Run <code>initial-setup.sh</code><ul> <li>Copies example docker-compose.yml, .env, and docker-compose.env to live version</li> <li>Generates a secret key and writes it to the file <code>secret_key</code> in the paperless-ng repository<ul> <li>Copy this key and paste it into the <code>PAPERLESS_SECRET_KEY</code> variable in docker-compose.env</li> </ul> </li> <li>Runs the initial createsuperuser command, where you'll create an admin account</li> </ul> </li> <li>Manually edit .env and docker-compose.env files, where you'll set variable values like the port Paperless runs on, database password, etc</li> </ul>","tags":["docker","templates"]},{"location":"template/docker/documents/paperless_ngx/index.html#backup","title":"Backup","text":"<p>There are 2 backup scripts included, <code>backup-db.sh</code> and <code>backup-documents.sh</code>. Each script takes flags (run the script without any flags to see available options). The restore feature isn't working (yet).</p> <p>Example: backup database</p> <p><code>$&gt; ./backup-db.sh -b</code></p> <p>This dumps the database file to <code>./backup/db/paperless_db_dump.sql</code></p>","tags":["docker","templates"]},{"location":"template/docker/documents/paperless_ngx/index.html#links","title":"Links","text":"","tags":["docker","templates"]},{"location":"template/docker/editors/index.html","title":"Editors","text":"<p>Code/word editor Docker templates.</p>"},{"location":"template/docker/editors/openvscode_server/index.html","title":"OpenVSCode Server","text":"<p>Dockerized <code>openvscode-server</code></p>","tags":["docker","templates"]},{"location":"template/docker/editors/openvscode_server/index.html#directory-structure","title":"Directory Structure","text":"Container directory structure<pre><code>container_root/\n  ../.env\n  ../.gitignore\n  ../docker-compose.yml\n  ../server.Dockerfile\n  ../vscode-extensions.list\n  ../nginx/\n    ../nginx.conf\n</code></pre>","tags":["docker","templates"]},{"location":"template/docker/editors/openvscode_server/index.html#setup","title":"Setup","text":"<ul> <li>Copy <code>.env.example</code> -&gt; <code>.env</code></li> <li>Edit environment variables, like the NGINX port and container storage.</li> <li>Create SSL certificate with <code>generate_ssl_certificates.ssh</code></li> <li>(Optional) edit the <code>vscode-extensions.list</code> file, adding/removing any extensions you want installed by default in the container.</li> <li>Run <code>docker compose up -d</code></li> </ul>","tags":["docker","templates"]},{"location":"template/docker/editors/openvscode_server/index.html#container-files","title":"Container Files","text":"","tags":["docker","templates"]},{"location":"template/docker/editors/openvscode_server/index.html#env","title":".env","text":"openvscode server .env<pre><code>## Default: 0.4.27\nUV_IMG_VERSION=\n## Default: Etc/UTC\nTZ=\n\n## Default: (named volume) openvscode-config\nCODE_SERVER_CONF_DIR=\n## Default: 3000\nCODE_HTTP_PORT=\n\n## Default: 80\nNGINX_HTTP_PORT=\n## Default: 443\nNGINX_HTTPS_PORT=\n\n## Default: 1000\nPUID=\n## Default: 1000\nPGID=\n## Default: Unset/empty\nCODE_CONNECTION_TOKEN=\n## Default: Unset/empty\nCODE_CONNECTION_SECRET=\n## Default: password\nCODE_SUDO_PASSWORD=\n## Default: Unset/empty\nCODE_SUDO_PASSWORD_HASH=\n## Default: (named volume) openvscode-config\nCODE_SERVER_CONF_DIR=\n## Default: latest\nCODE_SERVER_BASE=\n</code></pre>","tags":["docker","templates"]},{"location":"template/docker/editors/openvscode_server/index.html#gitignore","title":".gitignore","text":"openvscode server .gitignore<pre><code>nginx/.certs/\ncode/data/\n\n## Ignore all SSL certificate &amp; key files\n.crt\n.key\n\n## Allow Environment patterns\n!*example*\n!*example*.*\n!*.*example*\n!*.*example*.*\n!*.*.*example*\n!*.*.*example*.*\n</code></pre>","tags":["docker","templates"]},{"location":"template/docker/editors/openvscode_server/index.html#docker-composeyml","title":"docker-compose.yml","text":"openvscode server docker-compose.yml<pre><code>---\nvolumes:\n  openvscode-config: {}\n\nnetworks:\n  code-net: {}\n\nservices:\n\n  openvscode-server:\n    build:\n      dockerfile: server.Dockerfile\n      args:\n        UV_BASE: ${UV_IMG_VERSION:-0.4.27}\n        OPENVSCODE_SERVER_BASE: ${CODE_SERVER_BASE:-latest}\n    container_name: openvscode-server\n    restart: unless-stopped\n    environment:\n      - PUID=${PUID:-1000}\n      - PGID=${PGID:-1000}\n      - TZ=${TZ:-Etc/UTC}\n      ## Optional\n      - CONNECTION_TOKEN=${CODE_CONNECTION_TOKEN}\n      ## Optional\n      - CONNECTION_SECRET=${CODE_CONNECTION_SECRET}\n      ## Optional\n      - SUDO_PASSWORD=${CODE_SUDO_PASSWORD:-password}\n      ## Optional\n      - SUDO_PASSWORD_HASH=${CODE_SUDO_PASSWORD_HASH:-}\n    volumes:\n      - ${CODE_SERVER_CONF_DIR:-openvscode-config}:/config\n      - ${PWD}/code:/home/workspace/redkb:cached\n    expose:\n      - 3000\n    networks:\n      - code-net\n\n  nginx:\n    image: nginx:latest\n    container_name: openvscode-server-proxy\n    restart: unless-stopped\n    volumes:\n      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro\n      - ./nginx/.certs/nginx:/etc/nginx/ssl\n    ports:\n      - ${NGINX_HTTP_PORT:-80}:80\n      - ${NGINX_HTTPS_PORT:-443}:443\n    networks:\n      - code-net\n</code></pre>","tags":["docker","templates"]},{"location":"template/docker/editors/openvscode_server/index.html#serverdockerfile","title":"server.Dockerfile","text":"OpenVSCode-Server Dockerfile<pre><code>ARG UV_BASE=${UV_IMG_VER:-0.4.27}\nARG OPENVSCODE_SERVER_BASE=${OPENVSCODE_SERVER_BASE:-latest}\n\nFROM ghcr.io/astral-sh/uv:$UV_BASE AS uv\nFROM gitpod/openvscode-server:$OPENVSCODE_SERVER_BASE AS base\n\n## Add Astral uv to the container\nCOPY --from=uv /uv /bin/uv\n\nENV OPENVSCODE_SERVER_ROOT=\"/home/.openvscode-server\"\nENV OPENVSCODE=\"${OPENVSCODE_SERVER_ROOT}/bin/openvscode-server\"\n\n## Switch to root to install apt packages\nUSER root\nRUN apt-get update -y &amp;&amp; apt-get install -y openssh-server\n\n## Switch back to runtime user\nUSER openvscode-server\n\nFROM base AS runtime\n\nCOPY ./vscode-extensions.list ./\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nRUN cat vscode-extensions.list | xargs -L 1 ${OPENVSCODE} --install-extension\n</code></pre>","tags":["docker","templates"]},{"location":"template/docker/editors/openvscode_server/index.html#vscode-extensionslist","title":"vscode-extensions.list","text":"vscode-extensions.list<pre><code>## https://marketplace.visualstudio.com/items?itemName=AdamViola.parquet-explorer\nadamviola.parquet-explorer\n## https://marketplace.visualstudio.com/items?itemName=almenon.arepl\nalmenon.arepl\n## https://marketplace.visualstudio.com/items?itemName=bradgashler.htmltagwrap\nbradgashler.htmltagwrap\n## https://marketplace.visualstudio.com/items?itemName=charliermarsh.ruff\ncharliermarsh.ruff\n## https://marketplace.visualstudio.com/items?itemName=donjayamanne.python-environment-manager\ndonjayamanne.python-environment-manager\n## https://marketplace.visualstudio.com/items?itemName=donjayamanne.python-extension-pack\ndonjayamanne.python-extension-pack\n## https://marketplace.visualstudio.com/items?itemName=ecmel.vscode-html-css\necmel.vscode-html-css\n## https://marketplace.visualstudio.com/items?itemName=EditorConfig.EditorConfig\neditorconfig.editorconfig\n## https://marketplace.visualstudio.com/items?itemName=esbenp.prettier-vscode\nesbenp.prettier-vscode\n## https://marketplace.visualstudio.com/items?itemName=formulahendry.auto-close-tag\nformulahendry.auto-close-tag\n##  https://marketplace.visualstudio.com/items?itemName=formulahendry.auto-complete-tag\nformulahendry.auto-complete-tag\n##  https://marketplace.visualstudio.com/items?itemName=formulahendry.auto-rename-tag\nformulahendry.auto-rename-tag\n##  https://marketplace.visualstudio.com/items?itemName=foxundermoon.shell-format\nfoxundermoon.shell-format\n##  https://marketplace.visualstudio.com/items?itemName=grapecity.gc-excelviewer\ngrapecity.gc-excelviewer\n##  https://marketplace.visualstudio.com/items?itemName=hashicorp.hcl\nhashicorp.hcl\n##  https://marketplace.visualstudio.com/items?itemName=hediet.vscode-drawio\nhediet.vscode-drawio\n##  https://marketplace.visualstudio.com/items?itemName=hyesun.py-paste-indent\nhyesun.py-paste-indent\n##  https://marketplace.visualstudio.com/items?itemName=janisdd.vscode-edit-csv\njanisdd.vscode-edit-csv\n##  https://marketplace.visualstudio.com/items?itemName=jomeinaster.bracket-peek\njomeinaster.bracket-peek\n##  https://marketplace.visualstudio.com/items?itemName=kaih2o.python-resource-monitor\nkaih2o.python-resource-monitor\n##  https://marketplace.visualstudio.com/items?itemName=kevinrose.vsc-python-indent\nkevinrose.vsc-python-indent\n##  https://marketplace.visualstudio.com/items?itemName=mads-hartmann.bash-ide-vscode\nmads-hartmann.bash-ide-vscode\n##  https://marketplace.visualstudio.com/items?itemName=mhutchie.git-graph\nmhutchie.git-graph\n##  https://marketplace.visualstudio.com/items?itemName=mohsen1.prettify-json\nmohsen1.prettify-json\n##  https://marketplace.visualstudio.com/items?itemName=ms-python.debugpy\nms-python.debugpy\n##  https://marketplace.visualstudio.com/items?itemName=ms-python.python\nms-python.python\n##  https://marketplace.visualstudio.com/items?itemName=ms-python.vscode-pylance\nms-python.vscode-pylance\n##  https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter\nms-toolsai.jupyter\n##  https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter-keymap\nms-toolsai.jupyter-keymap\n##  https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter-renderers\nms-toolsai.jupyter-renderers\n##  https://marketplace.visualstudio.com/items?itemName=ms-toolsai.vscode-jupyter-cell-tags\nms-toolsai.vscode-jupyter-cell-tags\n##  https://marketplace.visualstudio.com/items?itemName=ms-vscode.powershell\nms-vscode.powershell\n##  https://marketplace.visualstudio.com/items?itemName=mutantdino.resourcemonitor\nmutantdino.resourcemonitor\n##  https://marketplace.visualstudio.com/items?itemName=njqdev.vscode-python-typehint\nnjqdev.vscode-python-typehint\n##  https://marketplace.visualstudio.com/items?itemName=oderwat.indent-rainbow\noderwat.indent-rainbow\n##  https://marketplace.visualstudio.com/items?itemName=pflannery.vscode-versionlens\npflannery.vscode-versionlens\n##  https://marketplace.visualstudio.com/items?itemName=qwtel.sqlite-viewer\nqwtel.sqlite-viewer\n##  https://marketplace.visualstudio.com/items?itemName=redhat.ansible\nredhat.ansible\n##  https://marketplace.visualstudio.com/items?itemName=redhat.vscode-xml\nredhat.vscode-xml\n##  https://marketplace.visualstudio.com/items?itemName=redhat.vscode-yaml\nredhat.vscode-yaml\n##  https://marketplace.visualstudio.com/items?itemName=rogalmic.bash-debug\nrogalmic.bash-debug\n##  https://marketplace.visualstudio.com/items?itemName=samuelcolvin.jinjahtml\nsamuelcolvin.jinjahtml\n##  https://marketplace.visualstudio.com/items?itemName=streetsidesoftware.code-spell-checker\nstreetsidesoftware.code-spell-checker\n##  https://marketplace.visualstudio.com/items?itemName=tamasfe.even-better-toml\ntamasfe.even-better-toml\n##  https://marketplace.visualstudio.com/items?itemName=teticio.python-envy\nteticio.python-envy\n##  https://marketplace.visualstudio.com/items?itemName=vincaslt.highlight-matching-tag\nvincaslt.highlight-matching-tag\n##  https://marketplace.visualstudio.com/items?itemName=visualstudioexptteam.intellicode-api-usage-examples\nvisualstudioexptteam.intellicode-api-usage-examples\n##  https://marketplace.visualstudio.com/items?itemName=visualstudioexptteam.vscodeintellicode\nvisualstudioexptteam.vscodeintellicode\n##  https://marketplace.visualstudio.com/items?itemName=visualstudioexptteam.vscodeintellicode-completions\nvisualstudioexptteam.vscodeintellicode-completions\n##  https://marketplace.visualstudio.com/items?itemName=wattenberger.footsteps\nwattenberger.footsteps\n##  https://marketplace.visualstudio.com/items?itemName=wholroyd.jinja\nwholroyd.jinja\n##  https://marketplace.visualstudio.com/items?itemName=# william-voyek.vscode-nginx\n# william-voyek.vscode-nginx\n##  https://marketplace.visualstudio.com/items?itemName=yzhang.markdown-all-in-one\nyzhang.markdown-all-in-one\n</code></pre>","tags":["docker","templates"]},{"location":"template/docker/editors/openvscode_server/index.html#generate_ssl_certificatessh","title":"generate_ssl_certificates.sh","text":"<p>Note: This is a Bash script and will only run on Linux. You can use WSL if you are on Windows.</p> generate_ssl_certificates.sh<pre><code>#!/bin/bash\n\nCONTAINER_CERT_DIR=\"./nginx/.certs/nginx\"\n\nif [[ ! -d \"${CONTAINER_CERT_DIR}\" ]]; then\n  echo \"SSL certificate directory '${CONTAINER_CERT_DIR}' does not exist. Creating.\"\n  mkdir -pv \"${CONTAINER_CERT_DIR}\"\nfi\n\nread -p \"What is your domain (i.e. hostname.home, localhost.home, etc): \" DOMAIN\n\necho \"Generating SSL certificate for edit.* domains\"\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout ${CONTAINER_CERT_DIR}/edit_key.key -out ${CONTAINER_CERT_DIR}/edit_cert.crt -subj \"/CN=edit.${DOMAIN}\"\n\necho \"Generating SSL certificate for docs.* domains\"\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout ${CONTAINER_CERT_DIR}/docs_key.key -out ${CONTAINER_CERT_DIR}/docs_cert.crt -subj \"/CN=docs.${DOMAIN}\"\n</code></pre>","tags":["docker","templates"]},{"location":"template/docker/editors/openvscode_server/index.html#nginxconf","title":"nginx.conf","text":"<p>This configuration enables HTTPS reverse proxying on your LAN using a self signed certificate. You will get an HTTPS error that you will need to proceed past, but this enables functionality in OpenVSCode Server like previewing Markdown files.</p> nginx.conf<pre><code>worker_processes 1;\n\nevents {\n    worker_connections 1024;\n}\n\nhttp {\n    include /etc/nginx/mime.types;\n    default_type application/octet-stream;\n\n    # Enable logging\n    access_log /var/log/nginx/access.log;\n    error_log /var/log/nginx/error.log;\n\n    # Generic server block for edit.* subdomains\n    server {\n        listen 443 ssl;\n        server_name edit.*;\n\n        ssl_certificate /etc/nginx/ssl/edit_cert.crt;  # Path to your SSL certificate\n        ssl_certificate_key /etc/nginx/ssl/edit_key.key;  # Path to your SSL key\n\n        location / {\n            proxy_pass http://openvscode-server:3000;  # Adjust the proxy_pass to your service\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n\n            ## Add these lines to fix the error:\n            #  The workbench failed to connect to the server (Error: WebSocket close with status code 1006)\n            proxy_set_header Upgrade $http_upgrade;\n            proxy_set_header Connection upgrade;\n            proxy_set_header Accept-Encoding gzip;\n        }\n    }\n\n}\n</code></pre>","tags":["docker","templates"]},{"location":"template/docker/editors/openvscode_server/index.html#notes","title":"Notes","text":"","tags":["docker","templates"]},{"location":"template/docker/editors/openvscode_server/index.html#links","title":"Links","text":"","tags":["docker","templates"]},{"location":"template/docker/messaging/index.html","title":"Messaging","text":"<p>Containers for messaging (i.e. inter-app messaging with RabbitMQ, or message relays for push notifications, etc).</p>"},{"location":"template/docker/messaging/mosquitto/index.html","title":"Mosquitto","text":"<p>An open source message broker like RabbitMQ. Mosquitto is simpler and more lightweight than RabbitMQ and better suited for devices like a Raspberry Pi.</p>","tags":["docker","templates","messaging","mosquitto","mqtt"]},{"location":"template/docker/messaging/mosquitto/index.html#directory-structure","title":"Directory Structure","text":"Container directory structure<pre><code>container_root/\n  ../.env\n  ../.gitignore\n  ../docker-compose.yml\n  ../mosquitto-host-setup.sh\n  ../config/mosquitto\n    ../ mosquitto.conf\n</code></pre>","tags":["docker","templates","messaging","mosquitto","mqtt"]},{"location":"template/docker/messaging/mosquitto/index.html#container-files","title":"Container Files","text":"","tags":["docker","templates","messaging","mosquitto","mqtt"]},{"location":"template/docker/messaging/mosquitto/index.html#env","title":".env","text":"mosquitto .env<pre><code>## Default: mosquitto\nMQTT_CONTAINER_NAME=\n## Default: 1883\nMQTT_PORT=\n## Default: 9001\nMQTT_HTTP_PORT=\n## Default: ./config/mosquitto\nMQTT_CONFIG_DIR=\n## Default: ./data/mosquitto\nMQTT_DATA_DIR=\n## Default: ./logs/mosquitto\nMQTT_LOGS_DIR=\n## Default: America/New_York\nTZ=\n## Default: 1883\nPUID=\n## Default: 1883\nPGID=\n</code></pre>","tags":["docker","templates","messaging","mosquitto","mqtt"]},{"location":"template/docker/messaging/mosquitto/index.html#gitignore","title":".gitignore","text":"mosquitto .gitignore<pre><code>data/*\nconfig/*\n\n!*.example\n!*.example.*\n!.*.example\n!.*.example.*\n</code></pre>","tags":["docker","templates","messaging","mosquitto","mqtt"]},{"location":"template/docker/messaging/mosquitto/index.html#docker-composeyml","title":"docker-compose.yml","text":"mosquitto docker-compose.yml<pre><code>---\nnetworks:\n  default:\n    name: mqtt-network\n\nservices:\n  mosquitto:\n    image: eclipse-mosquitto\n    container_name: ${MQTT_CONTAINER_NAME:-mosquitto}\n    restart: unless-stopped\n    env_file: ./.env\n    stdin_open: true\n    tty: true\n    # network_mode: \"host\"\n    ports:\n      - ${MQTT_PORT:-1883}:1883\n      - ${MQTT_HTTP_PORT:-9001}:9001\n    volumes:\n      - ${MQTT_CONFIG_DIR:-./config/mosquitto}:/mosquitto/config:rw\n      - ${MQTT_DATA_DIR:-./data/mosquitto}:/mosquitto/data:rw\n      - ${MQTT_LOGS_DIR:-./logs/mosquitto}:/mosquitto/log:rw\n    environment:\n      TZ: ${TZ:-America/New_York}\n    user: \"${PUID:-1883}:${PGID:-1883}\"\n</code></pre>","tags":["docker","templates","messaging","mosquitto","mqtt"]},{"location":"template/docker/messaging/mosquitto/index.html#mosquitto-host-setupsh","title":"mosquitto-host-setup.sh","text":"mosquitto-host-setup.sh<pre><code>#!/bin/bash\n\n## https://pimylifeup.com/home-assistant-docker-compose/#creating-a-user-for-the-mosquitto-docker-container\n\nif [[ ! $(getent passwd \"mosquitto\" 2&gt;&amp;1) ]]; then\n\n    echo \"Creating mosquitto user\"\n    sudo useradd -u 1883 -g 1883 mosquitto\n\nfi\n\nif [[ ! $(getent group mosquitto /dev/null 2&gt;&amp;1) ]]; then\n    echo \"Creating mosquitto group\"\n    sudo groupadd -g 1883 mosquitto\nfi\n\nif [[ ! -d ./config/mosquitto ]]; then\n    echo \"Creating config dir\"\n    mkdir -pv ./config/mosquitto\nfi\n\nif [[ ! -d ./data/mosquitto ]]; then\n    echo \"Creating mosquitto data dir\"\n    mkdir -pv ./data/mosquitto\nfi\n\nif [[ ! -d ./logs/mosquitto ]]; then\n    echo \"Creating mosquitto logs directory\"\n    mkdir -pv ./logs/mosquitto\nfi\n\nif [[ ! -f ./logs/mosquitto/mosquitto.log ]]; then\n    echo \"Creating empty mosquitto log file.\"\n    touch ./logs/mosquitto/mosquitto.log\nfi\n\nif [[ ! -f ./config/mosquitto/mosquitto.conf ]]; then\n    echo \"Creating config file\"\n\n    ## Create mosquitto conf file &amp; echo config into it\n    cat &lt;&lt;EOF &gt;./config/mosquitto/mosquitto.conf\nallow_anonymous true\n# password_file /mosquitto/config/pwfile\nlistener        1883 0.0.0.0\nlistener        9001 0.0.0.0\nprotocol websockets\npersistence     true\npersistence_file mosquitto.db\npersistence_location /mosquitto/data/\nlog_type subscribe\nlog_type unsubscribe\nlog_type websockets\nlog_type error\nlog_type warning\nlog_type notice\nlog_type information\nlog_dest        file /mosquitto/log/mosquitto.log\nEOF\n\n    # sudo chmod 0700 ./config/mosquitto/mosquitto.conf\n\nfi\n\nif [[ ! -f ./data/mosquitto/mosquitto.db ]]; then\n    echo \"Creating empty mosquitto.db database file for container\"\n    touch ./data/mosquitto/mosquitto.db\n\n    # sudo chown 1883:1883 mosquitto.db\n    sudo chmod 0700 ./data/mosquitto/mosquitto.db\nfi\n\necho \"Setting owner of ./data, ./logs, ./config to 1883:1883\"\ndeclare -a chmod_dirs=(./data ./logs ./config)\nfor d in \"${chmod_dirs[@]}\"; do\n    sudo chmod o+w ./logs/mosquitto/mosquitto.log\n    sudo chown -R 1883:1883 \"${d}\"\ndone\n</code></pre>","tags":["docker","templates","messaging","mosquitto","mqtt"]},{"location":"template/docker/messaging/mosquitto/index.html#configmosquittomosquittoconf","title":"config/mosquitto/mosquitto.conf","text":"mosquitto.conf<pre><code> allow_anonymous  true\n# password_file /mosquitto/config/pwfile\n listener         1883 0.0.0.0\n listener         9001 0.0.0.0\n protocol         websockets\n persistence      true\n persistence_file mosquitto.db\npersistence_location /mosquitto/data/\n log_type         subscribe\n log_type         unsubscribe\n log_type         websockets\n log_type         error\n log_type         warning\n log_type         notice\n log_type         information\n log_dest         file /mosquitto/log/mosquitto.log\n</code></pre>","tags":["docker","templates","messaging","mosquitto","mqtt"]},{"location":"template/docker/messaging/mosquitto/index.html#notes","title":"Notes","text":"","tags":["docker","templates","messaging","mosquitto","mqtt"]},{"location":"template/docker/messaging/mosquitto/index.html#links","title":"Links","text":"","tags":["docker","templates","messaging","mosquitto","mqtt"]},{"location":"template/docker/messaging/rabbitmq/index.html","title":"RabbitMQ","text":"<p>RabbitMQ is a messaging/event queue broker, useful for asynchronous processing and PUB/SUB architecture.</p>","tags":["docker","templates","rabbitmq","messaging","mqtt"]},{"location":"template/docker/messaging/rabbitmq/index.html#directory-structure","title":"Directory Structure","text":"Container directory structure<pre><code>container_root/\n  ../.env\n  ../.gitignore\n  ../docker-compose.yml\n  ../provision/\n    ../rabbitmq.config\n    ../rabbitmq.enabled_plugins\n</code></pre>","tags":["docker","templates","rabbitmq","messaging","mqtt"]},{"location":"template/docker/messaging/rabbitmq/index.html#container-files","title":"Container Files","text":"","tags":["docker","templates","rabbitmq","messaging","mqtt"]},{"location":"template/docker/messaging/rabbitmq/index.html#env","title":".env","text":"rabbitmq .env<pre><code>## Default: rabbitmq\nRABBITMQ_CONTAINER_NAME=\n## Default: rabbitmq\nRABBITMQ_USER=\n## Default: rabbitmq\nRABBITMQ_PASS=\n## Default: 5672\nRABBITMQ_AMPQ_PORT=\n## Default: 15672\nRABBITMQ_HTTP_PORT=\n## Default: ./data/rabbitmq\nRABBITMQ_DATA_DIR=\n## Default: ./logs/rabbitmq\nRABBITMQ_LOGS_DIR=\n## Default: ./provision/rabbitmq.config\nRABBITMQ_CONFIG_FILE=\n## Default: ./provision/rabbitmq.enabled_plugins\nRABBITMQ_ENABLED_PLUGINS_FILE=\n</code></pre>","tags":["docker","templates","rabbitmq","messaging","mqtt"]},{"location":"template/docker/messaging/rabbitmq/index.html#gitignore","title":".gitignore","text":"rabbitmq .gitignore<pre><code>provision/*\n\n!*.example\n!*.example.*\n!example.*\n!.example.*\n!example.*\n!example.*.*\n</code></pre>","tags":["docker","templates","rabbitmq","messaging","mqtt"]},{"location":"template/docker/messaging/rabbitmq/index.html#docker-composeyml","title":"docker-compose.yml","text":"rabbitmq docker-compose.yml<pre><code>---\nservices:\n  rabbitmq:\n    image: rabbitmq:management\n    container_name: ${RABBITMQ_CONTAINER_NAME:-rabbitmq}\n    environment:\n      RABBITMQ_DEFAULT_USER: ${RABBITMQ_USER:-rabbitmq}\n      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASS:-rabbitmq}\n      # RABBITMQ_DEFAULT_VHOST: ${RABBITMQ_VHOST:-rabbitmq}\n      # RABBITMQ_ENABLED_PLUGINS_FILE: /etc/rabbitmq/enabled_plugins\n    ports:\n      ## AMQP protocol\n      - ${RABBITMQ_AMPQ_PORT:-5672}:5672\n      - ${RABBITMQ_HTTP_PORT:-15672}:15672\n    volumes:\n      - ${RABBITMQ_DATA_DIR:-./data/rabbitmq}:/var/lib/rabbitmq\n      - ${RABBITMQ_LOGS_DIR:-./logs/rabbitmq}:/var/log/rabbitmq\n      - ${RABBITMQ_CONFIG_FILE:-./provision/rabbitmq.config}:/etc/rabbitmq/rabbitmq.config\n      - ${RABBITMQ_ENABLED_PLUGINS_FILE:-./provision/rabbitmq.enabled_plugins}:/etc/rabbitmq/enabled_plugins\n    healthcheck:\n      test: rabbitmq-diagnostics -q ping\n      interval: 30s\n      timeout: 30s\n      retries: 3\n</code></pre>","tags":["docker","templates","rabbitmq","messaging","mqtt"]},{"location":"template/docker/messaging/rabbitmq/index.html#provisionrabbitmqconfig","title":"provision/rabbitmq.config","text":"rabbitmq.config<pre><code>[\n  {rabbit, [\n    {queue_index_max_journal_entries, 10000},\n    {vm_memory_high_watermark, 0.4},\n    {disk_free_limit, {mem_relative, 0.1}}\n  ]}\n].\n</code></pre>","tags":["docker","templates","rabbitmq","messaging","mqtt"]},{"location":"template/docker/messaging/rabbitmq/index.html#provisionrabbitmqenabled_plugins","title":"provision/rabbitmq.enabled_plugins","text":"rabbitmq.enabled_plugins<pre><code>[rabbitmq_management,rabbitmq_prometheus].\n</code></pre>","tags":["docker","templates","rabbitmq","messaging","mqtt"]},{"location":"template/docker/messaging/rabbitmq/index.html#notes","title":"Notes","text":"<ul> <li>The <code>rabbitmq.config</code> and <code>rabbitmq.enabled_plugins</code> files are not required.<ul> <li>These files allow you to modify RabbitMQ's container runtime, installing plugins like <code>rabbitmq_management</code> (for a webUI), or modifying RabbitMQ's parameters on free disk space &amp; memory usage.</li> <li>If you do not create these files, RabbitMQ will run with its predefined defaults.</li> </ul> </li> </ul>","tags":["docker","templates","rabbitmq","messaging","mqtt"]},{"location":"template/docker/messaging/rabbitmq/index.html#links","title":"Links","text":"","tags":["docker","templates","rabbitmq","messaging","mqtt"]},{"location":"template/docker/monitoring/index.html","title":"Monitoring &amp; Alerting","text":"<p>Containers for monitoring &amp; alerting, like Zabbix or UptimeKuma.</p>"},{"location":"template/docker/monitoring/beszel/index.html","title":"Beszel","text":"<p>A simple, no-frills monitoring server.</p>","tags":["docker","templates","monitoring","beszel"]},{"location":"template/docker/monitoring/beszel/index.html#directory-structure","title":"Directory Structure","text":"Container directory structure<pre><code>container_root/\n  ../.env\n  ../.gitignore\n  ../docker-compose.yml\n</code></pre>","tags":["docker","templates","monitoring","beszel"]},{"location":"template/docker/monitoring/beszel/index.html#container-files","title":"Container Files","text":"<p><code>...</code></p>","tags":["docker","templates","monitoring","beszel"]},{"location":"template/docker/monitoring/beszel/index.html#env","title":".env","text":"beszel .env<pre><code>## Default: 8090\nBESZEL_WEBUI_PORT=\n## Default: ./beszel/data\nBESZEL_DATA_DIR=\n</code></pre>","tags":["docker","templates","monitoring","beszel"]},{"location":"template/docker/monitoring/beszel/index.html#gitignore","title":".gitignore","text":"beszel .gitignore<pre><code>beszel/data\n</code></pre>","tags":["docker","templates","monitoring","beszel"]},{"location":"template/docker/monitoring/beszel/index.html#docker-composeyml","title":"docker-compose.yml","text":"beszel docker-compose.yml<pre><code>---\nservices:\n  beszel:\n    image: 'henrygd/beszel'\n    container_name: 'beszel'\n    restart: unless-stopped\n    ports:\n      - ${BESZEL_WEBUI_PORT:-8090}:8090\n    volumes:\n      - ${BESZEL_DATA_DIR:-./beszel/data}:/beszel_data\n</code></pre>","tags":["docker","templates","monitoring","beszel"]},{"location":"template/docker/monitoring/beszel/index.html#notes","title":"Notes","text":"<ul> <li>After setting up a Beszel server, log into the web UI and add servers using the <code>+</code> button in the top right.<ul> <li>When you add a new server, you will be given an option to copy a <code>docker-compose.yml</code> or shell command for installing the agent.</li> <li>If you're able to use Docker, that's the simplest way to add a new server.</li> <li>Create the new client, save it, then paste the shell script or <code>docker-compose.yml</code> on the target you want to monitor.<ul> <li>The client will automatically connect back to the Beszel server using a preshared SSH key.</li> </ul> </li> </ul> </li> </ul>","tags":["docker","templates","monitoring","beszel"]},{"location":"template/docker/monitoring/beszel/index.html#links","title":"Links","text":"","tags":["docker","templates","monitoring","beszel"]},{"location":"template/docker/monitoring/uptime_kuma/index.html","title":"Uptime Kuma","text":"<p>A simple but powerful uptime monitor.</p>"},{"location":"template/docker/monitoring/uptime_kuma/index.html#directory-structure","title":"Directory Structure","text":"Container directory structure<pre><code>container_root/\n  ../.env\n  ../.gitignore\n  ../docker-compose.yml\n</code></pre>"},{"location":"template/docker/monitoring/uptime_kuma/index.html#container-files","title":"Container Files","text":"<p><code>...</code></p>"},{"location":"template/docker/monitoring/uptime_kuma/index.html#env","title":".env","text":"uptime kuma .env<pre><code>## Default: 3001\nUPTIME_KUMA_PORT=\n## Default: named volume \"uptime_kuma_data\"\nUPTIME_KUMA_DATA_DIR=\n</code></pre>"},{"location":"template/docker/monitoring/uptime_kuma/index.html#gitignore","title":".gitignore","text":"uptime kuma .gitignore<pre><code>**/data\n</code></pre>"},{"location":"template/docker/monitoring/uptime_kuma/index.html#docker-composeyml","title":"docker-compose.yml","text":"uptime kuma docker-compose.yml<pre><code>---\nvolumes:\n  uptime_kuma_data:\n\nservices:\n\n  uptime-kuma:\n    image: louislam/uptime-kuma\n    container_name: uptime-kuma\n    restart: unless-stopped\n    ports:\n      - ${UPTIME_KUMA_PORT:-3001}:3001\n    volumes:\n      - ${UPTIME_KUMA_DATA_DIR:-uptime_kuma_data}:/app/data\n</code></pre>"},{"location":"template/docker/monitoring/uptime_kuma/index.html#notes","title":"Notes","text":""},{"location":"template/docker/monitoring/uptime_kuma/index.html#links","title":"Links","text":""},{"location":"template/docker/monitoring/watchtower/index.html","title":"Watchtower","text":"<p>A container to watch other containers and automate performing image pulls &amp; updates.</p>"},{"location":"template/docker/monitoring/watchtower/index.html#directory-structure","title":"Directory Structure","text":"Container directory structure<pre><code>container_root/\n  ../.env\n  ../.gitignore\n  ../docker-compose.yml\n</code></pre>"},{"location":"template/docker/monitoring/watchtower/index.html#container-files","title":"Container Files","text":""},{"location":"template/docker/monitoring/watchtower/index.html#env","title":".env","text":"watchtower .env<pre><code># Default: true\nWATCHTOWER_MONITOR_ONLY=\n# Add the following label to containers if below is \"true\" (default)\n#   com.centurylinklabs.watchtower.enable=\"true\"\nWATCHTOWER_LABEL_ENABLE=\n# Default: false\nWATCHTOWER_CLEANUP=true\n# Default: false\nWATCHTOWER_INCLUDE_RESTARTING=\n# Default: false\nWATCHTOWER_INCLUDE_STOPPED=\n# Default: false\nWATCHTOWER_REVIVE_STOPPED=\n\n## Notification settings\n\n# Options: (default) email, msteams, slack, gotify, shoutrrr\nWATCHTOWER_NOTIFICATION_TYPE=\nWATCHTOWER_FROM_ADDR=\nWATCHTOWER_TO_ADDR=\n# Default: smtp.gmail.com\nWATCHTOWER_EMAIL_SERVER=\nWATCHTOWER_NOTIFICATION_DELAY=\nWATCHTOWER_NOTIFICATION_PASSWORD=\nWATCHTOWER_EMAIL_SUBJECT=\n# Default: 587\nWATCHTOWER_NOTIFICATION_EMAIL_PORT=\n</code></pre>"},{"location":"template/docker/monitoring/watchtower/index.html#gitignore","title":".gitignore","text":"watchtower .gitignore<pre><code># Ignore .env file\n.env\n\n# Don't ignore specific files\n!**/*.example\n</code></pre>"},{"location":"template/docker/monitoring/watchtower/index.html#docker-composeyml","title":"docker-compose.yml","text":"watchtower docker-compose.yml<pre><code>---\nnetworks:\n  watchtower:\n    external: true\n\nservices:\n\n  watchtower:\n    image: containrrr/watchtower:latest\n    container_name: watchtower\n    restart: unless-stopped\n    environment:\n      WATCHTOWER_MONITOR_ONLY: ${WATCHTOWER_MONITOR_ONLY:-true}\n      WATCHTOWER_CLEANUP: ${WATCHTOWER_CLEANUP:-false}\n      WATCHTOWER_LABEL_ENABLE: ${WATCHTOWER_LABEL_ENABLE:-true}\n      WATCHTOWER_INCLUDE_RESTARTING: ${WATCHTOWER_INCLUDE_RESTARTING:-false}\n      WATCHTOWER_INCLUDE_STOPPED: ${WATCHTOWER_INCLUDE_STOPPED:-false}\n      WATCHTOWER_REVIVE_STOPPED: ${WATCHTOWER_REVIVE_STOPPED:-false}\n      ## Notification settings below. Comment section until \"volumes\" to disable notifications\n      WATCHTOWER_NOTIFICATIONS: ${WATCHTOWER_NOTIFICATION_TYPE:-email}\n      WATCHTOWER_NOTIFICATION_EMAIL_FROM: ${WATCHTOWER_FROM_ADDR}\n      WATCHTOWER_NOTIFICATION_EMAIL_TO: ${WATCHTOWER_TO_ADDR}\n      # you have to use a network alias here, if you use your own certificate\n      WATCHTOWER_NOTIFICATION_EMAIL_SERVER: ${WATCHTOWER_EMAIL_SERVER:-smtp.gmail.com}\n      WATCHTOWER_NOTIFICATION_EMAIL_SERVER_PORT: ${WATCHTOWER_EMAIL_PORT:-587}\n      WATCHTOWER_NOTIFICATION_EMAIL_DELAY: ${WATCHTOWER_NOTIFICATION_DELAY:-2}\n      WATCHTOWER_NOTIFICATION_EMAIL_SERVER_PASSWORD: ${WATCHTOWER_NOTIFICATION_PASSWORD}\n      WATCHTOWER_NOTIFICATION_EMAIL_SUBJECTTAG: ${WATCHTOWER_EMAIL_SUBJECT}\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n    networks:\n      - watchtower\n</code></pre>"},{"location":"template/docker/monitoring/watchtower/index.html#notes","title":"Notes","text":"<ul> <li>You must create a Docker network on the host (i.e. not just in the <code>docker-compose.yml</code> file), named <code>watchtower</code>, for this to function.</li> <li>This container assumes you are installing watchtower on the host and want it to watch any container on the <code>watchtower</code> network with the watchtower label applied.</li> </ul>"},{"location":"template/docker/monitoring/watchtower/index.html#links","title":"Links","text":"<ul> <li>Watchtower container args</li> </ul>"},{"location":"template/docker/networking/index.html","title":"Networking","text":"<p>Containers related to networking, like reverse proxies, adblockers/DNS servers, etc.</p>"},{"location":"template/docker/networking/adguard_home/index.html","title":"Adguard Home","text":"<p>An ad-blocking DNS manager.</p>","tags":["docker","templates","adblocking","adguard"]},{"location":"template/docker/networking/adguard_home/index.html#directory-structure","title":"Directory Structure","text":"Container directory structure<pre><code>container_root/\n  ../.env\n  ../.gitignore\n  ../docker-compose.yml\n</code></pre>","tags":["docker","templates","adblocking","adguard"]},{"location":"template/docker/networking/adguard_home/index.html#container-files","title":"Container Files","text":"","tags":["docker","templates","adblocking","adguard"]},{"location":"template/docker/networking/adguard_home/index.html#env","title":".env","text":"adguard-home .env<pre><code>## Default: ./adguardhome/work\nADGUARD_WORK_DIR=\n## Default: ./adguardhome/config\nADGUARD_CONF_DIR=\n</code></pre>","tags":["docker","templates","adblocking","adguard"]},{"location":"template/docker/networking/adguard_home/index.html#gitignore","title":".gitignore","text":"adguard-home .gitignore<pre><code>adguardhome/\n</code></pre>","tags":["docker","templates","adblocking","adguard"]},{"location":"template/docker/networking/adguard_home/index.html#docker-composeyml","title":"docker-compose.yml","text":"adguard-home docker-compose.yml<pre><code>services:\n  adguardhome:\n    image: 'adguard/adguardhome:latest'\n    container_name: 'adguard'\n    hostname: 'adguard'\n    restart: 'unless-stopped'\n    volumes:\n      - /etc/localtime:/etc/localtime:ro\n      - ${ADGUARD_WORK_DIR:-./adguardhome/work}:/opt/adguardhome/work\n      - ${ADGUARD_CONF_DIR:-./adguardhome/config}:/opt/adguardhome/conf\n    ports:\n      # Plain DNS\n      - 53:53/tcp\n      - 53:53/udp\n      # AdGuard Home Admin Panel as well as DNS-over-HTTPS\n      - 80:80/tcp\n      - 443:443/tcp\n      - 443:443/udp\n      - 3000:3000/tcp\n      # DNS-over-TLS\n      - 853:853/tcp\n      # DNS-over-QUIC\n      - 784:784/udp\n      - 853:853/udp\n      - 8853:8853/udp\n      # DNSCrypt\n      - 5443:5443/tcp\n      - 5443:5443/udp\n</code></pre>","tags":["docker","templates","adblocking","adguard"]},{"location":"template/docker/networking/adguard_home/index.html#notes","title":"Notes","text":"","tags":["docker","templates","adblocking","adguard"]},{"location":"template/docker/networking/adguard_home/index.html#links","title":"Links","text":"<ul> <li>Docker Hub: Adguard Home</li> </ul>","tags":["docker","templates","adblocking","adguard"]},{"location":"template/docker/networking/nginx_proxy_manager/index.html","title":"NGINX Proxy Manager","text":"<p>A GUI application for managing an NGINX reverse proxy server.</p>","tags":["docker","templates","nginx"]},{"location":"template/docker/networking/nginx_proxy_manager/index.html#directory-structure","title":"Directory Structure","text":"Container directory structure<pre><code>container_root/\n  ../.env\n  ../.gitignore\n  ../docker-compose.yml\n  ../backup_nginx-proxy-manager.sh\n  ../fix_watchtower_permissions.sh\n  ../safe_update.sh\n</code></pre>","tags":["docker","templates","nginx"]},{"location":"template/docker/networking/nginx_proxy_manager/index.html#container-files","title":"Container Files","text":"","tags":["docker","templates","nginx"]},{"location":"template/docker/networking/nginx_proxy_manager/index.html#env","title":".env","text":"nginx proxy manager .env<pre><code>## Default: ./data\nNPM_DATA_DIR=\n## Default: ./letsencrypt\nNPM_LETSENCRYPT_DIR=\n</code></pre>","tags":["docker","templates","nginx"]},{"location":"template/docker/networking/nginx_proxy_manager/index.html#gitignore","title":".gitignore","text":"nginx proxy manager .gitignore<pre><code>logs/\n.env\n</code></pre>","tags":["docker","templates","nginx"]},{"location":"template/docker/networking/nginx_proxy_manager/index.html#docker-composeyml","title":"docker-compose.yml","text":"nginx proxy manager docker-compose.yml<pre><code>---\nservices:\n\n  app:\n    image: 'jc21/nginx-proxy-manager:latest'\n    container_name: npm_proxy\n    restart: unless-stopped\n    ports:\n      - '80:80'\n      - '81:81'\n      - '443:443'\n    volumes:\n      - ${NPM_DATA_DIR:-./data}:/data\n      - ${NPM_LETSENCRYPT_DIR:-./letsencrypt}:/etc/letsencrypt\n    labels:\n      - \"com.centurylinklabs.watchtower.enable=true\"\n\n  watchtower:\n    image: containrrr/watchtower\n    container_name: npm_proxy_watchtower\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n    restart: unless-stopped\n    environment:\n      ## Remove old images after updating\n      WATCHTOWER_CLEANUP: ${WATCHTOWER_CLEANUP:-\"true\"}\n      ## Check for updates once every 24h\n      WATCHTOWER_POLL_INTERVAL: ${WATCHTOWWER_POLL_INTERVAL:-86400}\n    command: --label-enable\n</code></pre>","tags":["docker","templates","nginx"]},{"location":"template/docker/networking/nginx_proxy_manager/index.html#backup_nginx-proxy-managersh","title":"backup_nginx-proxy-manager.sh","text":"backup_nginx-proxy-manager.sh<pre><code>#!/bin/bash\n\n## Make a backup of the Nginx Proxy Manager directory.\n\ntimestamp() { date +\"%Y-%m-%d_%H:%M\"; }\n\nBACKUP_DEST=\"${HOME}/backup/nginx_proxy_manager\"\nSRC_PATH=\"/home/${USER}/git/docker_templates/templates/docker_nginx-proxy-manager/new-version\"\nTAR_PATH=\"${BACKUP_DEST}/backup_${HOSTNAME}_nginx-proxy-manager_$(timestamp).tar.gz\"\n\nfunction make_backup() {\n  echo \"Backing up source path: ${SRC_PATH}\"\n  echo \"  To destination: ${TAR_PATH}\"\n\n  sudo tar czvf \"${TAR_PATH}\" \"${SRC_PATH}\"\n\n}\n\nfunction fix_dest_dir_permissions() {\n  sudo chown -R ${USER}:${USER} \"${BACKUP_DEST}\"\n}\n\nfunction main() {\n  if [[ ! -d \"${BACKUP_DEST}\" ]]; then\n    sudo mkdir -pv \"${BACKUP_DEST}\"\n    fix_dest_dir_permissions\n  fi\n\n  make_backup\n\n  echo \"Fixing permissions on output file: ${TAR_PATH}\"\n  fix_dest_dir_permissions\n}\n\nmain\n</code></pre>","tags":["docker","templates","nginx"]},{"location":"template/docker/networking/nginx_proxy_manager/index.html#fix_watchtower_permissionssh","title":"fix_watchtower_permissions.sh","text":"fix_watchtower_permissions.sh<pre><code>#!/bin/bash\n\n##\n# Allow watchtower container to interact with docker daemon\n# to remove images and pull updates/restart containers.\n##\n\nUSER=${USER}\n\necho \"Granting watchtower container permissions to interact with the Docker daemon.\"\n\nsudo setfacl --modify user:${USER}:rw /var/run/docker.sock\n</code></pre>","tags":["docker","templates","nginx"]},{"location":"template/docker/networking/nginx_proxy_manager/index.html#safe_updatesh","title":"safe_update.sh","text":"safe_update.sh<pre><code>#!/bin/bash\n\nDATA_DIR=\"data\"\nDATA_BAK_DIR=\"data.bak\"\n\nfunction check_path_exists() {\n  if [[ -d \"${1}\" ]]; then\n    # echo \"Path '${1}' exists\"\n    return 0\n  else\n    # echo \"Path '${1}' does not exist\"\n    return 1\n  fi\n}\n\nfunction main() {\n  ## 0=exists, 1=not exists\n  BAK_DIR_EXISTS=$(check_path_exists \"${DATA_BAK_DIR}\")\n\n  if [[ $BAK_DIR_EXISTS -eq 0 ]]; then\n    echo \"$DATA_BAK_DIR exists\"\n  else\n    echo \"$DATA_BAK_DIR does not exist\"\n  fi\n}\n\nmain\n</code></pre>","tags":["docker","templates","nginx"]},{"location":"template/docker/networking/nginx_proxy_manager/index.html#notes","title":"Notes","text":"","tags":["docker","templates","nginx"]},{"location":"template/docker/networking/nginx_proxy_manager/index.html#links","title":"Links","text":"","tags":["docker","templates","nginx"]},{"location":"template/docker/networking/unifi_controller/index.html","title":"Unifi Controller","text":"<p>Dockerized version of the Unifi Controller software (alternative to using their CloudKey).</p> <p>Warning</p> <p>The LinuxServer container I have been using is no longer supported.</p> <p>Please do not use the template on this page until this message is removed, indicating I have updated my template to use the new container from LinuxServer.</p>","tags":["docker","templates","unifi"]},{"location":"template/docker/networking/unifi_controller/index.html#directory-structure","title":"Directory Structure","text":"Container directory structure<pre><code>container_root/\n  ../.env\n  ../.gitignore\n  ../docker-compose.yml\n  ../update-controller-container.sh\n</code></pre>","tags":["docker","templates","unifi"]},{"location":"template/docker/networking/unifi_controller/index.html#container-files","title":"Container Files","text":"","tags":["docker","templates","unifi"]},{"location":"template/docker/networking/unifi_controller/index.html#env","title":".env","text":"unifi controller .env<pre><code>UNIFI_CONF_DIR=\nUNIFI_INFORM_DIR=\nUNIFI_WEB_PORT=443\nUNIFI_GUEST_HTTP_PORT=\nUNIFI_GUEST_HTTPS_PORT=\n</code></pre>","tags":["docker","templates","unifi"]},{"location":"template/docker/networking/unifi_controller/index.html#gitignore","title":".gitignore","text":"unifi controller .gitignore<pre><code>config/\n</code></pre>","tags":["docker","templates","unifi"]},{"location":"template/docker/networking/unifi_controller/index.html#docker-composeyml","title":"docker-compose.yml","text":"unifi controller docker-compose.yml<pre><code>---\n# networks:\n#   watchtower:\n#     external: true\n\nservices:\n\n  unifi-controller:\n    image: ghcr.io/linuxserver/unifi-controller\n    # image: ghcr.io/linuxserver/unifi-controller:7.1.65\n    container_name: unifi-controller\n    restart: always\n    environment:\n      - PUID=1000\n      - PGID=1000\n      # - MEM_LIMIT=768M #optional\n    volumes:\n      # - ./unifi-controller/config:/config\n      - ${UNIFI_CONF_DIR:-./config}:/config\n    ports:\n      - 3478:3478/udp # STUN\n      - 10001:10001/udp # AP discovery\n      - ${UNIFI_INFORM_PORT:-8080}:8080 # Device comm., i.e. set-inform\n      - ${UNIFI_WEB_PORT:-8443}:8443  # web admin\n      - 1900:1900/udp # optional, \"make controller discoverable on L2 network\n      - ${UNIFI_GUEST_HTTP_PORT:-8843}:8843 # optional, guest portal HTTPS redirect\n      - ${UNIFI_GUEST_HTTPS_PORT:-8880}:8880 # optional, guest portal HTTP redirect\n      - 6789:6789 # optional, mobile throughput test\n      - 5514:5514/udp #optional, remote syslog\n    # labels:\n    #   - com.centurylinklabs.watchtower.enable=\"true\"\n    # networks:\n    #   - watchtower\n\n  watchtower:\n    image: containrrr/watchtower\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n    ## 259200s = 3 days\n    command: --interval 259200\n</code></pre>","tags":["docker","templates","unifi"]},{"location":"template/docker/networking/unifi_controller/index.html#update-controller-containersh","title":"update-controller-container.sh","text":"Update Unifi Controller container<pre><code>#!/bin/bash\n\n# Update the unifi controller container\n\n# Pull latest image\ndocker compose pull unifi-controller\n\n# Run commands to remove existing container, just in case\ndocker stop unifi-controller\ndocker rm unifi-controller\n\n# Recreate container from new image\ndocker compose up -d\n</code></pre>","tags":["docker","templates","unifi"]},{"location":"template/docker/networking/unifi_controller/index.html#notes","title":"Notes","text":"","tags":["docker","templates","unifi"]},{"location":"template/docker/networking/unifi_controller/index.html#links","title":"Links","text":"<ul> <li>link1</li> </ul>","tags":["docker","templates","unifi"]},{"location":"template/docker/networking/wg-easy/index.html","title":"WireGuard Easy","text":"<p>Utilities for simplifying deployment &amp; management of a WireGuard VPN.</p>","tags":["docker","templates","networking","wireguard","vpn"]},{"location":"template/docker/networking/wg-easy/index.html#directory-structure","title":"Directory Structure","text":"Container directory structure<pre><code>container_root/\n  ../.env\n  ../.gitignore\n  ../docker-compose.yml\n  ../generate_wg_password_hash.sh\n</code></pre>","tags":["docker","templates","networking","wireguard","vpn"]},{"location":"template/docker/networking/wg-easy/index.html#container-files","title":"Container Files","text":"","tags":["docker","templates","networking","wireguard","vpn"]},{"location":"template/docker/networking/wg-easy/index.html#env","title":".env","text":"wireguard easy .env<pre><code>## Default: (unset) encrypt your password using\n#  the generate_wg_password_hash.sh script, paste\n#  encrypted password here, replacing any $ characters\n#  with $$.\nWG_EASY_ADMIN_PASSWORD_HASH=\n\n## Default: latest\n#  Find most recent release at: https://github.com/wg-easy/wg-easy/releases\nWG_EASY_IMG_VER=\n## Default: en\nWG_EASY_LANG=\n## Default: 127.0.0.1\nWG_EASY_HOST=\n## Default: 1.1.1.1\nWG_EASY_DEFAULT_DNS=\n## Default: 1420\nWG_EASY_MTU=\n## Default: 25\nWG_EASY_KEEPALIVE=\n## Default: true\nWG_EASY_ENABLE_TRAFFIC_STATS=\n## Default: true\nWG_EASY_ENABLE_ONETIME_LINKS=\n## Default: true\nWG_EASY_ENABLE_UI_CLIENT_SORT=\n## Default: (named volume) wg-easy_etc\nWG_EASY_DATA_DIR=\n## Default: 51820\nWG_EASY_LISTEN_PORT=\n## Default: 51821\nWG_EASY_WEBUI_PORT=\n</code></pre>","tags":["docker","templates","networking","wireguard","vpn"]},{"location":"template/docker/networking/wg-easy/index.html#gitignore","title":".gitignore","text":"wireguard easy .gitignore<pre><code>wg-easy/data\n</code></pre>","tags":["docker","templates","networking","wireguard","vpn"]},{"location":"template/docker/networking/wg-easy/index.html#docker-composeyml","title":"docker-compose.yml","text":"wireguard easy docker-compose.yml<pre><code>---\nvolumes:\n  wg-easy_etc: {}\n\nservices:\n  wg-easy:\n    image: ghcr.io/wg-easy/wg-easy:${WG_EASY_IMG_VER:-latest}\n    container_name: wg-easy\n    restart: unless-stopped\n    cap_add:\n      - NET_ADMIN\n      - SYS_MODULE\n      # - NET_RAW # \u26a0\ufe0f Uncomment if using Podman\n    sysctls:\n      - net.ipv4.ip_forward=1\n      - net.ipv4.conf.all.src_valid_mark=1\n    environment:\n      # Change Language:\n      # (Supports: en, ua, ru, tr, no, pl, fr, de, ca, es, ko, vi, nl, is, pt, chs, cht, it, th, hi, ja, si)\n      - LANG=${WG_EASY_LANG:-en}\n      # \u26a0\ufe0f Required:\n      # Change this to your host's public address\n      - WG_HOST=${WG_EASY_HOST:-127.0.0.1}\n\n      # Optional:\n      - PASSWORD_HASH=${WG_EASY_ADMIN_PASSWORD_HASH}\n      # - PORT=51821\n      # - WG_PORT=51820\n      # - WG_CONFIG_PORT=92820\n      # - WG_DEFAULT_ADDRESS=10.8.0.x\n      - WG_DEFAULT_DNS=${WG_EASY_DEFAULT_DNS:-1.1.1.1}\n      - WG_MTU=${WG_EASY_MTU:-1420}\n      # - WG_ALLOWED_IPS=192.168.15.0/24, 10.0.1.0/24\n      ## Second(s) to keep connection alive. 0=don't keep connection alive\n      - WG_PERSISTENT_KEEPALIVE=${WG_EASY_KEEPALIVE:-25}\n      # - WG_PRE_UP=echo \"Pre Up\" &gt; /etc/wireguard/pre-up.txt\n      # - WG_POST_UP=echo \"Post Up\" &gt; /etc/wireguard/post-up.txt\n      # - WG_PRE_DOWN=echo \"Pre Down\" &gt; /etc/wireguard/pre-down.txt\n      # - WG_POST_DOWN=echo \"Post Down\" &gt; /etc/wireguard/post-down.txt\n      ## Enable detailed RX/TX client stats in webUI\n      - UI_TRAFFIC_STATS=${WG_EASY_ENABLE_TRAFFIC_STATS:-true}\n      ## 0=Charts disabled, 1=Line chart, 2=Area chart, 3=Bar chart)\n      # - UI_CHART_TYPE=0 \n      - WG_ENABLE_ONE_TIME_LINKS=${WG_EASY_ENABLE_ONETIME_LINKS:-true}\n      ## Sort clients in webUI by name\n      - UI_ENABLE_SORT_CLIENTS=${WG_EASY_ENABLE_UI_CLIENT_SORT:-true}\n      ## Enable client expiration\n      # - WG_ENABLE_EXPIRES_TIME=true\n      # - ENABLE_PROMETHEUS_METRICS=false\n      # - PROMETHEUS_METRICS_PASSWORD=$$2a$$12$$vkvKpeEAHD78gasyawIod.1leBMKg8sBwKW.pQyNsq78bXV3INf2G # (needs double $$, hash of 'prometheus_password'; see \"How_to_generate_an_bcrypt_hash.md\" for generate the hash)\n    volumes:\n      - ${WG_EASY_DATA_DIR:-wg-easy_etc}:/etc/wireguard\n    ports:\n      - ${WG_EASY_LISTEN_PORT:-51820}:51820/udp\n      - ${WG_EASY_WEBUI_PORT:-51821}:51821/tcp\n</code></pre>","tags":["docker","templates","networking","wireguard","vpn"]},{"location":"template/docker/networking/wg-easy/index.html#generate_wg_password_hashsh","title":"generate_wg_password_hash.sh","text":"<p>This script prompts the user for a password for the <code>wg-easy</code> webUI admin, then returns the password as a bcrypt string. This password hash should be pasted in the <code>.env</code> file's <code>WG_EASY_ADMIN_PASSWORD_HASH</code> env variable, replacing any <code>$</code> characters with <code>$$</code>.</p> generate_wg_easy_password_hash.sh<pre><code>#!/bin/bash\n\n## Get user password before running container.\n#  Hide password input with -s\nread -s -p \"Password to encrypt: \" USER_PASSWORD\n\necho \"Hashing password with wg-easy container\"\ndocker run -it ghcr.io/wg-easy/wg-easy wgpw \"${USER_PASSWORD}\"\n\necho \"Paste the password above into your .env file's 'WG_EASY_ADMIN_PASSWORD_HASH' variable. Make sure to change any '$' symbols to '\\$\\$'!\"\n</code></pre>","tags":["docker","templates","networking","wireguard","vpn"]},{"location":"template/docker/networking/wg-easy/index.html#notes","title":"Notes","text":"","tags":["docker","templates","networking","wireguard","vpn"]},{"location":"template/docker/networking/wg-easy/index.html#usage","title":"Usage","text":"<ul> <li>Copy <code>.env.example</code> -&gt; <code>.env</code></li> <li>Generate your admin password by running the <code>generate_wg_password_hash.sh</code> script.<ul> <li>Copy the generated password into the <code>WG_EASY_ADMIN_PASSWORD_HASH</code> env variable in <code>.env</code>.</li> <li>NOTE: You must replace any <code>$</code> characters with <code>$$</code>.</li> </ul> </li> <li>Set your machine's hostname/address in <code>WG_EASY_HOST</code><ul> <li>This can be an IP address or FQDN (i.e. <code>wg.your-domain.com</code>), but FQDN is preferred.</li> </ul> </li> <li>Allow the following ports through your firewall:<ul> <li><code>51820/udp</code> (WireGuard's communication port)</li> <li><code>51821/tcp</code> (WireGuard's webUI port)</li> </ul> </li> <li>Run the stack with <code>docker compose up -d</code></li> <li>Access the web UI at <code>http://&lt;your-wireguard-hostname&gt;:51821</code></li> </ul>","tags":["docker","templates","networking","wireguard","vpn"]},{"location":"template/docker/networking/wg-easy/index.html#links","title":"Links","text":"<ul> <li>Github: wg-easy<ul> <li>Run WireGuard Easy</li> <li>WireGuard Docker env variables</li> <li>Using WireGuard Easy with NGINX SSL</li> </ul> </li> </ul>","tags":["docker","templates","networking","wireguard","vpn"]},{"location":"template/docker/portainer/index.html","title":"Portainer","text":"<ul> <li>Portainer docs site</li> <li>Portainer CE install</li> <li>Portainer CE Linux - Docker Standalone</li> </ul>","tags":["docker","portainer","template"]},{"location":"template/docker/portainer/index.html#running-portainer-server","title":"Running Portainer Server","text":"<p>At least 1 Portainer server must be available for agents to connect to. Copy this script to a file, i.e. <code>run-portainer.sh</code>.</p> <p>Note</p> <p>Don't forget to set <code>chmod +x run-portainer.sh</code>, or execute the script with <code>bash run-portainer.sh</code>.</p> run-portainer.sh<pre><code>#!/bin/bash\n\nWEBUI_PORT=\"9000\"\n## Defaults to 'portainer' if empty\nCONTAINER_NAME=\n## Defaults to a named volume, portainer_data.\n#  Note: create this volume with $ docker volume create portainer_data\nDATA_DIR=\n\necho \"\"\necho \"Checking for new image\"\necho \"\"\n\ndocker pull portainer/portainer-ce\n\necho \"\"\necho \"Restarting Portainer\"\necho \"\"\n\ndocker stop portainer &amp;&amp; docker rm portainer\n\ndocker run -d \\\n    -p 8000:8000 \\\n    -p ${WEBUI_PORT:-9000}:9000 \\\n    --name=${CONTAINER_NAME:-portainer} \\\n    --restart=unless-stopped \\\n    -v /var/run/docker.sock:/var/run/docker.sock \\\n    -v ${DATA_DIR:-portainer_data}:/data \\\n    portainer/portainer-ce\n</code></pre>","tags":["docker","portainer","template"]},{"location":"template/docker/portainer/index.html#running-portainer-agent","title":"Running Portainer Agent","text":"<p>Start a Portainer in agent mode to allow connection from a Portainer server. This setup is done in the Portainer server's webUI.</p> <p>Warning</p> <p>It is probably easier to just download the agent script from the Portainer server when you are adding a connection. It offers a command you can run to simplify setup.</p> run-portainer_agent.sh<pre><code>#!/bin/bash\n\necho \"\"\necho \"Checking for new container image\"\necho \"\"\n\ndocker pull portainer/agent\n\necho \"\"\necho \"Restarting Portainer\"\necho \"\"\n\ndocker stop portainer-agent &amp;&amp; docker rm portainer-agent\n\ndocker run -d \\\n    -p 9001:9001 \\\n    --name=portainer-agent \\\n    --restart=unless-stopped \\\n    -v /var/run/docker.sock:/var/run/docker.sock \\\n    -v /var/lib/docker/volumes:/var/lib/docker/volumes \\\n    portainer/agent:latest\n</code></pre>","tags":["docker","portainer","template"]},{"location":"template/docker/portainer/index.html#my-portainer-backup-script","title":"My Portainer backup script","text":"<p>Run this script to backup the Portainer <code>portainer_data</code> volume. Backup will be placed at <code>${CWD}/portainer_data_backup</code></p> backup_portainer.sh<pre><code>#!/bin/bash\n\n# Name of container containing volume(s) to back up\nCONTAINER_NAME=${1:-portainer}\nTHIS_DIR=${PWD}\nBACKUP_DIR=$THIS_DIR\"/portainer_data_backup\"\n# Directory to back up in container\nCONTAINER_BACKUP_DIR=${2:-/data}\n# Container image to use as temporary backup mount container\nBACKUP_IMAGE=${3:-busybox}\nBACKUP_METHOD=${4:-tar}\nDATA_VOLUME_NAME=${5:-portainer-data}\n\nif [[ ! -d $BACKUP_DIR ]]; then\n  echo \"\"\n  echo $BACKUP_DIR\" does not exist. Creating.\"\n  echo \"\"\n\n  mkdir -pv $BACKUP_DIR\nfi\n\nfunction RUN_BACKUP () {\n\n  sudo docker run --rm --volumes-from $1 -v $BACKUP_DIR:/backup $BACKUP_IMAGE $2 /backup/backup.tar $CONTAINER_BACKUP_DIR\n\n}\n\nfunction RESTORE_BACKUP () {\n\n  echo \"\"\n  echo \"The restore function is experimental until this comment is removed.\"\n  echo \"\"\n  read -p \"Do you want to continue? Y/N: \" choice\n\n  case $choice in\n    [yY] | [YyEeSs])\n      echo \"\"\n      echo \"Test print: \"\n      echo \"sudo docker create -v $CONTAINER_BACKUP_DIR --name $DATA_VOLUME_NAME\"2\" $BACKUP_IMAGE true\"\n      echo \"\"\n      echo \"Test print: \"\n      echo \"sudo docker run --rm --volumes-from $DATA_VOLUME_NAME\"2\" -v $BACKUP_DIR:/backup $BACKUP_IMAGE tar xvf /backup/backup.tar\"\n      echo \"\"\n\n      echo \"\"\n      echo \"Compare to original container: \"\n      echo \"\"\n      echo \"Test print: \"\n      echo \"sudo docker run --rm --volumes-from $CONTAINER_NAME -v $BACKUP_DIR:/backup $BACKUP_IMAGE ls /data\"\n    ;;\n    [nN] | [NnOo])\n      echo \"\"\n      echo \"Ok, nevermind.\"\n      echo \"\"\n    ;;\n  esac\n\n}\n\n# Run a temporary container, mount volume to back up, create backup file\ncase $1 in\n  \"-b\" | \"--backup\")\n  case $BACKUP_METHOD in\n    \"tar\")\n      echo \"\"\n      echo \"Running \"$BACKUP_METHOD\" backup using image \"$BACKUP_IMAGE\n      echo \"\"\n\n      RUN_BACKUP $CONTAINER_NAME \"tar cvf\"\n    ;;\n  esac\n  ;;\n  \"-r\" | \"--restore\")\n  ;;\nesac\n</code></pre>","tags":["docker","portainer","template"]},{"location":"template/docker/portainer/index.html#run-portainer-with-docker-compose","title":"Run Portainer with Docker Compose","text":"<p>Note</p> <p>I do not use this method. I find it easier to run Portainer with a shell script.</p> Portainer docker-compose.yml<pre><code>version: \"3\"\n\nservices:\n\n  portainer:\n    image: portainer/portainer-ce:linux-amd64\n    container_name: portainer\n    restart: unless-stopped\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - ${PORTAINER_DATA:-./data}:/data\n</code></pre>","tags":["docker","portainer","template"]},{"location":"template/docker/python/index.html","title":"Python Dockerfiles","text":"<p>Building/running a Python app in a Dockerfile can be accomplished many different ways. The example(s) below are my personal approach, meant to serve as a starting point/example of Docker's capabilities.</p> <p>One thing you will commonly see in my Python Dockerfiles are a set of <code>ENV</code> variables in the <code>base</code> layer. Below is a list of the <code>ENV</code> variables I commonly set in Dockerfiles, and what they do:</p> <p>Note</p> <p>In some Dockerfiles, you will see <code>ENV</code> variables declared with an equal sign, and others without. These are equivalent, you can declare/set these variables either way and they will produce the same result. For example, the following 2 <code>ENV</code> variable declarations are equivalent:</p> <p><code>PYTHONUNBUFFERED 1</code></p> <p><code>PYTHONUNBUFFERED=1</code></p> <ul> <li><code>PYTHONDONTWRITEBYTECODE=1</code><ul> <li>Disables creation of <code>.pyc</code> files<ul> <li><code>.pyc</code> files are essentially simplified bytecode versions of your scripts that are created when a specific <code>.py</code> file is executed the first time.</li> <li>Using a Starbucks coffee as an analogy, <code>.pyc</code> files are the instructions/ingredients written like code on the side of your cup that tell the barista what to make. They don't need to know the full recipe, just the ingredients. A <code>.pyc</code> file is the \"ingredients\" for a <code>.py</code> file, meant to speed up subsequent executions of the same script.</li> </ul> </li> <li>We do not want to create these files in a Dockerfile; they would affect subsequent re-builds of the container if the bytecode is cached. We want to execute the <code>.py</code> file as a <code>.py</code> file each time for reproducibility.</li> </ul> </li> <li><code>PYTHONUNBUFFERED=1</code><ul> <li>Tell Python to output <code>stdout</code> and <code>stderr</code> messages directly instead of buffering, ensuring realtime output to the container's <code>sdtdout</code>/<code>stderr</code></li> </ul> </li> <li><code>PIP_NO_CACHE_DIR=off</code><ul> <li>Tell <code>pip</code> not to cache dependencies. This would affect reproducibility in the container, and also needlessly takes up space.</li> <li>Docker will handle caching <code>pip</code> installs as a Dockerfile layer with <code>buildkit</code></li> </ul> </li> <li><code>PIP_DISABLE_PIP_VERSION_CHECK=on</code><ul> <li>Suppress warnings about <code>pip</code> being out of date in a container environment. Also for reproducibility.</li> </ul> </li> <li><code>PIP_DEFAULT_TIMEOUT=100</code><ul> <li>Some Docker connections may be slow, and when buildtime isn't a concern, it's helpful to set <code>pip</code>'s timeout value to a higher number to give it more time to finish downloading/installing a dependency.</li> </ul> </li> </ul>","tags":["templates","python","docker"]},{"location":"template/docker/python/index.html#simple-single-layer-standard-python-dockerfile","title":"Simple, single-layer \"standard\" Python Dockerfile","text":"<p>Use this container for small/simple Python projects, or as a starting point for a multistage build.</p> Python simple Dockerfile<pre><code>FROM python:3.11-slim as base\n\n## Set ENV variables to control Python/pip behavior inside container\nENV PYTHONDONTWRITEBYTECODE 1 \\\n    PYTHONUNBUFFERED 1 \\\n    ## Pip\n    PIP_NO_CACHE_DIR=off \\\n    PIP_DISABLE_PIP_VERSION_CHECK=on \\\n    PIP_DEFAULT_TIMEOUT=100\n\nWORKDIR /app\n\nCOPY requirements.txt requirements.txt\nRUN pip install -r requirements.txt\n\nCOPY ./src .\n</code></pre>","tags":["templates","python","docker"]},{"location":"template/docker/python/index.html#multistage-python-dockerfile","title":"Multistage Python Dockerfile","text":"<p>This Dockerfile is a multi-stage build, which means it uses \"layers.\" These layers are cached by the Docker <code>buildkit</code>, meaning if nothing has changed in a given layer between builds, Docker will speed up the total buildtime by using a cached layer.</p> <p>For example, the <code>build</code> layer below installs dependencies from the <code>requirements.txt</code> file. If no new dependencies are added between <code>docker build</code> commands, this layer will be re-used, \"skipping\" the <code>pip install</code> command.</p> Python multistage Dockerfile<pre><code>FROM python:3.11-slim as base\n\n## Set ENV variables to control Python/pip behavior inside container\nENV PYTHONDONTWRITEBYTECODE 1 \\\n    PYTHONUNBUFFERED 1 \\\n    ## Pip\n    PIP_NO_CACHE_DIR=off \\\n    PIP_DISABLE_PIP_VERSION_CHECK=on \\\n    PIP_DEFAULT_TIMEOUT=100\n\nFROM base AS build\n\nWORKDIR /app\n\nCOPY requirements.txt requirements.txt\nRUN pip install -r requirements.txt\n\n## Use target: dev to build this step\nFROM build AS dev\n\nENV ENV_FOR_DYNACONF=dev\n## Tell Dynaconf to always load from the environment first while in the container\nENV DYNACONF_ALWAYS_LOAD_ENV_VARS=True\n\nWORKDIR /app\nCOPY ./src .\n\n############\n# Optional #\n############\n# Export ports, set an entrypoint/CMD, etc\n#   Note: This is normally handled by your orchestrator (docker-compose, Azure Container App, etc)\n\n# EXPOSE 5000\n# CMD [\"python\", \"main.py\"]\n\n## Use target: prod to build this step\nFROM build AS prod\n\nENV ENV_FOR_DYNACONF=prod\n## Tell Dynaconf to always load from the environment first while in the container\nENV DYNACONF_ALWAYS_LOAD_ENV_VARS=True\n\nWORKDIR /app\nCOPY ./src .\n\n############\n# Optional #\n############\n# Export ports, set an entrypoint/CMD, etc\n#   Note: This is normally handled by your orchestrator (docker-compose, Azure Container App, etc)\n\n# EXPOSE 5000\n# CMD [\"python\", \"main.py\"]\n</code></pre>","tags":["templates","python","docker"]},{"location":"template/docker/python/index.html#multistage-astraluv-python-dockerfile","title":"Multistage astral/uv Python Dockerfile","text":"<p>The Astral.sh <code>uv</code> Python project manager can be used inside of a Dockerfile. In your \"base\" stage (or at the top of the Dockerfile, if you're not doing a multistage build), you can import <code>uv</code> by adding a <code>COPY</code> line to your Dockerfile.</p> Python uv Dockerfile<pre><code>FROM python:3.12-slim AS base\n## Import uv from Astral's Docker container, add uv to /bin/\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/\n\n...\n</code></pre> <p>After adding <code>uv</code> to your Dockerfile, it can be used in other stages.</p> Python multistage Dockerfile with uv<pre><code>ARG PYTHON_BASE=3.12-slim\nARG UV_BASE=0.4.27\nFROM python:$PYTHON_BASE AS base\n## Add astral.sh/uv to container's /bin/ path\nCOPY --from=ghrc.io/astral-sh/uv:$UV_BASE /uv /bin/\n\n## Set environment variables. These will be passed\n#  to stages that inherit from this layer\nENV PYTHONDONTWRITEBYTECODE 1 \\\n  PYTHONUNBUFFERED 1\n\n## Set CWD in container\nWORKDIR /project\n\n## Copy project files &amp; install with uv\nCOPY pyproject.toml uv.lock ./\nRUN uv sync --all-extras --dev &amp;&amp; \\\n  uv pip install .\n\n## Build layer to install system dependencies, copy scripts,\n#  setup container users, etc\nFROM base AS build\n\nWORKDIR /project\n\n## Install system dependencies\nRUN apt-get update -y &amp;&amp; \\\n  apt-get install -y --no-install-recommends dos2unix\n\n## Copy an entrypoint script &amp; set executable\nCOPY ./scripts/docker-entrypoint.sh ./entrypoint.sh\n## Replace line endings\nRUN sed -i 's/\\r$//g' ./entrypoint.sh &amp;&amp; \\\n  dos2unix ./entrypoint.sh &amp;&amp; \\\n  chmod +x ./entrypoint.sh\n\n## Copy remaining project files, i.e. source code\nCOPY ./src ./src\n\n## Runtime layer\nFROM build AS run\n\nCOPY --from=build /project /project\n\nWORKDIR /project\n\n## Expose a port from a service inside the container\n# EXPOSE 8000\n\n## Run a command/script inside the container\nENTRYPOINT [\"./entrypoint.sh\"]\n</code></pre>","tags":["templates","python","docker"]},{"location":"template/docker/storage/minio_s3_storage/index.html","title":"Minio S3 Storage","text":"<p>Minio is an implementation of the S3 storage type. You can self host your own S3 storage!</p>","tags":["docker","templates","storage","s3"]},{"location":"template/docker/storage/minio_s3_storage/index.html#directory-structure","title":"Directory Structure","text":"Container directory structure<pre><code>container_root/\n  ../.env\n  ../.gitignore\n  ../docker-compose.yml\n  ../data/\n    ../alert.rules\n    ../prometheus.yml\n</code></pre>","tags":["docker","templates","storage","s3"]},{"location":"template/docker/storage/minio_s3_storage/index.html#container-files","title":"Container Files","text":"","tags":["docker","templates","storage","s3"]},{"location":"template/docker/storage/minio_s3_storage/index.html#env","title":".env","text":"minio s3 storage .env<pre><code>## Default: admin\nMINIO_ROOT_USER=\n## Default: minio-devAdmin\nMINIO_ROOT_PASSWORD=\n## Default: 9000\nMINIO_COMM_PORT=\n## Default: 9001\nMINIO_WEBUI_PORT=\n## Default: ./data/minio\nMINIO_DATA_DIR=\n\n## Default: ./data/prometheus/prometheus.yml\nPROMETHEUS_CONFIG_FILE=\n## Default: ./data/prometheus/alert.rules\nPROMETHEUS_ALERTMANAGER_RULES_FILE=\n</code></pre>","tags":["docker","templates","storage","s3"]},{"location":"template/docker/storage/minio_s3_storage/index.html#gitignore","title":".gitignore","text":"minio s3 storage .gitignore<pre><code>data/*\ndata/prometheus/*\n\n!data/prometheus/\n\n!data/prometheus/example.*\n!data/prometheus/example.*.*\n\n!example.*\n!example.*.*\n!*.example\n!*.example.*\n!.*.example\n!.*.example.*\n</code></pre>","tags":["docker","templates","storage","s3"]},{"location":"template/docker/storage/minio_s3_storage/index.html#docker-composeyml","title":"docker-compose.yml","text":"minio s3 storage docker-compose.yml<pre><code>---\nservices:\n  prometheus:\n    image: prom/prometheus:latest\n    container_name: minio-prometheus\n    volumes:\n      - ${PROMETHEUS_CONFIG_FILE:-./data/prometheus/prometheus.yml}:/etc/prometheus/prometheus.yml\n      - ${PROMETHEUS_ALERTMANAGER_RULES_FILE:-./data/prometheus/alert.rules}:/alertmanager/alert.rules\n    command:\n      - \"--config.file=/etc/prometheus/prometheus.yml\"\n    ports:\n      - \"9090:9090\"\n\n  minio:\n    image: quay.io/minio/minio:latest\n    container_name: minio\n    command: server /data --console-address \":9001\"\n    restart: unless-stopped\n    environment:\n      - MINIO_ROOT_USER=${MINIO_ROOT_USER:-admin}\n      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD:-minio-devAdmin}\n      ## Should only be enabled for a local minio instance. Use bearer tokens in prod\n      - MINIO_PROMETHEUS_AUTH_TYPE=\"public\"\n      - MINIO_PROMETHUS_URL=\"prometheus:9090\"\n    ports:\n      - ${MINIO_COMM_PORT:-9000}:9000\n      - ${MINIO_WEBUI_PORT:-9001}:9001\n    volumes:\n      - ${MINIO_DATA_DIR:-./data/minio}:/data\n\n  watchtower:\n    image: containrrr/watchtower\n    container_name: minio-watchtower\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n</code></pre>","tags":["docker","templates","storage","s3"]},{"location":"template/docker/storage/minio_s3_storage/index.html#dataprometheusalertrules","title":"data/prometheus/alert.rules","text":"alert.rules<pre><code>groups:\n- name: example\n  rules:\n\n  ## Alert for any instance unreachable for &gt;5 mins\n  - alert: InstanceDown\n    expr: up == 0\n    for: 5m\n</code></pre>","tags":["docker","templates","storage","s3"]},{"location":"template/docker/storage/minio_s3_storage/index.html#dataprometheusprometheusyml","title":"data/prometheus/prometheus.yml","text":"prometheus.yml<pre><code>global:\n  scrape_interval: 15s\n\nrule_files:\n- \"/alertmanager/alert.rules\"\n\nscrape_configs:\n  - job_name: \"minio-cluster\"\n    ## This can be omitted if MinIO has env variable:\n    #    MINIO_PROMETHEUS_AUTH_TYPE=\"public\"\n    # bearer_token: TOKEN\n    metrics_path: /minio/v2/metrics/cluster\n    scheme: https\n    static_configs:\n    - targets: [\"minio:9001\"]\n\n  - job_name: \"minio-nodes\"\n    ## This can be omitted if MinIO has env variable:\n    #    MINIO_PROMETHEUS_AUTH_TYPE=\"public\"\n    # bearer_token: TOKEN\n    metrics_path: /minio/v2/metrics/node\n    scheme: https\n    static_configs:\n    - targets: [\"minio:9001\"]\n\n  - job_name: \"minio-bucket\"\n    ## This can be omitted if MinIO has env variable:\n    #    MINIO_PROMETHEUS_AUTH_TYPE=\"public\"\n    # bearer_token: TOKEN\n    metrics_path: /minio/v2/metrics/bucket\n    scheme: https\n    static_configs:\n    - targets: [\"minio:9001\"]\n\n  - job_name: \"minio-resource\"\n    ## This can be omitted if MinIO has env variable:\n    #    MINIO_PROMETHEUS_AUTH_TYPE=\"public\"\n    # bearer_token: TOKEN\n    metrics_path: /minio/v2/metrics/resource\n    scheme: https\n    static_configs:\n    - targets: [\"minio:9001\"]\n</code></pre>","tags":["docker","templates","storage","s3"]},{"location":"template/docker/storage/minio_s3_storage/index.html#notes","title":"Notes","text":"<ul> <li>This container includes <code>prometheus</code> for monitoring/logging.<ul> <li>If you use the <code>prometheus</code> container, you must create the <code>data/prometheus/</code> directory and manually create an <code>alert.rules</code> and <code>prometheus.yml</code> within.</li> </ul> </li> </ul>","tags":["docker","templates","storage","s3"]},{"location":"template/docker/storage/minio_s3_storage/index.html#links","title":"Links","text":"","tags":["docker","templates","storage","s3"]},{"location":"utilities/index.html","title":"Utilities","text":"<p>Documentation for software utilities, like terminal emulators (Alacritty), Bash utilities (<code>rsync</code>, <code>ssh</code>, etc), and more.</p> <p>Info</p> <p>Some documentation for utilities is under another section, because that is where I would expect to find it when browsing around this site. If a documentation page is nested under another section and becomes large enough to warrant moving to this path, I will do that (make note of this if you bookmark pages and the link breaks).</p>","tags":["utilities"]},{"location":"utilities/alacritty/index.html","title":"Alacritty","text":"<p>A modern terminal emulator with sensible defaults, configurable with a simple <code>alacritty.toml</code> file.</p>","tags":["utilities","terminal"]},{"location":"utilities/alacritty/index.html#installation","title":"Installation","text":"<p>Download &amp; install Alacritty using one of the methods below. More info in the Alacritty Github repository.</p> <ul> <li>Download from the homepage and install manually</li> <li>Download from the Alacritty Github releases page and install manually.</li> <li>Download &amp; install with a package manager</li> <li>Windows: <code>winget install --id=\"Alacritty.Alacritty\"</code></li> <li>Linux: <code>{apt,dnf,...} install alacritty</code></li> </ul>","tags":["utilities","terminal"]},{"location":"utilities/alacritty/index.html#configuration","title":"Configuration","text":"<p>Alacritty looks for a configuration file in the following locations, depending on OS; this file does not exist by default, you must create one if you want to configure Alacritty's defaults':</p> <ul> <li>Windows: <code>${env:APPDATA}\\alacritty\\alacritty.toml</code></li> <li>Linux/Mac: <code>~/.config/alacritty/alacritty.toml</code> OR <code>~/alacritty.toml</code></li> </ul> <p>The Alacritty site has a page dedicated to the available configuration options. Below are my personal configurations for Windows and Linux.</p> <ul> <li>\ud83d\udd17 Linux configuration</li> <li>\ud83d\udd17 Windows configuration</li> </ul>","tags":["utilities","terminal"]},{"location":"utilities/alacritty/index.html#links","title":"Links","text":"Link Description Homepage Project home Github Project source Configuration Reference <code>man</code>page-like documentation for the <code>alacritty.toml</code> config file","tags":["utilities","terminal"]},{"location":"utilities/alacritty/linux.html","title":"Alacritty on Linux","text":""},{"location":"utilities/alacritty/linux.html#alacrittytoml-config-file","title":"alacritty.toml config file","text":"<p>Configuration file: <code>~/.config/alacritty/alacritty.toml</code></p> <pre><code>## Configuration docs: https://github.com/alacritty/alacritty/blob/master/extra/man/alacritty.5.scd\n\n## Reload config on changes\nlive_config_reload = true\n## Default path for terminal\nworking_directory = \"~\"\n\n## Import themes.\n#  NOTE: You must clone the git repository or download the themes/ directory,\n#  and put it at the location in the import statement.\n#  https://github.com/alacritty/alacritty-theme\nimport = [\"~/.config/alacritty/themes/atom_one_dark.toml\"]\n\n[env]\nTERM = \"xterm-256color\"\n\n[window]\n## Set CWD as window title\ndynamic_title = true\n## Ignored when dynamic_title = true\ntitle = \"Alacritty\"\n\n## Fullscreen, Maximized, Windowed, SimpleFullscreen\nstartup_mode = \"Windowed\"\n\n## How transparent the window is; 0.0 to 1.0\nopacity = 0.95\n\n## Add additional padding evenly around terminal content\ndynamic_padding = true\npadding.x = 4\npadding.y = 0\n\n## Full, None, Transparent, Buttonless\ndecorations = \"Full\"\n## Dark, Light\ndecorations_theme_variant = \"Dark\"\n\n## Default terminal width\ndimensions.columns = 100\n## Default terminal height\ndimensions.lines = 25\n\n## On-screen dimensions for window placement on open\nposition.x = 300\nposition.y = 150\n\n[scrolling]\nhistory = 1000\nmultiplier = 3\n\n[font]\nsize = 15.0\n\noffset.x = 0\noffset.y = 0\n\nglyph_offset.x = 0\nglyph_offset.y = 0\n\n[colors]\ndraw_bold_text_with_bright_colors = true\n\n[bell]\n## \"Ease\", \"EaseOut\", \"EaseOutSine\", \"EaseOutQuad\",\n#  \"EaseOutCubic\", \"EaseOutQuart\", \"EaseOutQuint\",\n#  \"EaseOutExpo\", \"EaseOutCirc\", \"Linear\"\nanimation = \"EaseOutExpo\"\ncolor     = \"#C0C5CE\"\n## Command to run when bell is rung\ncommand   = \"None\"\n## Time in milliseconds. 0=disabled\nduration  = 0\n\n[selection]\nsave_to_clipboard = true \nsemantic_escape_chars = \",\u2502`|:\\\"' ()[]{}&lt;&gt;\\t\"\n\n[cursor]\n## Time in milliseconds between blinks, default 750\nblink_interval = 650\n## Time in seconds for cursor to \"hold\" its blink, default 5\nblink_timeout = 5\n## Thickness of cursor relative to cell width. 0.0 to 1.0, default 0.15\nthickness = 0.25\n## Render cursor as a hollow box when window loses focus\nunfocused_hollow = true\n## Block, Beam, Underline\nstyle.shape = \"Block\"\n## Never, Off, On, Always\nstyle.blinking = \"On\"\n\nvi_mode_style.shape = \"Block\"\nvi_mode_style.blinking = \"Never\"\n\n[shell]\nprogram = \"/bin/bash\"\n# args = []\n\n[[keyboard.bindings]]\naction = \"Paste\"\nkey    = \"V\"\nmods   = \"Control|Shift\"\n\n[[keyboard.bindings]]\naction = \"Copy\"\nkey    = \"C\"\nmods   = \"Control|Shift\"\n\n[[keyboard.bindings]]\naction = \"ScrollPageUp\"\nkey    = \"PageUp\"\nmods   = \"Shift\"\n\n[[keyboard.bindings]]\naction = \"ScrollPageDown\"\nkey    = \"PageDown\"\nmods   = \"Shift\"\n\n[[keyboard.bindings]]\naction = \"ScrollToTop\"\nkey    = \"Home\"\nmods   = \"Shift\"\n\n[[keyboard.bindings]]\naction = \"ScrollToBottom\"\nkey    = \"End\"\nmods   = \"Shift\"\n\n[[keyboard.bindings]]\nkey = \"Return\"\nmods = \"Control|Shift\"\naction = \"SpawnNewInstance\"\n\n[[keyboard.bindings]]\naction = \"IncreaseFontSize\"\nkey = \"Equals\"\nmods = \"Control\"\n\n[[keyboard.bindings]]\naction = \"DecreaseFontSize\"\nkey = \"Minus\"\nmods = \"Control\"\n\n[[keyboard.bindings]]\naction = \"ClearLogNotice\"\nkey = \"L\"\nmods = \"Control\"\n\n[[keyboard.bindings]]\nchars = \"\\f\"\nkey = \"L\"\nmods = \"Control\"\n\n[mouse]\nhide_when_typing = true\n\n[[mouse.bindings]]\naction = \"PasteSelection\"\nmouse = \"Middle\"\n\n[debug]\nhighlight_damage   = false\nlog_level          = \"Warn\"\npersistent_logging = false\nprint_events       = false\nrender_timer       = false\n\n[[hints.enabled]]\nregex = \"[^ ]+\\\\.rs:\\\\d+:\\\\d+\"\ncommand = { program = \"code\", args = [ \"--goto\" ] }\nmouse = { enabled = true }\n</code></pre>"},{"location":"utilities/alacritty/themes.html","title":"Alacritty Themes","text":"<p>You can customize the appearance/theme of Alacritty by cloning the alacrity-theme Github repository.</p>"},{"location":"utilities/alacritty/themes.html#install-themes","title":"Install themes","text":"<ul> <li>Create a directory <code>~/.config/alacritty</code></li> <li><code>cd</code> into the newly created directory and clone the themes repository</li> <li><code>cd ~/.config/alacritty</code></li> <li><code>git clone https://github.com/alacritty/alacritty-theme .</code></li> </ul>"},{"location":"utilities/alacritty/themes.html#configure-alacritty-theme","title":"Configure Alacritty theme","text":"<ul> <li>Edit your <code>alacritty.toml</code> file</li> <li>At the top of the file, add an <code>import</code> statement with the path to a theme in the <code>themes/</code> directory of the cloned repository</li> <li>Example: <code>import = [\"~/.config/alacritty/themes/atom_one_dark.toml\"]</code></li> </ul> <p>Visit the Alacritty Github for all themes &amp; previews.</p>"},{"location":"utilities/alacritty/windows.html","title":"Alacritty on Windows","text":""},{"location":"utilities/alacritty/windows.html#alacrittytoml-config-file","title":"alacritty.toml config file","text":"<p>Configuration file: <code>%APPDATA%\\alacritty\\alacritty.toml</code></p> <pre><code>## Configuration docs: https://github.com/alacritty/alacritty/blob/master/extra/man/alacritty.5.scd\n\n## Reload config on changes\nlive_config_reload = true\n## NOTE: env vars like %APPDATA% or ${env:APPDATA} do NOT work here\nworking_directory = \"c:\\\\\"\n\n## Import themes.\n#  NOTE: You must clone the git repository or download the themes/ directory,\n#  and put it at the location in the import statement.\n#  https://github.com/alacritty/alacritty-theme\nimport = [\"~/.config/alacritty/themes/atom_one_dark.toml\"]\n\n[env]\nTERM = \"alacritty\"\n\n[window]\n## Set CWD as window title\ndynamic_title = true\n## Ignored when dynamic_title = true\ntitle = \"Alacritty\"\n\n## Fullscreen, Maximized, Windowed, SimpleFullscreen\nstartup_mode = \"Windowed\"\n\n## How transparent the window is; 0.0 to 1.0\nopacity = 0.95\n\n## Add additional padding evenly around terminal content\ndynamic_padding = true\npadding.x = 4\npadding.y = 0\n\n## Full, None, Transparent, Buttonless\ndecorations = \"Full\"\n## Dark, Light\ndecorations_theme_variant = \"Dark\"\n\n## Default terminal width\ndimensions.columns = 100\n## Default terminal height\ndimensions.lines = 25\n\n## On-screen dimensions for window placement on open\nposition.x = 300\nposition.y = 150\n\n[scrolling]\nhistory = 1000\nmultiplier = 3\n\n[font]\nsize = 15.0\n\noffset.x = 0\noffset.y = 0\n\nglyph_offset.x = 0\nglyph_offset.y = 0\n\n[colors]\ndraw_bold_text_with_bright_colors = true\n\n[bell]\n## \"Ease\", \"EaseOut\", \"EaseOutSine\", \"EaseOutQuad\",\n#  \"EaseOutCubic\", \"EaseOutQuart\", \"EaseOutQuint\",\n#  \"EaseOutExpo\", \"EaseOutCirc\", \"Linear\"\nanimation = \"EaseOutExpo\"\ncolor     = \"#C0C5CE\"\n## Command to run when bell is rung\ncommand   = \"None\"\n## Time in milliseconds. 0=disabled\nduration  = 0\n\n[selection]\nsave_to_clipboard = true \nsemantic_escape_chars = \",\u2502`|:\\\"' ()[]{}&lt;&gt;\\t\"\n\n[cursor]\n## Time in milliseconds between blinks, default 750\nblink_interval = 650\n## Time in seconds for cursor to \"hold\" its blink, default 5\nblink_timeout = 5\n## Thickness of cursor relative to cell width. 0.0 to 1.0, default 0.15\nthickness = 0.25\n## Render cursor as a hollow box when window loses focus\nunfocused_hollow = true\n## Block, Beam, Underline\nstyle.shape = \"Block\"\n## Never, Off, On, Always\nstyle.blinking = \"On\"\n\nvi_mode_style.shape = \"Block\"\nvi_mode_style.blinking = \"Never\"\n\n[shell]\n## Windows Powershell 1.0\nprogram = \"powershell\"\n# args = []\n\n## Windows PowerShell Core (7)\n# program = \"C:\\\\Program Files\\\\PowerShell\\\\7\\\\pwsh.exe\"\n# args = []\n\n[[keyboard.bindings]]\naction = \"Paste\"\nkey    = \"V\"\nmods   = \"Control|Shift\"\n\n[[keyboard.bindings]]\naction = \"Copy\"\nkey    = \"C\"\nmods   = \"Control|Shift\"\n\n[[keyboard.bindings]]\naction = \"ScrollPageUp\"\nkey    = \"PageUp\"\nmods   = \"Shift\"\n\n[[keyboard.bindings]]\naction = \"ScrollPageDown\"\nkey    = \"PageDown\"\nmods   = \"Shift\"\n\n[[keyboard.bindings]]\naction = \"ScrollToTop\"\nkey    = \"Home\"\nmods   = \"Shift\"\n\n[[keyboard.bindings]]\naction = \"ScrollToBottom\"\nkey    = \"End\"\nmods   = \"Shift\"\n\n[[keyboard.bindings]]\nkey = \"Return\"\nmods = \"Control|Shift\"\naction = \"SpawnNewInstance\"\n\n[[keyboard.bindings]]\naction = \"IncreaseFontSize\"\nkey = \"Equals\"\nmods = \"Control\"\n\n[[keyboard.bindings]]\naction = \"DecreaseFontSize\"\nkey = \"Minus\"\nmods = \"Control\"\n\n[[keyboard.bindings]]\naction = \"ClearLogNotice\"\nkey = \"L\"\nmods = \"Control\"\n\n[[keyboard.bindings]]\nchars = \"\\f\"\nkey = \"L\"\nmods = \"Control\"\n\n[mouse]\nhide_when_typing = true\n\n[[mouse.bindings]]\naction = \"PasteSelection\"\nmouse = \"Middle\"\n\n[debug]\nhighlight_damage   = false\nlog_level          = \"Warn\"\npersistent_logging = false\nprint_events       = false\nrender_timer       = false\n\n[[hints.enabled]]\nregex = \"[^ ]+\\\\.rs:\\\\d+:\\\\d+\"\ncommand = { program = \"code\", args = [ \"--goto\" ] }\nmouse = { enabled = true }\n</code></pre>"},{"location":"utilities/ansible/index.html","title":"Ansible","text":"<p>Warning</p> <p>These docs are incomplete, with large gaps in explanations &amp; examples.</p> <p>Until this message is removed, you will most likely be better reading through the Ansible docs and blog entries you can find online.</p> <p>I am basing these docs off my Ansible repository for my homelab.</p> <p>Ansible is a very useful tool for automating your infrastructure. It is simple to install, and runs over SSH, making it flexible &amp; portable (if you keep all of your roles/collections/playbooks and Ansible configs in the same directory).</p> <p>Info</p> <p>There is more than 1 way to structure an Ansible repository, call roles/playbooks, and use the tool. These docs are written around the way I personally use Ansible.</p> <p>Some concepts, like collections and roles, are pretty universal no matter how you end up structuring your Ansible project.</p>","tags":["ansible","automation","ssh"]},{"location":"utilities/ansible/index.html#install","title":"Install","text":"<p>The Ansible installation docs are a good place to read more about the different installation methods for Ansible. I personally install it as a Python module. I initialize a project, i.e. <code>mkdir ansible_project &amp;&amp; cd ansible_project &amp;&amp; git init -b main</code>, initialize a Python project with <code>pdm init</code> (if I'm using <code>pdm</code>) or <code>uv init</code> (if I'm using <code>uv</code>), and add Ansible dependencies.</p> <p>Whether you use a Python project manager, a virtual environment, or <code>pipx</code>, the Python dependencies I install for Ansible are:</p> <ul> <li><code>ansible-core</code><ul> <li>The core Ansible package</li> <li>Includes <code>ansible-playbook</code> and <code>ansible-galaxy</code> commands</li> </ul> </li> <li>(Optional) <code>ansible-runner</code><ul> <li>Once you get more comfortable with Ansible, you may want to have more control over execution of playbooks.</li> <li><code>ansible-runner</code> lets you structure your repository a specific way and create Python files that describe automations for the runner to execute.</li> <li>The runner is highly portable, it has a CLI for local execution, can be run as a container, and is able to be run in CI pipelines</li> <li><code>ansible-runner</code> is a more advanced tool. Learn the basics of Ansible before trying to add it into your workflow.</li> </ul> </li> </ul> <p>I also add the following dev dependencies (i.e. <code>pdm add -d &lt;pkg&gt;</code> or <code>uv add --dev &lt;pkg&gt;</code>):</p> <ul> <li><code>passlib</code><ul> <li>Handle secrets &amp; keys in your Ansible repository</li> </ul> </li> <li><code>yamllint</code><ul> <li>Check &amp; lint your Ansible <code>.yml</code> files</li> </ul> </li> </ul>","tags":["ansible","automation","ssh"]},{"location":"utilities/ansible/index.html#project-setup","title":"Project Setup","text":"<p>Note</p> <p>I use Ansible by keeping all of configurations, plays, roles, collections, etc under a single directory. I can then turn the directory into a Git repository, push it to Github/Gitlab/etc, and have a \"portable\" copy of the Ansible project.</p> <p>If you set your project up a different way, you may need to adjust some steps in the tutorials on this site to fit your environment.</p> <p>This tutorial assumes you are storing your full Ansible project (configuration, playbooks, roles, collections, inventories, etc) under a single directory, named <code>ansible-repository/</code></p>","tags":["ansible","automation","ssh"]},{"location":"utilities/ansible/index.html#root-directory-setup-python-install","title":"Root directory setup &amp; Python install","text":"<p>Note</p> <p>You do not need to install Python to follow this guide. Installing <code>uv</code> is sufficient, as the <code>uv</code> tool can manage Python versions for you.</p> <ul> <li>Create your project directory<ul> <li><code>mkdir ansible-repository &amp;&amp; cd ansible-repository</code></li> </ul> </li> <li>Initialize the project<ul> <li><code>git init -b main</code></li> <li>(Astral uv) <code>uv init</code><ul> <li>Remove the <code>hello.py</code> file that <code>uv</code> generates</li> <li><code>uv add ansible-core</code></li> <li><code>uv add --dev passlib yamllint</code></li> </ul> </li> <li>(PDM) <code>pdm init</code><ul> <li><code>pdm add ansible-core</code></li> <li><code>pdm add -d passlib yamllint</code></li> </ul> </li> <li>(virtualenv) <code>virtualenv .venv &amp;&amp; .venv/{bin,Scripts}/activate</code><ul> <li><code>pip install ansible-core passlib yamllint</code></li> </ul> </li> </ul> </li> <li>Create a <code>.gitignore</code> file so you don't accidentally commit anything sensitive/secret<ul> <li>This <code>.gitignore</code> at the root of the repository will apply to all directories in the repository</li> <li>This is useful for mass-ignoring files like <code>.env</code> or the <code>.vault</code> directory when using Ansible Vault</li> </ul> </li> </ul> Ansible .gitignore<pre><code>## Python\n__pycache__/\n*.py[cod]\n.python-version\n*.egg\n*.egg-info\ndist/\nbuild/\neggs/\n.eggs/\n\n## Environments\n.env\n*.env\n*.*.env\n.venv/\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n## Allow Environment patterns\n!*example*\n!*example*.*\n!*.*example*\n!*.*example*.*\n!*.*.*example*\n!*.*.*example*.*\n\n## Ansible\n.vault/\n.ansible/\n</code></pre>","tags":["ansible","automation","ssh"]},{"location":"utilities/ansible/index.html#ansible-setup","title":"Ansible setup","text":"<ul> <li>Create an inventory</li> </ul>","tags":["ansible","automation","ssh"]},{"location":"utilities/ansible/inventories.html","title":"Inventories","text":"<p>Inventories instruct Ansible on how to interact with your infrastructure, describing things like the path to a Python interpreter on the remote host, the SSH user Ansible should connect as and which SSH key to use, variables for your playbooks and roles, as well as custom/arbitrary variable declaration for things like software versions (if a play/role installs a specific version of a piece of software), secrets/tokens, &amp; more.</p> <p>Variables are declared in a directory alongside the inventory file, <code>group_vars/all.yml</code>. You can add other <code>.yml</code> files for variables, and Ansible will \"join\" them all when running plays.</p> <p>Note</p> <p>There are multiple ways to create your inventory. Ansible supports inventory files described in <code>ini</code> and <code>yaml</code>.</p> <p>I have used both, but more recently prefer <code>yaml</code> for describing my inventory. This guide assumes you will also use <code>yaml</code> inventories, but if you choose to use <code>ini</code> instead, you can translate the configurations by referencing the Ansible documentation.</p>","tags":["ansible","automation"]},{"location":"utilities/ansible/inventories.html#example-inventories","title":"Example inventories","text":"","tags":["ansible","automation"]},{"location":"utilities/ansible/inventories.html#example-simple-inventory","title":"Example: simple inventory","text":"Simple Ansible inventory<pre><code>---\n## A group for all machines in this inventory.\n#  You can call these hosts in groups within this inventory after declaring here\nall:\n  ## Describe your hosts\n  host1:\n    ## Set the host IP/FQDN Ansible will use to connect\n    ansible_host: \"192.168.1.xxx\"\n  host2:\n    ansible_host: \"192.168.1.xxx\"\n    ## Override the remote machine's user Ansible will run as\n    ansible_user: \"username\"\n  host3:\n    ansible_host: \"192.168.1.xxx\"\n    ## Override the example_var_int value for just this host\n    example_var_int: 42\n\n  ## Vars that apply to all hosts.\n  #  Can be overridden at the host definition or group level\n  vars:\n    ## Set variables Ansible will pass to plays/roles when this inventory is used\n    #  Call in a playbook/role with {{ example_var }} or {{ example_var_int }}\n    example_var: \"I'm an example string\"\n    example_var_int: 100\n\n## Group hosts declared in 'all' for controlling execution\n#  with ansible-playbook -i inventory.yml --limit &lt;group-name&gt;\ndebian:\n  ## You can just use the hostname from 'all'. Any vars declared at this level\n  #  will override values set in 'all' definition\n  host1:\n  host3:\n    example_var: \"My value is now this string, and example_var_int will be 42 because it was overridden in the 'all' group\"\n\n## If you use a service like DigitalOcean, you can create a group for those hosts\ndigitalOcean:\n  host2:\n    ## You can add variables you haven't declared yet\n    example_bool: true\n</code></pre>","tags":["ansible","automation"]},{"location":"utilities/ansible/ssh.html","title":"SSH Setup","text":"<p>Ansible uses SSH to connect to remote machines and run tasks. It is advisable to create an SSH key just for executing Ansible playbooks, and an Ansible service account on the remote, i.e. <code>ansible_svc</code>. This user account is not meant to be logged into on the remote, but can be granted permissions so Ansible can run and do things like install software, update packages, etc.</p> <p>Creating a service account for Ansible to use is easily automated. I always create an <code>onboard</code> inventory, which is where I declare machines temporarily to run any base/common setup steps, including creating my Ansible service user.</p> <p>After running my onboarding playbook, I remote the host from the inventory file and add it to whatever other \"permanent\" inventories it will exist in. This step allows me to quickly set up a new host by passing an SSH password for when I have not already copied my Ansible SSH keys.</p>","tags":["ansible","automation","ssh"]},{"location":"utilities/ansible/ssh.html#create-ssh-key","title":"Create SSH key","text":"<p>Create an SSH key just for Ansible execution. You will copy the public key to your remote manually, then Ansible will use this key to authenticate.</p> Create Ansible SSH key<pre><code>ssh-keygen -t rsa -b 4096 -f ~/.ssh/ansible_id_rsa -N \"\"\n</code></pre> <p>This will create 2 keys, <code>~/.ssh/ansible_id_rsa</code> (the private key, do not copy this to the remote), and <code>~/.ssh/ansible_id_rsa.pub</code> (this is the key you copy to the remote).</p> <p>In your <code>~/.ssh/config</code>, add your hosts by the same name you use in your Ansible inventory file. </p>","tags":["ansible","automation","ssh"]},{"location":"utilities/ansible/ssh.html#playbook-create-ansible_svc-user","title":"Playbook: Create ansible_svc user","text":"plays/create_ansible_svc_user.yml<pre><code>---\n- name: \"Setup service account for Ansible\"\n  hosts: all\n\n  vars:\n    service_account_name: \"{{ ansible_svc_account_name | default('ansible_svc') }}\"\n    service_account_shell: \"{{ ansible_svc_account_shell | default('/bin/bash') }}\"\n    local_\n\n  tasks:\n    - name: \"Gather facts\"\n      ansible.builtin.package_facts:\n        manager: auto\n</code></pre>","tags":["ansible","automation","ssh"]},{"location":"utilities/logrotate/index.html","title":"Logrotate","text":"<p>Logrotate is a Linux utility for rotating logs by reading policies defined in <code>/etc/logrotate.d/</code>. You can read more about setting up logrotate in this RedHat blog entry.</p> <p>You can create a logrotate policy for pretty much any file you output logs to, but it is recommended to store your logs in <code>/var/log/</code> or a subdirectory in that path, i.e. <code>/var/log/program_name/some_logfile.log</code>.</p> <p>Logrotate policies should be defined in files in the <code>/etc/logrotate.d/</code> directory.</p> <p>Tip</p> <p>When creating a logrotate policy file, choose a name that describes the log file it applies to, and create the file at the root of <code>/etc/logrotate.d/</code>, don't use subdirectories. There are ways to do that, but unless you have trouble maintaining your policies in a 'flat' structure, logrotate will not detect your nested policies.</p> <p>An example of a logrotate policy that rotates <code>/var/log/some_program/function.log</code> could be named <code>/etc/logrotate.d/some_program_function</code>.</p> <p>You do not need to put a file extension in the filename.</p>","tags":["linux","logging","utilities"]},{"location":"utilities/logrotate/index.html#policy-file","title":"Policy file","text":"<p>A policy file defined at <code>/etc/logrotate.d/policy_filename</code> should start with the path to the file it applies to. For example, a policy that rotates a log at <code>/var/log/program_name/function.log</code> would start with:</p> logrotate policy file path<pre><code>/var/log/program_name/function.log {}\n</code></pre> <p>You then add your configurations in the <code>{}</code> mapping.</p> <p>For example, a logrotate policy that:</p> <ul> <li>rotates the file daily, keeping the last 14 rotated logs</li> <li>compresses each rotated file with <code>gzip</code>, but delays the most recently compressed logfile (for easy access of the most recently rotated log file)</li> <li>does not error if the log file is missing</li> <li>skips if the log file's contents are empty</li> <li>recreates the file after rotation with <code>chmod 0640</code> and <code>chown root:adm</code></li> </ul> Example logrotate policy<pre><code>/var/log/program_name/function.log {\n    daily\n    rotate 14\n    compress\n    delaycompress\n    missingok\n    notifempty\n    create 0640 root adm\n}\n</code></pre>","tags":["linux","logging","utilities"]},{"location":"utilities/logrotate/index.html#examples","title":"Examples","text":"","tags":["linux","logging","utilities"]},{"location":"utilities/logrotate/index.html#backup-home-directory-with-logging","title":"Backup home directory with logging","text":"<p>For this example, let's say you have a cron job that runs every 6 hours and creates a backup of your home directory. The job outputs its logs to <code>/var/log/home_backup/backup.log</code>. You want to rotate this file at 10MB, or every week (whichever occurs first), retaining 2 weeks worth of backups, and recreate the logfile once it's rotated.</p> <p>You create logrotate policies in <code>/etc/logrotate.d/&lt;logrotate_policy_name&gt;</code>. The file can be named whatever you want, but it's advisable to make the name of the file descriptive of the log file it's rotating. You can omit a file extension from the file name.</p> <p>Create a file at <code>/etc/logrotate.d/home_dir_backup</code>:</p> <pre><code>/var/log/home_backup.log {\n    size 10M           # Rotate if log file reaches 10MB\n    weekly             # Or rotate weekly, whichever happens first\n    rotate 14          # Keep 14 rotated logs (2 weeks worth)\n    compress           # Compress rotated logs with gzip\n    delaycompress      # Delay compression until next rotation cycle\n    missingok          # Don't issue error if log file is missing\n    notifempty         # Skip rotation if log file is empty\n    create 0640 root adm  # Recreate log file with correct permissions and ownership. You can also use $USER for the user and group\n}\n</code></pre> <p>Now, if you schedule a cron job like this, the log file will automatically rotate:</p> <pre><code>0 */6 * * * cp -R /home/username /backup/homedir &gt; /var/log/home_backup.log 2&gt;&amp;1  # this is the path you should use in the logrotate policy\n</code></pre>","tags":["linux","logging","utilities"]},{"location":"utilities/ntfy/index.html","title":"Ntfy","text":"<p>Ntfy is a PUB/SUB for sending/receiving push notifications. It uses simple HTTP requests, making it highly flexible and portable, and can be self-hosted easily with Docker.</p>","tags":["http","utilities","bash","powershell","python"]},{"location":"utilities/ntfy/index.html#todo","title":"TODO","text":"<ul> <li> Break this file up into smaller sections</li> <li> Add example Bash scripts for controlling Dockerized ntfy instance</li> </ul>","tags":["http","utilities","bash","powershell","python"]},{"location":"utilities/ntfy/index.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>TODO</li> <li>Quick How-To</li> <li>Basic notification</li> <li>Title, priority, and tags</li> <li>Style messages with emoji \\&amp; tags</li> <li>Style messages with Markdown</li> <li>Attachments</li> <li>Action buttons</li> <li>Priorities</li> <li>Scheduled Messages</li> <li>Send as GET requests with webhooks</li> <li>Publish as JSON</li> <li>Links</li> </ul>","tags":["http","utilities","bash","powershell","python"]},{"location":"utilities/ntfy/index.html#quick-how-to","title":"Quick How-To","text":"<p>Ntfy works off of HTTP requests. This makes it highly flexible, allowing you to choose which HTTP client you use. Throughout this page, code examples have a header with tabs indicating a language to show the example in, i.e. <code>Command line (curl)</code>, <code>ntfy CLI</code>, <code>HTTP</code>, etc.</p> <p>This section assumes you are using Bash + cURL. It should be easy to translate the examples if you're using another tool or library. In cURL, <code>-d</code> is the \"data\" to send (i.e. the message), <code>-H</code> is a header, and <code>-u</code> is for API token auth.</p> <p>An Ntfy request needs, at minimum, the data/message to send and a URL to your topic, i.e. <code>ntfy.sh/topicName</code>. If you are self-hosting ntfy, replace any <code>ntfy.sh</code> domain in this section with your domain, i.e. <code>ntfy.example.com</code>.</p> <p>This page as a full list of supported parameters you can pass in a request.</p>","tags":["http","utilities","bash","powershell","python"]},{"location":"utilities/ntfy/index.html#basic-notification","title":"Basic notification","text":"<p>This is a very simply request that sends a message \"Hello, world!\" to the <code>greetings</code> topic on Ntfy's official PUBSUB instance.</p> <pre><code>curl -d \"Hello, world!\" ntfy.sh/greetings\n</code></pre>","tags":["http","utilities","bash","powershell","python"]},{"location":"utilities/ntfy/index.html#title-priority-and-tags","title":"Title, priority, and tags","text":"<p>To set the title, priority, and tags properties, pass them as headers to cURL with <code>-H \"&lt;key&gt;: &lt;value&gt;\"</code>. You can optionally pass them as <code>X-&lt;Key&gt;</code>.</p> <p>Priorities range from 1 to 5, with 1 being the lowest and 5 being the highest priority.</p> <pre><code>curl \\\n  -H \"X-Title: This is a notification title\" \\\n  -H \"X-Priority: urgent\" \\\n  -H \"X-Tags: warning,skull\" \\\n  -d \"This is an urgent alert!\" \\\n  ntfy.sh/urgent-alerts\n</code></pre> <p>You can do multi-line messages, too.</p> <pre><code>curl \\\n  -H \"X-Title: Multiple lines?\" \\\n  -d \"This message will have multiple lines.\n\nJust add newlines without closing the quote,\n\nsplitting the content over multiple lines.\"\n  ntfy.sh/multiple-lines\n</code></pre>","tags":["http","utilities","bash","powershell","python"]},{"location":"utilities/ntfy/index.html#style-messages-with-emoji-tags","title":"Style messages with emoji &amp; tags","text":"<p>See the ntfy tags &amp; emojis docs</p> <p>You can pass an <code>X-Tags</code> parameter to add tags to a notification.</p> <pre><code>curl -H \"X-Tags: warning,somehostname,job-name\" \\\n  -d \"Job name failed on host: somehostname\" \\\n  ntfy.sh/backups\n</code></pre> <p>You can also pass them with <code>-H ta:tagname</code> (no quotes).</p>","tags":["http","utilities","bash","powershell","python"]},{"location":"utilities/ntfy/index.html#style-messages-with-markdown","title":"Style messages with Markdown","text":"<p>ntfy docs: Markdown formatting</p> <pre><code>curl \\\n  -H \"X-Markdown: yes\" \\\n  -d \"This message is *styled*! **Big and bold**.\n\n&gt; Let he who is without blame throw the first stone.\n\n![meaning image tag name](url-to-file)\n\n```python\nprint(f\"This is a code block in a message!\")\n```\n\n[This is a link to Ntfy](https://ntfy.sh)\n\nThings to do:\n\n- Make some requests of your own!\n  - Learn Ntfy syntax\n  - Pick a tool\n    - cURL\n    - Powershell Invoke-WebRequest\n    - Python httpx/requests\n  - Create some notifications!\n\n---\n\nThat's all, folks.\"\n\n  ntfy.sh/markdown-message\n</code></pre>","tags":["http","utilities","bash","powershell","python"]},{"location":"utilities/ntfy/index.html#attachments","title":"Attachments","text":"<p>Ntfy attachments documentation</p>","tags":["http","utilities","bash","powershell","python"]},{"location":"utilities/ntfy/index.html#action-buttons","title":"Action buttons","text":"<p>Ntfy action buttons documentation</p>","tags":["http","utilities","bash","powershell","python"]},{"location":"utilities/ntfy/index.html#priorities","title":"Priorities","text":"Priority ID Name Description Max 5 <code>max</code>/<code>urgent</code> Really long vibration bursts, default notification sound with a pop-over notification. High 4 <code>high</code> Long vibration burst, default notification sound with a pop-over notification. Default 3 <code>default</code> Short default vibration and sound. Default notification behavior. Low 2 <code>low</code> No vibration or sound. Notification will not visibly show up until notification drawer is pulled down. Min 1 <code>min</code> No vibration or sound. The notification will be under the fold in \"Other notifications\". <p>Examples:</p> <pre><code>curl -H \"X-Priority: 5\" -d \"An urgent message\" ntfy.sh/alerts\ncurl -H \"Priority: low\" -d \"Low priority message\" ntfy.sh/alerts\ncurl -H p:4 -d \"A high priority message\" ntfy.sh/alerts\n</code></pre>","tags":["http","utilities","bash","powershell","python"]},{"location":"utilities/ntfy/index.html#scheduled-messages","title":"Scheduled Messages","text":"<p>Ntfy scheduled message docs</p> <p>If message caching is enabled on the server, you can pass a header with syntax describing when the message should be sent. Header options are <code>X-Delay</code>, <code>X-At</code>, <code>X-In</code>.</p> <p>Examples:</p> <pre><code>curl -H \"At: tomorrow, 10am\" -d \"Good morning\" ntfy.sh/hello\ncurl -H \"In: 30min\" -d \"It's 30 minutes later now\" ntfy.sh/reminder\ncurl -H \"Delay: 1639194738\" -d \"Unix timestamps are awesome\" ntfy.sh/itsaunixsystem\n</code></pre>","tags":["http","utilities","bash","powershell","python"]},{"location":"utilities/ntfy/index.html#send-as-get-requests-with-webhooks","title":"Send as GET requests with webhooks","text":"<p>Ntfy webhooks docs</p>","tags":["http","utilities","bash","powershell","python"]},{"location":"utilities/ntfy/index.html#publish-as-json","title":"Publish as JSON","text":"<p>Ntfy JSON publishing docs</p> <p>In some instances, you may not have control over the headers of a request, such as with Jellyfin. You can also pass your full notification as JSON. The example below uses most of thee available parameters. The only required parameter is <code>topic</code>.</p> <pre><code>curl ntfy.sh \\\n  -d '{\n    \"topic\": \"alerts\",\n    \"message\": \"Disk space is low at 5.1 GB\",\n    \"title\": \"Low disk space alert\",\n    \"tags\": [\"warning\",\"cd\"],\n    \"priority\": 4,\n    \"attach\": \"https://filesrv.lan/space.jpg\",\n    \"filename\": \"diskspace.jpg\",\n    \"click\": \"https://homecamera.lan/xasds1h2xsSsa/\",\n    \"actions\": [{ \"action\": \"view\", \"label\": \"Admin panel\", \"url\": \"https://filesrv.lan/admin\" }]\n  }'\n</code></pre>","tags":["http","utilities","bash","powershell","python"]},{"location":"utilities/ntfy/index.html#links","title":"Links","text":"<ul> <li>ntfy home</li> <li>ntfy Github</li> <li>ntfy docs</li> <li>ntfy docs: publishing (sending messages)</li> <li>ntfy docs: emoji shortcodes</li> <li>ntfy docs: list of all parameters</li> </ul>","tags":["http","utilities","bash","powershell","python"]},{"location":"utilities/ollama/index.html","title":"ollama","text":"<p>Ollama is an open-source app that lets you create, run, and share large language models (LLMs) locally with a CLI for Mac and Linux. You can also install ollama on Windows, but Unix support is better (as of 11/11/24).</p> <p>The utility is developed by Meta (I know...but they've developed a lot of great open source tools like the React.js framework), but Meta keeps their grubby paws off your data. All LLMs executed with ollama are local-only and private.</p>","tags":["utilities","llm"]},{"location":"utilities/ollama/index.html#system-requirements","title":"System Requirements","text":"<p>LLMs require a lot of power to run. Ollama uses weights differently from LLMs like chatGPT, shrinking the model size and enabling them to be run on (powerful enough) home devices.</p> <p>Warning</p> <p>Just because ollama is lighter than other LLMs does not mean it is \"light.\" While ollama can run on a regular CPU, it's much better to have a dedicated Graphics card with at least 6GB of VRAM. The \"heavier\" the model you wish to use with ollama, the more system resources you will need.</p> <p>Ollama's Github has a page listing supported GPUs so you can quickly check if yours is supported and ollama will run without issue, or if you'll have to struggle and optimize to get this working on your device.</p> <p>I was not able to find an official source for system requirements, but the table below is often cited as the minimum requirements to run an ollama local server:</p> Resource Minimum Required Notes OS Linux: Ubuntu 18.04 or later, macOS: macOS 11 Big Sur or later There is technically Windows support, but ollama runs best on Unix. RAM 8GB for running 3B models, 16GB for running 7B models, 32GB for running 13B models The more the better. Models are loaded into RAM, and very large models (like dolphin-mixtral, which is ~26GB) will crash without sufficient memory. Storage 12GB for installing Ollama and the base models, Additional space required for storing model data, depending on the models you use. The more storage the better, especially if you plan to experiment with a lot of different models. These things are big. CPU Any modern CPU with at least 4 cores is recommended, for running 13B models, a CPU with at least 8 cores is recommended. ollama is less efficient running via CPU than GPU, make sure you have a decently powerful CPU if going this route. GPU(Optional) Guide to help you pick a compatible CPU A GPU is not required for running Ollama, but it can improve performance, especially for running larger models. If you have a GPU, you can use it to accelerate training of custom models.","tags":["utilities","llm"]},{"location":"utilities/ollama/index.html#installing-ollama","title":"Installing ollama","text":"<ul> <li>You can download ollama right from their website.</li> <li>On Linux, you can simple use this command:</li> </ul> Install/upgrade ollama on Linux<pre><code>curl -fsSL https://ollama.com/install.sh | sh\n</code></pre>","tags":["utilities","llm"]},{"location":"utilities/ollama/index.html#choosing-a-model","title":"Choosing a model","text":"<p>You can browse available models on ollama's website. You can install multiple models side-by-side and switch between them at will, you're really only limited to how many models you can download by the size of your disk.</p> <p>Once you've installed ollama, you can download a model by running:</p> Download an ollama model<pre><code>ollama pull &lt;model-name&gt;\n</code></pre> <p>For example, to get started with the <code>llama3.2</code> model (current as of 11/11/24), you can run:</p> Download llama3.2 model<pre><code>ollama pull llama3.2\n</code></pre> <p>To run your new model, open your CLI and run (the model will be downloaded if you have not already run <code>ollama pull</code>):</p> Run an ollama model<pre><code>ollama run &lt;model-name&gt;\n</code></pre>","tags":["utilities","llm"]},{"location":"utilities/ollama/index.html#ollama-control-script","title":"Ollama control script","text":"<p>This Bash script can help manage ollama. The script includes the following arguments:</p> <ul> <li><code>start</code>: Start the ollama service &amp; server</li> <li><code>stop</code>: Stop the ollama server &amp; stop/disable the service</li> <li><code>install</code>: Install the ollama server if you have not already</li> <li><code>upgrade</code>: Upgrading ollama is as simple as re-running the install script. This script takes care of that for you if you use this argument</li> </ul> ollama_ctl.sh<pre><code>#!/bin/bash\n\n## ollama systemd service name\nSERVICE_NAME=\"ollama\"\n## pid of this script (avoid terminating this script when ./ollama_ctl.sh stop is called)\nCURRENT_SCRIPT_PID=$$\n\n## Ensure at least 1 command was run\nif [ $# -ne 1 ]; then\n    echo \"Usage: $0 {start|stop|install|update}\"\n    exit 1\nfi\n\n## Assign user's CLI arg to a variable\nCOMMAND=$1\n\ncase $COMMAND in\n    start)\n        ## Start the ollama service\n        echo \"Attempting to start the ${SERVICE_NAME} service...\"\n\n        ## Check if ollama service is already running\n        if systemctl is-active --quiet \"${SERVICE_NAME}\"; then\n            echo \"${SERVICE_NAME} service is already running.\"\n        else\n            ## Start ollama service\n            echo \"Starting and enabling ${SERVICE_NAME} service...\"\n            sudo systemctl enable \"${SERVICE_NAME}\"\n            sudo systemctl start \"${SERVICE_NAME}\"\n\n            if systemctl is-active --quiet \"${SERVICE_NAME}\"; then\n                echo \"${SERVICE_NAME} service started successfully.\"\n            else\n                ## ollama startup failed\n                echo \"Failed to start ${SERVICE_NAME} service. Check system logs for details.\"\n                exit 1\n            fi\n        fi\n        ;;\n    stop)\n        ## Stop the ollama service\n        echo \"Attempting to stop the ${SERVICE_NAME} service and any running 'ollama' processes...\"\n\n        ## Stop &amp; disable systemd service\n        if systemctl is-active --quiet \"${SERVICE_NAME}\"; then\n            echo \"Stopping ${SERVICE_NAME} service...\"\n            sudo systemctl stop \"${SERVICE_NAME}\"\n            sudo systemctl disable \"${SERVICE_NAME}\"\n        else\n            ## ollama service is not running\n            echo \"${SERVICE_NAME} service is not running.\"\n        fi\n\n        # Stop any remaining 'ollama' processes, excluding this script\n        OLLAMA_PIDS=$(pgrep -f ollama | grep -v \"$CURRENT_SCRIPT_PID\")\n\n        if [ \"${OLLAMA_PIDS}\" ]; then\n            ## Kill any ollama process that is not this script\n            echo \"Killing remaining 'ollama' processes with PIDs: ${OLLAMA_PIDS}\"\n            echo \"${OLLAMA_PIDS}\" | xargs -r sudo kill\n        else\n            ## No ollama processes found\n            echo \"No additional 'ollama' processes found.\"\n        fi\n\n        echo \"Completed stopping processes.\"\n        ;;\n    install | update)\n        ## The install &amp; update command are the same for ollama, simply re-running the\n        #  curl command used to install ollama. For install or update args, run the same command.\n        echo \"Running ${COMMAND} command for Ollama...\"\n        curl -fsSL https://ollama.com/install.sh | sh\n        echo \"Completed ${COMMAND} command.\"\n        ;;\n    *)\n        echo \"Invalid command. Usage: $0 {start|stop|install|update}\"\n        exit 1\n        ;;\nesac\n</code></pre>","tags":["utilities","llm"]},{"location":"utilities/ollama/index.html#links","title":"Links","text":"<ul> <li>Ollama Home</li> <li>Ollama Download</li> <li>Ollama Github</li> </ul>","tags":["utilities","llm"]},{"location":"utilities/restic/index.html","title":"Restic","text":"<p>Restic is a backup utility written in Go. The tool is fast &amp; relatively simple to setup &amp; script, making it useful for per-machine backups.</p> <p>Restic works by creating a repository (or multiple repositories) to store backup files in, and does differential snapshot backups on subsequent runs, backing up only what has changed and de-duplicating data in the process.</p> <p>The repository itself can live on your machine, network, or in cloud storage (Amazon S3, Backblaze B2, an SFTP directory, etc).</p> <p>Learn more about:</p> <ul> <li>Restic setup</li> <li>Excluding files/paths from backups</li> <li>Handling Restic repository passwords</li> <li>Scripting <code>restic</code> commands</li> </ul>","tags":["linux","windows","mac","utilities","bash","backup"]},{"location":"utilities/restic/index.html#tips-tricks","title":"Tips &amp; Tricks","text":"","tags":["linux","windows","mac","utilities","bash","backup"]},{"location":"utilities/restic/index.html#permission-denied-errors","title":"'Permission Denied' Errors","text":"<p>If you get a permission error when creating backups, you should use <code>sudo</code> to perform the backup. This also means the owner of the backup in restic's repository will be owned by root; any time you run a command that interacts with the repository, i.e. <code>restic snapshots</code>, you will get permission errors when restic tries to interact with that path.</p> <p>You can safely change the ownership of your restic repository path, without affecting the ownership in the repository itself. When restoring a backup, run with <code>sudo</code> or as the root user; this will restore paths with their original ownership, i.e. paths owned by root will be restored with root ownership, and paths owned by <code>$USER</code> will have ownership restored to the user.</p> <p>If you restore a restic backup as a regular, non-root user after changing ownership in the restic repository path, you will get a permisssion error.</p> <p>Warning</p> <p>When changing ownership in your restic repository path, you will not damage the backup, but you do still need to be mindful of ACLs. You should create a group, i.e. <code>resticusers</code>, and set group ownership on your restic repository path.</p> <p>If you are running restic on a single-user home machine and are ok with the security risks of your user owning the files in the restic repository, you can safely set ownership with <code>chmod -R $USER:$USER /path/to/restic-repo</code> without harming the backups.</p>","tags":["linux","windows","mac","utilities","bash","backup"]},{"location":"utilities/restic/index.html#links","title":"Links","text":"<ul> <li>Restic home</li> <li>Restic Github</li> <li>Restic docs<ul> <li>Restic install docs</li> <li>Restic prepare new repo docs</li> </ul> </li> </ul>","tags":["linux","windows","mac","utilities","bash","backup"]},{"location":"utilities/restic/cheatsheet.html","title":"Restic Cheatsheet","text":"<p>The cheat sheets below provide a quick guide for things you might already know but forget the commands/variable names.</p>","tags":["linux","windows","mac","utilities","backup","restic","cheatsheet"]},{"location":"utilities/restic/cheatsheet.html#cli-args","title":"CLI Args","text":"Arg Notes <code>-r/--repo/--repository /path/to/repo</code> When running <code>restic</code> commands, sets the path to the repository. <code>--repository-file /path/to/restic_repo_path</code> Read the path to a restic repository. Same as passing the path with <code>-r</code>, but reads it from a file instead. <code>--password-file /path/to/restic_password</code> Read the password to a Restic repository from a file. <code>--password-command &lt;command&gt;</code> Shell command to obtain repository password (i.e. password manager CLI, read vault secret, etc). <code>--key-hint &lt;key&gt;</code> Key ID of a Restic repository key. <code>--skip-if-unchanged</code> When doing a <code>restic backup</code> operation, this will skip snapshotting path(s) that have not changed.","tags":["linux","windows","mac","utilities","backup","restic","cheatsheet"]},{"location":"utilities/restic/cheatsheet.html#commands","title":"Commands","text":"Command Explained <code>restic init --repo /path/to/repo</code> Initializes a restic repository at the given path. Note that the path must exist before you run the init command (<code>mkdir -p /path/to/repo</code>). <code>restic --repo /path/to/backup_repository --password-file ~/.restic/passwords/main backup /home/username --exclude-file ~/.restic/ignores/default --exclude *.jpg</code> Creates a backup of <code>/home/repository</code> at the path in the <code>/path/to/backup_repository</code> path, provides the password in a file <code>restic</code> can read, provides a file defining patterns &amp; paths to ignore, and explicitly excludes <code>.jpg</code> files from the backup. <code>restic key add [--password-file path/to/current/password] [--new-password-file path/to/new/password]</code> Generate a new key by inputting the master password, then a new password that can also unlock the database. Optionally add <code>--new-password-file</code> to output the new password to a file.","tags":["linux","windows","mac","utilities","backup","restic","cheatsheet"]},{"location":"utilities/restic/cheatsheet.html#environment-variables","title":"Environment Variables","text":"Environment Variable Example Value Notes <code>RESTIC_REPOSITORY</code> <code>/opt/backups/restic/repo_name</code> When running <code>restic</code> commands, provides the <code>--repo</code> value. <code>RESTIC_REPOSITORY_FILE</code> <code>/path/to/restic_repo_path</code> Read the path to a Restic repository from a file. You do not need to use <code>-r/--repo</code> when this is set. <code>RESTIC_PASSWORD_FILE</code> <code>/path/to/restic_password</code> Read the password to a Restic repository from a file. Without this, you will be prompted for the repository's password. <code>RESTIC_KEY_HINT</code> <code>eb78040b</code> Key ID of a Restic repository key.","tags":["linux","windows","mac","utilities","backup","restic","cheatsheet"]},{"location":"utilities/restic/exclude.html","title":"Ignore/exclude files","text":"<p>Restic docs: Excluding files</p> <p>You can pass <code>--exclude path_or_pattern</code> to <code>restic</code> commands to exclude matching paths in the source directory from the backup.</p> <p>For scheduled backups (or just as a general practice), you can create \"ignore files\" that you can pass like <code>restic src/ --exclude-file ~/.restic/ignores/ignore_filename</code>. You can also pass multiple <code>--exclude-filename ...</code> and <code>--exclude ...</code> flags.</p> <p>You can split your ignore files, i.e. by programming language or purpose. Below are some purpose-specific Restic ignore files.</p>","tags":["linux","windows","mac","utilities","restic","backup"]},{"location":"utilities/restic/exclude.html#defaultbase-ignores","title":"Default/base ignores","text":"~/.restic/ignores/default<pre><code>## Ignore caches\n*.cache\n.cache/\n\n## Ignore temporary files\n*.tmp\n*.temp\n*.bak\n\n## Ignore specific user directories\nDownloads/\n\n## Ignore MacOS and Linux system files\n.DS_Store\n*.swp\n\n## Ignore logs and temporary files\n*.log\n*.tmp\n*.swp\n\n## -----------------------------------\n\n## Don't ignore the restic ignore file\n!.resticignore\n</code></pre>","tags":["linux","windows","mac","utilities","restic","backup"]},{"location":"utilities/restic/exclude.html#python-ignores","title":"Python ignores","text":"~/.restic/ignores/python<pre><code>## Ignore Python bytecode files\n__pycache__/\n*.py[cod]\n*$py.class\n\n## Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\nsite/\ndownloads/\neggs/\n.eggs/\nlib64/\nparts/\nsdist/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n## Ruff cache\n.ruff_cache\n\n## Nox cache\n.nox\n\n## Ignore virtual environment directories\nvenv/\nenv/\n.env/\n\n## Ignore distribution/build directories\nbuild/\ndist/\n*.egg-info/\n\n## Ignore logs and temporary files\n*.log\n*.tmp\n*.swp\n\n## Ignore caches\n.cache/\n.pytest_cache/\n\n## Ignore IDE/editor files\n.vscode/\n.idea/\n\n## -----------------------------------\n\n## Don't ignore the restic ignore file\n!.resticignore\n</code></pre>","tags":["linux","windows","mac","utilities","restic","backup"]},{"location":"utilities/restic/exclude.html#go-ignores","title":"Go ignores","text":"~/.restic/ignores/go<pre><code>## Ignore Go build binaries\n*.exe\n*.exe~\n*.test\n\n## Ignore build output directories\nbin/\nbuild/\n\n## -----------------------------------\n\n## Don't ignore the restic ignore file\n!.resticignore\n</code></pre>","tags":["linux","windows","mac","utilities","restic","backup"]},{"location":"utilities/restic/exclude.html#windows-ignores","title":"Windows ignores","text":"<pre><code>## Exclude Windows system and special folders\nC:\\$RECYCLE.BIN\nC:\\swapfile.sys\nC:\\hiberfil.sys\nC:\\pagefile.sys\nC:\\Program Files\\Common Files\\BitDefender\nC:\\ProgramData\\Microsoft\\Crypto\nC:\\ProgramData\\Microsoft\\Windows\\WER\nC:\\Windows\\debug\nC:\\System Volume Information\n\n## Temp and cache folders\n\\temp\n\\tmp\n\\Users\\*\\AppData\\Local\\Temp\\\n\\Users\\*\\AppData\\Local\\Package Cache\n\\Users\\*\\AppData\\Roaming\\*\\cache\\\n\\Users\\*\\Local\\Temp\n\\Users\\*\\Local\\Microsoft\\Windows\\INetCache\n\n## User-specific exclusion patterns\n\\Users\\*\\appdata\\index.dat\n\n## Generic cache and temp files\n.cache\ncache\ncache2\n*~\n__pycache__\n*.pyc\n</code></pre>","tags":["linux","windows","mac","utilities","restic","backup"]},{"location":"utilities/restic/exclude.html#my-default-excludes-file","title":"My default excludes file","text":"<pre><code>*~\n\n## Exclude Windows system and special folders\nC:\\$RECYCLE.BIN\nC:\\swapfile.sys\nC:\\hiberfil.sys\nC:\\pagefile.sys\nC:\\Program Files\\Common Files\\BitDefender\nC:\\ProgramData\\Microsoft\\Crypto\nC:\\ProgramData\\Microsoft\\Windows\\WER\nC:\\Windows\\debug\nC:\\System Volume Information\n\n## Temp and cache folders\n\\temp\n\\tmp\n\\Users\\*\\AppData\\Local\\Temp\\\n\\Users\\*\\AppData\\Local\\Package Cache\n\\Users\\*\\AppData\\Roaming\\*\\cache\\\n\\Users\\*\\Local\\Temp\n\\Users\\*\\Local\\Microsoft\\Windows\\INetCache\n\n## User-specific exclusion patterns\n\\Users\\*\\appdata\\index.dat\n\n## Ignore caches\n*.cache\n.cache/\n\n## Ignore temporary files\n*.tmp\n*.temp\n\n## Ignore specific user directories\nDownloads/\n\n## Ignore log files\n*.log\n\n## Ignore MacOS and Linux system files\n.DS_Store\n*.swp\n\n## Ignore Go build binaries\n*.exe\n*.exe~\n*.test\n\n## Ignore shared libraries\n*.dll\n*.so\n*.dylib\n\n## Ignore coverage and output files\n*.out\n\n## Ignore vendor directory (if used)\nvendor/\n\n## Ignore build output directories\nbin/\nbuild/\n\n## Ignore Python bytecode files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\nsite/\ndownloads/\neggs/\n.eggs/\nlib64/\nparts/\nsdist/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n## Ignore virtual environment directories\nvenv/\nenv/\n.env/\n\n## Ignore distribution/build directories\nbuild/\ndist/\n*.egg-info/\n\n## Ignore logs and temporary files\n*.log\n*.tmp\n*.swp\n\n## Ignore caches\n.cache/\n.pytest_cache/\n\n## Ignore IDE/editor files\n.vscode/\n.idea/\n\n## -----------------------------------\n\n## Don't ignore the restic ignore file\n!.resticignore\n## Don't ignore resticprofile profiles.yaml\n!profiles.yaml\n</code></pre>","tags":["linux","windows","mac","utilities","restic","backup"]},{"location":"utilities/restic/passwords.html","title":"Handling repository passwords","text":"","tags":["linux","windows","mac","utilities","restic","backup"]},{"location":"utilities/restic/scripting.html","title":"Scripts &amp; Schedules","text":"<p><code>restic</code> can be used as a CLI utility that you run manually when you want to create backups, but you can also script its usage, making it great for scheduled backups. The documentation in this section is not complete, but the Restic docs have an entry for scripting <code>restic</code> that is worth reading.</p> <p>The documentation on this page assumes you are using my Restic setup docs. If not, you should be able to adapt the scripts for your own setup.</p> <p>Warning</p> <p>This documentation is not complete. I am starting by providing example scripts, and will fill in details/write more generic scripts over time. This page is incomplete until this message is gone.</p>","tags":["linux","windows","mac","utilities","backup","restic"]},{"location":"utilities/restic/scripting.html#examples","title":"Examples","text":"","tags":["linux","windows","mac","utilities","backup","restic"]},{"location":"utilities/restic/scripting.html#generic-backup-script","title":"Generic backup script","text":"<p>If you did not follow my Restic setup guide, or you want to create a more 'self contained' script that does not rely on it, you can use something like the script below.</p> restic_backup.sh<pre><code>#!/bin/bash\n\n##\n# This script is a generic Restic backup script.\n# You can use it as a starting/reference point for\n# customized backup scripts, or just use this to\n# run/schedule backups.\n#\n# This script assumes you're storing your password\n# locally in a file, and using a local repository.\n##\n\n## Set default vars\nSRC_DIR=\"\"\nRESTIC_REPO_FILE=\"\"\nRESTIC_PW_FILE=\"\"\nDRY_RUN=\"\"\nRESTIC_FORCE=\"\"\n\n## Create array of exclude files to pass to restic\nEXCLUDE_FILES=()\n## Create array of exclude patternss to pass to restic\nEXCLUDE_PATTERNS=()\n\n## Define cleanup parameters\nDO_CLEANUP=\"\"\nKEEP_DAILY=7\nKEEP_WEEKLY=4\nKEEP_MONTHLY=12\n\n## Define -h/--help function\nfunction print_help() {\n  cat &lt;&lt;EOF\nUsage: $(basename \"$0\") [OPTIONS]\n\nRestic backup script with support for multiple exclude files and patterns.\n\nOPTIONS:\n  -s, --backup-src DIR           Source directory to back up (required)\n  -r, --repo-file PATH           Path to file with restic repository path (required)\n  -p, --password-file PATH       Path to restic password file (required)\n  --exclude-file PATH            Path to a file containing exclude patterns (can be used multiple times)\n  --exclude-pattern PATTERN      Single exclude pattern (can be used multiple times)\n  --keep-daily N                 Retain last N daily snapshots when running with --cleanup (default: $KEEP_DAILY)\n  --keep-weekly N                Retain last N weekly snapshots when running with --cleanup (default: $KEEP_WEEKLY)\n  --keep-monthly N               Retain last N monthly snapshots when running with --cleanup (default: $KEEP_MONTHLY)\n  -c, --cleanup                  Run restic cleanup after backup (default: false)\n  --force                        Add the --force flag to Restic commands.\n  --dry-run                      Print the restic command that would be run, but do not execute\n  -h, --help                     Display this help and exit\n\nEXAMPLES:\n  $(basename \"$0\") -s /home/username -r /etc/restic/repo -p /etc/restic/pw \\\n      --exclude-file ~/.restic/ignores/default --exclude-pattern \"*.cache\" --dry-run\n\nNOTES:\n  - You can specify --exclude-file and --exclude-pattern multiple times to pass multiple values.\n  - Run with --dry-run to see command without running it.\nEOF\n}\n\n## Parse CLI args\nwhile [[ $# -gt 0 ]]; do\n  case \"$1\" in\n    -s|--backup-src)\n      if [[ -z $2 ]]; then\n        echo \"[ERROR] --backup-src provided but no path given.\"\n\n        print_help\n        exit 1\n      fi\n\n      SRC_DIR=\"$2\"\n      shift 2\n      ;;\n    -r|--repo-file)\n      if [[ -z $2 ]]; then\n        echo \"[ERROR] --repo-file provided but no path given.\"\n\n        print_help\n        exit 1\n      fi\n\n      RESTIC_REPO_FILE=\"$2\"\n      shift 2\n      ;;\n    -p|--password-file)\n      if [[ -z $2 ]]; then\n        echo \"[ERROR] --password-file provided but no path given.\"\n\n        print_help\n        exit 1\n      fi\n\n      RESTIC_PW_FILE=\"$2\"\n      shift 2\n      ;;\n    --exclude-file)\n      if [[ -n \"$2\" &amp;&amp; \"$2\" != --* ]]; then\n          EXCLUDE_FILES+=(\"$2\")\n          shift 2\n      else\n          echo \"[ERROR] --exclude-file provided but no path given.\"\n\n          print_help\n          exit 1\n      fi\n      ;;\n    --exclude-pattern)\n      if [[ -n \"$2\" &amp;&amp; \"$2\" != --* ]]; then\n          EXCLUDE_PATTERNS+=(\"$2\")\n          shift 2\n      else\n          echo \"[ERROR] --exclude-pattern provided but no path given.\"\n\n          print_help\n          exit 1\n      fi\n      ;;\n    --keep-daily)\n      if [[ -n \"$2\" &amp;&amp; \"$2\" != --* ]]; then\n          KEEP_DAILY=\"$2\"\n          shift 2\n      else\n          echo \"[ERROR] --keep-daily provided but no number given.\"\n\n          print_help\n          exit 1\n      fi\n      ;;\n    --keep-weekly)\n      if [[ -n \"$2\" &amp;&amp; \"$2\" != --* ]]; then\n          KEEP_WEEKLY=\"$2\"\n          shift 2\n      else\n          echo \"[ERROR] --keep-weekly provided but no number given.\"\n\n          print_help\n          exit 1\n      fi\n      ;;\n    --keep-monthly)\n      if [[ -n \"$2\" &amp;&amp; \"$2\" != --* ]]; then\n          KEEP_MONTHLY=\"$2\"\n          shift 2\n      else\n          echo \"[ERROR] --keep-monthly provided but no number given.\"\n\n          print_help\n          exit 1\n      fi\n      ;;\n    -c|--cleanup)\n      DO_CLEANUP=\"true\"\n      shift\n      ;;\n    -F|--force)\n      RESTIC_FORCE=\"true\"\n      shift\n      ;;\n    --dry-run)\n      DRY_RUN=\"true\"\n      shift\n      ;;\n    -h|--help)\n      print_help\n      exit 0\n      ;;\n    *)\n      echo \"[ERROR] Unrecognized flag: $1\"\n\n      print_help\n      exit 1\n      ;;\n  esac\ndone\n\n## Export env vars for script\nexport RESTIC_REPOSITORY_FILE=\"$RESTIC_REPO_FILE\"\nexport RESTIC_PASSWORD_FILE=\"$RESTIC_PW_FILE\"\n\nif [[ \"$SRC_DIR\" == \"\" ]]; then\n  echo \"[ERROR] --source-dir should not be empty\"\n  exit\nfi\n\n## Build command\ncmd=(restic backup \"$SRC_DIR\")\n\n## Append all exclude files\nfor excl_file in \"${EXCLUDE_FILES[@]}\"; do\n  cmd+=(--exclude-file \"$excl_file\")\ndone\n\n## Append all exclude patterns\nfor excl_pattern in \"${EXCLUDE_PATTERNS[@]}\"; do\n  cmd+=(--exclude \"$excl_pattern\")\ndone\n\n## Append --force if RESTIC_FORCE is true\nif [[ \"$RESTIC_FORCE\" != \"\" ]]; then\n  cmd+=(--force)\nfi\n\n## Print or run command\nif [[ -z \"$DRY_RUN\" ]] || [[ \"$DRY_RUN\" == \"\" ]]; then\n  echo \"Running restic backup command: \"\n  echo \"  $&gt; ${cmd[@]}\"\n\n  ## Run the command\n  \"${cmd[@]}\"\n\n  if [[ $? -ne 0 ]]; then\n    echo \"[ERROR] Failed to run restic backup command.\"\n\n    if [[ \"$DO_CLEANUP\" == \"true\" ]]; then\n      echo \"[WARNING] --cleanup detected, but restic backup failed. Cleanup operation will not run.\"\n    fi\n\n    exit 1\n  else\n    echo \"Restic backup performed successfully\"\n\n    if [[ \"$DO_CLEANUP\" == \"true\" ]]; then\n      cleanup_cmd=(restic forget\n        --keep-daily \"$KEEP_DAILY\"\n        --keep-weekly \"$KEEP_WEEKLY\"\n        --keep-monthly \"$KEEP_MONTHLY\"\n        --prune\n      )\n\n      echo \"Running restic cleanup command: \"\n      echo \"  $&gt; ${cleanup_cmd[@]}\"\n\n      \"${cleanup_cmd[@]}\"\n      if [[ $? -ne 0 ]]; then\n        echo \"[ERROR] Failed to run restic cleanup command.\"\n      fi\n\n    fi\n\n    exit 0\n  fi\nelse\n  echo \"[DRY RUN] Would run restic backup command: \"\n  echo \"  $&gt; ${cmd[@]}\"\n\n  exit 0\nfi\n</code></pre>","tags":["linux","windows","mac","utilities","backup","restic"]},{"location":"utilities/restic/scripting.html#backup-home","title":"Backup Home","text":"backup_home_dir.sh<pre><code>#!/bin/bash\n\nDOT_RESTIC=\"$HOME/.restic\"\nDOT_RESTICRC=\"$DOT_RESTIC/.resticrc\"\nRESTIC_IGNORES_DIR=\"$DOT_RESTIC/ignores\"\nRESTIC_PASSWORDS_DIR=\"$DOT_RESTIC/passwords\"\n\nif [[ ! -d \"${DOT_RESTIC}\" ]]; then\n  echo \"[ERROR] Could not find restic directory at '${DOT_RESTIC}'\"\n  exit 1\nfi\n\nif [[ ! -d \"${RESTIC_IGNORES_DIR}\" ]]; then\n  echo \"[ERROR] Could not find restic exclude files directory at path '${RESTIC_IGNORES_DIR}'\"\n  exit 1\nfi\n\nif [[ ! -d \"${RESTIC_PASSWORDS_DIR}\" ]]; then\n  echo \"[ERROR] Could not find restic password files directory at path '${RESTIC_PASSWORDS_DIR}'\"\n  exit 1\nfi\n\n## Source ~/.restic/.resticrc\nif [[ ! -f \"${DOT_RESTICRC}\" ]]; then\n  echo \"[ERROR] Could not find ~/.restic/.resticrc\"\n  exit 1\nfi\n. \"${DOT_RESTICRC}\"\n\n## Ensure ~/.restic/.resticrc values loaded correctly\nif [[ -z \"${RESTIC_REPOSITORY_FILE}\" ]] || [[ \"${RESTIC_REPOSITORY_FILE}\" == \"\" ]]; then\n  echo \"[ERROR] RESTIC_REPOSITORY_FILE should not be empty. Ensure ~/.restic/.resticrc defines this variable\"\n  exit 1\nfi\n\nif [[ -z \"${RESTIC_PASSWORD_FILE}\" ]] || [[ \"${RESTIC_PASSWORD_FILE}\" == \"\" ]]; then\n  echo \"[ERROR] RESTIC_PASSWORD_FILE should not be empty. Ensure ~/.restic/.resticrc defines this variable\"\n  exit 1\nfi\n\n## Ensure restic files exist\nif [[ ! -f \"${RESTIC_REPOSITORY_FILE}\" ]]; then\n  echo \"[ERROR] Could not find Restic repository file at path '$RESTIC_REPOSITORY_FILE'\"\n  exit 1\nfi\n\nif [[ ! -f \"${RESTIC_PASSWORD_FILE}\" ]]; then\n  echo \"[ERROR] Could not find Restic password file at path '$RESTIC_PASSWORD_FILE'\"\n  exit 1\nfi\n\n## Set default vars\nDRY_RUN=\"\"\n\n## Parse script inputs\nwhile [[ $# -gt 0 ]]; do\n  case \"$1\" in\n    --dry-run)\n      DRY_RUN=\"true\"\n      shift 1\n      ;;\n    *)\n      echo \"[ERROR] Unknown argument: $1\"\n      shift 1\n      ;;\n  esac\ndone\n\n## Build restic command\ncmd=\"restic backup /home/username --exclude-file \\\"${RESTIC_IGNORES_DIR}/default\\\"\"\n\n## Run or print command\nif [[ -z \"$DRY_RUN\" ]] || [[ \"$DRY_RUN\" == \"\" ]]; then\n  echo \"Running restic backup command: $cmd\"\n  eval \"${cmd}\"\n\n  if [[ $? -ne 0 ]]; then\n    echo \"[ERROR] Failed to create restic backup.\"\n    exit 1\n  else\n    echo \"Created restic snapshot of /home/jack\"\n    exit 0\n  fi\nelse\n  echo \"[DRY RUN] Would run restic backup command: $cmd\"\n  exit 0\nfi\n</code></pre>","tags":["linux","windows","mac","utilities","backup","restic"]},{"location":"utilities/restic/setup.html","title":"Restic Setup","text":"<p>Warning</p> <p>This documentation is the system I've settled on for my own Restic backups. I will explain it in detail and if it sounds like something you'd like to replicate, it should work the same on your machine.</p> <p>Your needs may differ from mine, but hopefully you can adapt some of the work here for your own needs.</p> <p>Restic provides a quickstart guide, which you should run through at least once to learn the steps.</p> <p>While you can pass many options as CLI args to <code>restic</code>, the setup below sets up a pipeline of sorts for providing <code>restic</code> commands with the values it needs.</p>","tags":["linux","windows","mac","restic","utilities","backup"]},{"location":"utilities/restic/setup.html#restic-dir","title":".restic dir","text":"<p>Create a directory at <code>~/.restic</code>. This is where you will store your Restic configurations.</p> <p>Create the following directories in <code>~/.restic</code>:</p> <ul> <li><code>~/.restic/ignores/</code>: This directory will store \"restic ignore\" files, which you can pass with <code>restic src/ --exclude-file ~/.restic/ignores/ignore_filename</code><ul> <li>Read more in the ignore/exclude docs</li> </ul> </li> <li><code>~/.restic/passwords/</code>: <ul> <li>Storing a repository password in a file is not the most secure, but can be used for smaller/non-sensitive backups, or while you're still learning Restic.</li> <li>When creating a repository, you're prompted for a password. Paste the password you used into a file in <code>~/.restic/passwords/password_filename</code> (you can use any filename).</li> <li>You can then set the env var (either in <code>~/.bashrc</code> or in a script before executing a <code>restic</code> command) <code>RESTIC_PASSWORD_FILE=\"~/.restic/passwords/password_filename</code></li> <li>Read more in the handling repository passwords docs</li> </ul> </li> <li><code>~/.restic/repo/</code>: (optional) You can create a symlink to your backup directory if you want, so you can use <code>restic -r ~/.restic/repo</code>:<ul> <li><code>ln -s /path/to/restic_repo /home/$USER/.restic/repo</code></li> </ul> </li> </ul>","tags":["linux","windows","mac","restic","utilities","backup"]},{"location":"utilities/restic/setup.html#resticrc-file","title":".resticrc file","text":"<p>In your <code>~/.restic</code> directory, you should also create a <code>.resticrc</code> file. This file can be used to provide Restic's env vars to the shell.</p> ~/.restic/.resticrc<pre><code>## Export path to the file with your repository file\nexport RESTIC_PASSWORD_FILE=\"/home/username/.restic/passwords/password_filename\"\n## Export path to file containing the path to the repository that the password unlocks\nexport RESTIC_REPOSITORY_FILE=\"/home/username/.restic/repo_path\"\n</code></pre> <p>After creating your <code>.resticrc</code> file, you can source it in your profile, i.e. by adding this to <code>~/.bashrc</code>:</p> ~/.basrc<pre><code>## Other .bashrc entries above\n\n## Source ~/.restic/.resticrc if it exists\nif [[ -f \"$HOME/.restic/.resticrc\" ]]; then\n  . \"$HOME/.restic/.resticrc\"\nfi\n</code></pre> <p>This step is optional, but will let you run <code>restic</code> commands directly without needing to provide a <code>-r/--repository</code> or <code>--password-file</code> parameter.</p> <p>You can also source this file in a shell script to add the <code>RESTIC_PASSWORD_FILE</code> and <code>RESTIC_REPOSITORY_FILE</code> to the script's environment.</p>","tags":["linux","windows","mac","restic","utilities","backup"]},{"location":"utilities/resticprofile/index.html","title":"Resticprofile","text":"<p><code>resticprofile</code> is a wrapper for <code>restic</code> that manages your backups as \"profiles\". You can split your backups into separate YAML files, or use the standard <code>~/profiles.yaml</code> to store all of your profiles.</p> <p>Warning</p> <p>This documentation does not cover all of the capabilities of this tool. You should read the <code>resticprofile</code> docs yourself to learn how to operate it.</p>","tags":["linux","windows","mac","utilities","bash","backup","restic","resticprofile"]},{"location":"utilities/resticprofile/index.html#links","title":"Links","text":"<ul> <li>Restic Profile home</li> <li>Restic Profile Github</li> <li>profiles.yaml examples</li> </ul>","tags":["linux","windows","mac","utilities","bash","backup","restic","resticprofile"]},{"location":"utilities/resticprofile/cleanup.html","title":"Cleanup","text":"<p>When using <code>resticprofile</code> for backups, regular cleanup of old snapshots and repository pruning is essential to keep the backup repository manageable and avoid running out of storage.</p> <p>Restic uses snapshots to save states of your backups over time. Old snapshots can accumulate and consume a lot of space. Cleaning up involves two main steps: - Forget snapshots according to a retention policy. - Prune the repository to remove unreferenced data.</p> <p>Preview operations with --dry-run</p> <p>You can add the <code>--dry-run</code> flag to most of the commands below to have <code>resticprofile</code> show you what it would clean up, without actually performing the operation.</p> Difference between 'prune' and 'forget' <p>The <code>forget</code> operation deletes snapshot metadata &amp; refs, but it does not delete the actual backup data. This means you can run this operation frequently, as it's a quick operation that queues up objects for deletion when a <code>prune</code> command is run.</p> <p>The <code>prune</code> command cleans up the repository by removing the actual data blobs and packs that are no longer referenced by any snapshot. It performs a low-level reorganization of the repository to reclaim storage space by deleting unneeded backup data. This operation is slower and should be run less frequently, i.e. once a week or as-needed when the repository is too large.</p>","tags":["restic","resticprofile"]},{"location":"utilities/resticprofile/cleanup.html#forget-old-snapshots","title":"Forget Old Snapshots","text":"<p>The <code>forget</code> command removes snapshot metadata &amp; refs when the backup's <code>forget:</code> conditions are met. The command does not delete any of the actual data in the backups, but it marks them for deletion with the prune command.</p> Forget old backups<pre><code>resticprofile -c ~/profiles.yaml forget\n</code></pre> <p>You can optionally add <code>--prune</code> to perform a pruning operation at the same time.</p>","tags":["restic","resticprofile"]},{"location":"utilities/resticprofile/cleanup.html#prune-backup-data","title":"Prune backup data","text":"<p>The <code>prune</code> command uses a similar schedule as the forget command, but also deletes the underlying blob data. This operation takes longer than <code>forget</code>, and is destructive.</p> Prune old backups<pre><code>resticprofile -c ~/profiles.yaml --name profile-name prune\n</code></pre>","tags":["restic","resticprofile"]},{"location":"utilities/resticprofile/cleanup.html#check-integrity-after-cleanup","title":"Check integrity after cleanup","text":"<p>After running a cleanup/forget command, you should run a <code>check</code> to ensure the integrity of your backups:</p> Check repository integrity<pre><code>resticprofile -c ~/profiles.yaml --name profile-name check\n</code></pre>","tags":["restic","resticprofile"]},{"location":"utilities/resticprofile/excludes.html","title":"Excluding paths in a backup","text":"<p><code>resticprofile</code> can handle Restic exclude patterns to skip paths during backups. When backing up a directory, you can (and should) provide exclude patterns to limit the size of the backup, skipping over any unnecessary files.</p> <p>You can pass excludes as a list of patterns in your <code>profiles.yaml</code>, or give a path to a file with exclude patterns using the <code>exclude-file:</code> parameter.</p> <p>Read more about excludes on the excludes page.</p>","tags":["restic","resticprofile"]},{"location":"utilities/resticprofile/profiles.html","title":"profiles.yaml","text":"<p><code>resticprofile</code> operates off of YAML files that define backup \"profiles\" for Restic. Read more about profile configuration in the <code>resticprofile</code> Getting Started docs.</p>","tags":["restic","resticprofile"]},{"location":"utilities/resticprofile/profiles.html#schedules","title":"Schedules","text":"<p>You can add schedules &amp; retention policies to your backups using a schedule configuration. Using options like <code>schedule:</code>, <code>schedule-permission:</code>, <code>schedule-priority:</code>, and more, you can set specific backup profiles to run at a given interval, or to run cleanup operations occasionally &amp; automatically.</p> <p>Tip</p> <p>Read more about scheduling in the <code>resticprofile</code> docs.</p> <p>You can run <code>resticprofile -c ~/profiles.yaml schedule install --all</code> to run your backup profiles on a schedule using the scheduler for your system.</p>","tags":["restic","resticprofile"]},{"location":"utilities/resticprofile/profiles.html#example-schedule","title":"Example schedule","text":"<p>The example below defines a backup, and schedules for cleanup &amp; caching. The schedules are:</p> <ul> <li>Retains the 2 most recent backups</li> <li>Retains 24 hourly snapshots for jobs that run hourly</li> <li>Retains 7 daily snapshots for jobs that run daily</li> <li>Retains 4 weekly snapshots for jobs that run weekly</li> <li>Skips deleting snapshots that would be deleted by one of the conditions above, if it has a tag <code>forever</code></li> <li>Runs a prune operation when the scheduled cleanup runs</li> <li>Prunes weekly by default</li> <li>Checks the backups once a week</li> </ul> <pre><code>---\nversion: \"1\"\n\nglobal:\n  ## Set scheduler options globally. You can still override in individual backup profiles\n  scheduler: auto\n  schedule-defaults:\n    permission: auto\n\ndefault:\n  repository: \"local:/path/to/restic-repo\"\n  password-file: \"/path/to/restic/password\"\n\n  backup:\n    skip-if-unchanged: true\n    group-by: \"tags,host,paths\"\n\n  ## Define backup retention policy\n  #  https://creativeprojects.github.io/resticprofile/schedules/index.html\n  forget:\n    keep-last: 2\n    keep-hourly: 24\n    keep-daily: 7\n    keep-weekly: 4\n    keep-tag:\n      - forever\n    prune: true\n\n  ## Define pruning policy\n  #  https://creativeprojects.github.io/resticprofile/reference/profile/prune/index.html\n  prune:\n    schedule: \"weekly\"\n    schedule-permission: \"auto\"\n\n  ## Checks the repository for errors\n  #  https://creativeprojects.github.io/resticprofile/reference/profile/check/index.html\n  check:\n    schedule: \"1 *-*-* 03:00\"  # 3:00am on the 1st of each month\n    schedule-after-network-online: false\n    schedule-ignore-on-battery: false\n    schedule-ignore-on-battery-less-than: 20\n    read-data: true\n    with-cache: false\n</code></pre>","tags":["restic","resticprofile"]},{"location":"utilities/resticprofile/profiles.html#example-windows-profile","title":"Example Windows profile","text":"<p>In your <code>$env:USERPROFILE</code> path, create a file <code>profiles.yaml</code>. This will be where you define your backups. You can create multiple profile files, but it's easy to start by just keeping them all in 1 place.</p> <p>This is an example of a <code>profiles.yaml</code> for a Windows machine, defining defaults for all jobs that can be overridden, and a number of paths to back up:</p> <pre><code># yaml-language-server: $schema=https://creativeprojects.github.io/resticprofile/jsonschema/config.json\n---\n\n## resticprofile configuration\nversion: \"1\"\n\nglobal:\n  default-command: ls latest\n  initialize: true\n  priority: low\n  ## Restic won't start a profile if there's less than 100MB of RAM available\n  min-memory: 100\n  scheduler: auto\n  schedule-defaults:\n    permission: auto\n  ## If you installed restic with something like scoop, your `restic` binary\n  #  might be in a different path, i.e. \"C:\\Users\\username\\scoop\\shims\\restic.exe\"\n  #  When resticprofile tries to run as an administrator, it will fail because\n  #  this path is not in the admin user's $PATH. You can tell it where the restic\n  #  binary is located with restic-binary:\n  restic-binary: \"C:/Users/username/scoop/shims/restic.exe\"\n\ngroups:\n  basic:\n    - home\n\n  full-backup:\n    - c_scripts\n    - home\n\n## Set defaults that profiles can inherit\ndefault:\n  repository: \"local:X:/path/to/restic-repo\"\n  password-file: \"C:/Users/username/.restic/passwords/user_access.txt\"\n  default-command: snapshots\n  initialize: false\n  priority: low\n  min-memory: 100\n\n  ## Backup operation defaults\n  #  https://creativeprojects.github.io/resticprofile/reference/profile/backup/index.html\n  backup:\n    verbose: false\n    one-file-system: false\n    read-concurrency: 4\n    skip-if-unchanged: true\n    group-by: \"tags,host,paths\"\n    exclude:\n      - *.tmp\n      - *.log\n      - *.log.*\n      - \"Temp\"\n      - \"$RECYCLE.BIN\"\n      - \"\\temp\"\n      - \"\\tmp\"\n      - \"\\Users\\*\\AppData\\Local\\Temp\\\"\n      - \"\\Users\\*\\AppData\\Local\\Package Cache\"\n      - \"\\Users\\*\\AppData\\Roaming\\*\\cache\\\"\n      - \"\\Users\\*\\Local\\Temp\"\n      - \"\\Users\\*\\Local\\Microsoft\\Windows\\INetCache\"\n    exclude-file: \"C:/Users/username/.restic/ignores/default\"\n    ## Exclude files like OneDrive On-Demand Files\n    exclude-cloud-files: true\n    schedule: \"weekly\"\n    schedule-ignore-on-battery-less-than: 20\n    read-data: true\n    with-cache: false\n    schedule-permission: \"auto\"\n\n  ## Define backup retention policy\n  #  https://creativeprojects.github.io/resticprofile/reference/profile/retention/index.html\n  forget:\n    keep-last: 2\n    keep-hourly: 24\n    keep-daily: 7\n    keep-weekly: 4\n    keep-tag:\n      - forever\n    prune: true\n\n  ## Define pruning policy\n  #  https://creativeprojects.github.io/resticprofile/reference/profile/prune/index.html\n  prune:\n    schedule: \"weekly\"\n    schedule-permission: \"auto\"\n\n  ## Checks the repository for errors\n  #  https://creativeprojects.github.io/resticprofile/reference/profile/check/index.html\n  check:\n    schedule: \"weekly\"\n    schedule-after-network-online: false\n    schedule-ignore-on-battery: false\n    schedule-ignore-on-battery-less-than: 20\n    read-data: true\n    with-cache: false\n\n  ## Cache settings\n  #  https://creativeprojects.github.io/resticprofile/reference/profile/cache/index.html\n  cache:\n    cleanup: true\n    max-age: 30\n    no-size: false\n\n  ## ignore restic warnings when files cannot be read\n  no-error-on-warning: true\n\nhome:\n  inherit: default\n  default-command: backup\n\n  backup:\n    verbose: true\n    source:\n      - \"C:/Users/username\"\n\n    ## Add more ignore files. They will be merged with the default\n    #  ignore defined in the default: profile\n    exclude-file:\n      - \"C:/Users/username/.restic/ignores/home\"\n      - \"C:/Users/username/.restic/ignores/gitdir\"\n      - \"C:/Users/username/.restic/ignores/desktop\"\n    ## Run backup twice a day\n    schedule: \"*/12:*\"\n    schedule-permission: \"user\"\n    schedule-priority: \"standard\"\n    schedule-lock-mode: default\n    schedule-lock-wait: 15m30s\n\n  tags:\n    - home\n    - userland\n\n  check:\n    schedule: \"*-*-01 03:00\"\n\nc_scripts:\n  inherit: default\n\n  backup:\n    verbose: true\n    source:\n      - \"C:/scripts\"\n    exclude-file:\n      - \"C:/Users/username/.restic/ignores/gitdir\"\n    ## Run backup daily\n    schedule: \"0,6,12,18:00\"  # every 6 hours\n    schedule-permission: \"system\"\n    schedule-priority: \"standard\"\n    schedule-lock-mode: default\n    schedule-lock-wait: 15m30s\n\n  tags:\n    - scripts\n\n  check:\n    schedule: \"*-*-01 03:00\"\n</code></pre> <p>After configuring the <code>profiles.yaml</code>, you can use <code>resticprofile -c path\\to\\profiles.yaml</code> to run the default backups. You can also run one of the backup \"groups\" with <code>resticprofile -c path\\to\\profiles.yaml --name &lt;group_name&gt;</code></p>","tags":["restic","resticprofile"]},{"location":"utilities/resticprofile/profiles.html#example-linux-profile","title":"Example Linux profile","text":"<pre><code># yaml-language-server: $schema=https://creativeprojects.github.io/resticprofile/jsonschema/config.json\n---\nversion: \"1\"\n\n###\n# This is a basic default profile I use that's kind of a\n# bare-minimum for all of my machines, and the starting point for\n# other profiles.\n#\n# To use this profile as-is, copy it to /home/user/profiles.yaml,\n# or pass it with `resticprofile -c /path/to/default.yaml`\n###\n\nglobal:\n  default-command: backup\n  initialize: true\n  priority: low\n  min-memory: 100\n\ngroups:\n  full-backup:\n    - home\n\n  userland:\n    - home\n\ndefault:\n  ## Path must exist. If you don't want to run resticprofile as root,\n  #  make sure it's owned by the user running resticprofile, i.e.\n  #  chmod 700 /opt/restic  (if repository is /opt/restic/repo)\n  repository: \"local:/opt/restic/repo\"\n  ## Create a password with:\n  #    $&gt; resticprofile generate --random-key $KEY_LENGTH &gt; /path/to/restic.key\n  password-file: \"\"\n  default-command: snapshots\n  initialize: false\n  priority: low\n  min-memory: 100\n\n  ## Define backup retention policy\n  #  https://creativeprojects.github.io/resticprofile/schedules/index.html\n  forget:\n    keep-last: 2\n    keep-hourly: 24\n    keep-daily: 7\n    keep-weekly: 4\n    keep-tag:\n      - forever\n    prune: true\n\n  ## Define pruning policy\n  #  https://creativeprojects.github.io/resticprofile/reference/profile/prune/index.html\n  prune:\n    schedule: \"weekly\"\n    schedule-permission: \"auto\"\n\n  ## Backup operation defaults\n  #  https://creativeprojects.github.io/resticprofile/reference/profile/backup/index.html\n  backup:\n    verbose: false\n    one-file-system: false\n    read-concurrency: 4\n    skip-if-unchanged: true\n    exclude:\n      - \".tmp/\"\n      - \".cache/\"\n    exclude-file:\n      - \"/home/user/.restic/ignores/default\"\n    exclude-cloud-files: true\n    group-by: \"tags,host,paths\"\n    schedule: \"weekly\"\n    schedule-permission: \"user\"\n    schedule-priority: \"standard\"\n    schedule-lock-mode: default\n    schedule-lock-wait: 15m30s\n\n  ## Checks the repository for errors\n  #  https://creativeprojects.github.io/resticprofile/reference/profile/check/index.html\n  check:\n    schedule: \"weekly\"\n    schedule-after-network-online: false\n    schedule-ignore-on-battery: false\n    schedule-ignore-on-battery-less-than: 20\n    read-data: true\n    with-cache: false\n\n  ## Cache settings\n  #  https://creativeprojects.github.io/resticprofile/reference/profile/cache/index.html\n  cache:\n    cleanup: false\n    max-age: 30\n    no-size: false\n\n  no-error-on-warning: true\n\n## Backup home dir\nhome:\n  inherit: default\n  default-command: backup\n\n  backup:\n    verbose: true\n    source: \"/home/user\"\n    read-concurrency: 4\n    skip-if-unchanged: true\n    exclude:\n      - \".tmp/\"\n      - \".cache/\"\n    exclude-file:\n      - \"/home/user/.restic/ignores/default\"\n      - \"/home/user/.restic/ignores/home\"\n    exclude-cloud-files: true\n    group-by: \"tags,host,paths\"\n    schedule: \"0,12:00\"  # twice a day\n\n  tags:\n    - home\n    - userland\n</code></pre>","tags":["restic","resticprofile"]},{"location":"utilities/resticprofile/setup.html","title":"Setup","text":"<p>Info</p> <p>The steps in this section are the way I personally have managed my Restic/resticprofile configurations. For most of the file/directory paths in this guide, you could substitute whatever other paths you want, and change those paths in any of the commands listed in the guide.</p> <p>Also note that using a file to store your password is bad practice and security. This guide is meant to be a quick setup for home/personal use, where the master key file is deleted once a 'user access' key is created. In a production setup, you would want to use an environment variable, or the <code>--password-cmd</code> option (or the <code>RESTIC_PASSWORD_COMMAND</code> env var for resticprofile) to retrieve the password from a vault.</p>","tags":["restic","resticprofile"]},{"location":"utilities/resticprofile/setup.html#initialize-a-new-repository","title":"Initialize a new repository","text":"<ul> <li>Create a directory <code>~/.restic/</code></li> <li>Create another directory <code>~/.restic/password/</code><ul> <li>Run the following commands to generate a 'main' and 'user' access key/password:<ul> <li><code>resticprofile generate --random-key 4096 &gt; ~/.restic/password/main</code><ul> <li>This key is incredibly important, make sure you back it up somewhere like a password manager!</li> <li>You can create and revoke keys as long as you still have this password.</li> <li>Use it only when required, i.e. for creating new keys. Once you've setup the repository the first time and added your user access key, you should delete the file containing the master password (<code>rm ~/.restic/password/main</code>)</li> </ul> </li> <li><code>resticprofile generate --random-key 4096 &gt; ~/.restic/password/user_access</code></li> <li>(Optional) Generate any other passwords you want to use, just make sure you save all of them in a password manager/vault somewhere.</li> </ul> </li> </ul> </li> <li>Create a file <code>~/profiles.yaml</code> (read more about profiles on the profiles.yaml page).<ul> <li>We will start small with a simple backup profile, and other sections of this documentation will detail adding more profiles later.</li> </ul> </li> <li>After creating your <code>profiles.yaml</code>, run <code>resticprofile -c ~/profiles.yaml --name &lt;backup-profile-name&gt; init</code></li> <li>Next, add your <code>~/.restic/passwords/user_acces</code> key with <code>resticprofile -c ~/profiles.yaml --name &lt;backup-profile-name&gt; key add --new-password-file ~/.restic/passwords/user_access</code><ul> <li>With the user access key added, edit your <code>~/profiles.yaml</code> and replace the <code>/home/user/.restic/passwords/main</code> line with <code>/home/user/.restic/passwords/user_access</code>.</li> <li>The only time you will need your master key going forward is to add or revoke keys. Restic/resticprofile will prompt you for the master password when needed, and you can just paste it.</li> <li>Otherwise, use your <code>user_access</code> key for everything after initializing the repository.</li> </ul> </li> <li>(Optional) If you want to add ignore patterns to a backup profile, you can create a file, i.e. <code>~/.restic/ignores/default</code>, with patterns that should be ignored any time a backup profiles specifies <code>exclude-file: ~/.restic/ignores/default</code>.<ul> <li>Example rules:<ul> <li><code>*.tmp</code></li> <li><code>*.bak</code></li> <li><code>*.log.*</code></li> </ul> </li> <li>Read more in the Excludes patterns section</li> </ul> </li> </ul>","tags":["restic","resticprofile"]},{"location":"utilities/resticprofile/setup.html#use-resticprofile-with-an-existing-repository","title":"Use resticprofile with an existing repository","text":"<p>If you already have a Restic repository, you can add it to your <code>profiles.yaml</code> and give it your password, and <code>resticprofile</code> will seamlessly take over the backup operations.</p>","tags":["restic","resticprofile"]},{"location":"utilities/resticprofile/setup.html#create-backup-profiles","title":"Create backup profiles","text":"<p><code>resticprofile</code> works off of profiles you define in YAML files. <code>resticprofile</code> looks for a file named <code>profiles.yaml</code> in the current directory, then in <code>~/profiles.yaml</code> in the user's <code>$HOME</code> directory.</p> <p>You can provide the path to a configuration file with <code>resticprofile -c /path/to/your_profiles.yaml</code>.</p> <p>The file can have any name, but if you use something other than <code>profiles.yaml</code>, you will have to provide it to <code>resticprofile</code> each time you run a command.</p> <p>Navigate to the [profiles.yaml page] to read more about backup profiles.</p>","tags":["restic","resticprofile"]},{"location":"utilities/resticprofile/setup.html#basic-profile","title":"Basic profile","text":"<p>A simple starting point that backs up your <code>~/.bashrc</code>, <code>~/.profile</code>, and <code>~/.bash_aliases</code>.</p> <pre><code># yaml-language-server: $schema=https://creativeprojects.github.io/resticprofile/jsonschema/config.json\n\n## The line above provides the profiles.yaml schema so editors like VSCode have syntax highlighting &amp; completion.\n---\n\n## resticprofile configuration\nversion: \"1\"\n\n## Set defaults that other profiles can inherit\ndefault:\n  repository: \"local:/path/to/restic-repo\"\n  ## Change this to '~/.restic/passwords/user_access'\n  #  after initializing the repository and adding the user_access key.\n  password-file: \"~/.restic/passwords/main\"\n\n## Create a profile to backup ~/.bash* dotfiles.\n#  When you run resticprofile, call this profile with\n#  `--name dot-bash-files\ndot-bash-files:\n  ## Inherit options defined in the default profile above. If an option\n  #  is specified in default but not in this profile, the default setting\n  #  will be used. Any options you define in this profile override the defaults.\n  inherit: default\n\n  ## Define the backup job\n  backup:\n    ## Enable verbose logging to see more output\n    verbose: true\n    ## Set the path(s) that should be backed up\n    source:\n      - \"/home/user/.bashrc\"\n      - \"/home/user/.profile\"\n      - \"/home/user/.bash_aliases\"\n\n    ## Define 1 or more exclude patterns, and any path\n    #  that matches during a backup will be skipped/excluded.\n    #  These patterns would be irrelevant for this job since we're\n    #  specifying the files explicitly. These excludes are merged\n    #  with any defined in the default profile, or passsed with an\n    #  exclude-file.\n    exclude:\n      - *.tmp\n      - *.bak\n\n    ## You can optionally pass the path(s) to 1 or more files\n    #  that have ignore patterns for restic. The syntax\n    #  is similar to .gitignores.\n    #  Excludes are merged; anything defined in default, or\n    #  passed as an explicit exclude: list (read above).\n    exclude-file:\n      - \"/path/to/default_excludes_filename\"\n      - \"/path/to/another_excludes_filename\"\n</code></pre>","tags":["restic","resticprofile"]},{"location":"utilities/resticprofile/setup.html#take-your-first-backup","title":"Take your first backup","text":"<p>Now that your profiles are all set up, you can test your configuration with  <code>resticprofile -c ~/profiles.yaml --name &lt;backup-profile-name&gt; backup --dry-run</code>. Using <code>--dry-run</code> will show you any issues the backup might encounter, like permission errors or invalid syntax.</p> <p>If you do not see any errors, run it again without <code>--dry-run</code> to take your first backup. You can see your backups with <code>resticprofile -c ~/profiles.yaml --name &lt;backup-profile-name&gt; snapshots</code>.</p>","tags":["restic","resticprofile"]},{"location":"utilities/resticprofile/remotes/index.html","title":"Remote Backends","text":"<p>The sections in this documentation detail how to connect to Restic repositories on remote machines.</p>","tags":["restic","resticprofile"]},{"location":"utilities/resticprofile/remotes/pcloud.html","title":"pcloud","text":"<p>pCloud remote for <code>resticprofile</code> via <code>rclone</code>.</p>","tags":["restic","resticprofile"]},{"location":"utilities/resticprofile/remotes/pcloud.html#requirements","title":"Requirements","text":"<p><code>resticprofile</code> does not support <code>pCloud</code> as a destination by default, but you can pass a pre-configured <code>rclone</code> destination to the backup profile to connect to a Restic repository stored in pCloud.</p> <p>Install Rclone to get started.</p> <p>Tip</p> <p>On Linux, you can install Rclone with <code>sudo -v ; curl https://rclone.org/install.sh | sudo bash</code>.</p> <p>On Mac, you can install Rclone with Homebrew using <code>brew install rclone</code>, or with macports using <code>sudo port install rclone</code>.</p> <p>On Linux, you can install Rclone with Winget using <code>winget install Rclone.Rclone</code>, with Chocolatey using <code>choco install rclone</code>, or with scoop using <code>scoop install rclone</code>.</p>","tags":["restic","resticprofile"]},{"location":"utilities/resticprofile/remotes/pcloud.html#get-your-pcloud-token","title":"Get your pCloud token","text":"<p>Note</p> <p>The commands below do not work in a headless/CLI environment. It is assumed you have access to a machine with a GUI and <code>rclone</code> installed, where you can pre-authorize <code>resticprofile</code> by signing in and obtaining a JSON token to pass to your backup profile.</p> <p>Install <code>rclone</code> on a machine with a GUI where you can open a web browser and run this command:</p> <pre><code>rclone authorize \"pcloud\"\n</code></pre> <p>This will open a browser and prompt you to login to pCloud, and the terminal where you ran the authorize command will show you a JSON token. Save this to a password manager.</p>","tags":["restic","resticprofile"]},{"location":"utilities/resticprofile/remotes/pcloud.html#add-pcloud-to-rclone-configuration","title":"Add pCloud to Rclone configuration","text":"<p>On the machine where your <code>resticprofile</code> backup profile wants to use the pCloud remote, run <code>rclone config</code>. Enter <code>n</code> to set up a new remote, and give it a name, i.e. <code>pcloud</code>.</p> <p>When you are presented with a list of remotes, type <code>pcloud</code> and hit enter. Leave the <code>client_id</code> and <code>client_secret</code> by pressing Enter when prompted for them, then type <code>n</code> when prompted to edit advanced config.</p> <p>Answer <code>N</code> when you are asked if you want to use a web browser to automatically authenticate rclone with the remote.</p> <p>This will walk you through choosing a new remote, then <code>pcloud</code>, and you should answer <code>n</code> at the prompt to autoconfig. You will then be prompted to input your <code>config_token</code>; paste the JSON you copied from authenticating earlier and hit enter, then finish setting up your new remote.</p> <p>When you are finished setting up the pCloud remote, you will have a new line in your <code>~/.config/rclone/rclone.conf</code>.</p>","tags":["restic","resticprofile"]},{"location":"utilities/resticprofile/remotes/pcloud.html#setup-pcloud-remote-in-your-profilesyaml","title":"Setup pCloud remote in your profiles.yaml","text":"<p>Create a profile like this in your <code>profiles.yaml</code> for resticprofile:</p> pCloud remote for resticprofile<pre><code>## ...\n#  other options, like global, default, groups, etc\n\n## Backup to pCloud via rclone\nremote_pcloud:\n  inherit: default\n\n  ## Use rclone:&lt;remote-name&gt;: to tell resticprofile to use the rclone pcloud backend you configured.\n  repository: \"rclone:pcloud:path/in/pcloud/to/restic-repo\"\n  password-file: \"/path/to/remote_pcloud_main\"\n\n  env:\n    ## so 'sudo resticprofile' commands work\n    RCLONE_CONFIG: \"/home/user/.config/rclone/rclone.conf\"\n\n  backup:\n    source:\n      - \"/path/to/backup\"\n    read-concurrency: 4\n    skip-if-unchanged: true\n    verbose: true\n    exclude:\n      - \".tmp/\"\n      - \".cache/\"\n    exclude-cloud-files: true\n    group-by: \"tags,host,paths\"\n    schedule: \"daily\"\n    schedule-permission: \"system\"\n    schedule-priority: \"standard\"\n    schedule-lock-mode: default\n    schedule-lock-wait: 15m30s\n\n  forget:\n    keep-daily: 21\n    keep-weekly: 12\n    keep-monthly: 36\n    prune: true\n\n  check:\n    schedule: \"03:00\"\n    schedule-lock-wait: 30m\n\n  tags:\n    - remote\n    - remote_pcloud\n    - privileged\n</code></pre> <p>Now when you run <code>resticprofile -c ~/profiles.yaml --name remote_pcloud backup</code>, Restic will connect to pCloud and backup to the repository stored at the path you specified.</p>","tags":["restic","resticprofile"]},{"location":"utilities/resticprofile/remotes/ssh_sftp.html","title":"SSH/SFTP","text":"<p>SSH/SFTP remote connects to a repository on a remote machine via SSH.</p>","tags":["restic","resticprofile"]},{"location":"utilities/resticprofile/remotes/ssh_sftp.html#setup","title":"Setup","text":"","tags":["restic","resticprofile"]},{"location":"utilities/resticprofile/remotes/ssh_sftp.html#ssh-key-setup","title":"SSH key setup","text":"<p>Create an SSH key to connect to the remote with using:</p> Create SSH keypair<pre><code>ssh-keygen -t rsa -b 4096 -f ~/.ssh/resticprofile_id_rsa -N \"\"\n</code></pre> <p>Create/edit <code>~/.ssh/config</code> and add an entry for your remote host:</p> SSH config file<pre><code>Host your-remote\n  HostName &lt;IP or FQDN of remote host&gt;\n  User &lt;remote username&gt;\n  # Port 22  # uncomment and change this if SSH is running on a different port on the remote\n  IdentityFile ~/.ssh/resticprofile_id_rsa\n</code></pre> <p>Run resticprofile SSH backup as root/with sudo</p> <p><code>resticprofile</code> requires root/sudo permissions to backup any path the user would also need root access to interact with. When using SSH as a backup destination, you can add a line to <code>/root/.ssh/config</code> that maches the one you added in the setup instructions, except for <code>IdentityFile</code>, use the full path instead of <code>~</code>, i.e.:</p> Root user's ~/.ssh/config<pre><code>Host your-remote\n  HostName &lt;IP or FQDN of remote host&gt;\n  User &lt;remote username&gt;\n  # Port 22  # uncomment and change this if SSH is running on a different port on the remote\n  IdentityFile /home/&lt;username-you-run-resticprofile-as&gt;/resticprofile_id_rsa\n</code></pre> <p>Now you can run <code>sudo resticprofile -c /home/&lt;username-you-run-resticprofile-as&gt;/profiles.yaml ...</code> and connect to the remote repository as the root user.</p> <p>Copy the SSH key to your remote with <code>ssh-copy-id -i ~/.ssh/resticprofile_id_rsa username@&lt;ip-or-fqdn&gt;</code>, or manually copy the contents of <code>~/.ssh/resticprofile_id_rsa.pub</code> to the remote host's <code>~/.ssh/authorized_keys</code>.</p> <p>Remote host unknown</p> <p>The first time you connect to a remote host via SSH, you will see a warning about host authenticity, like:</p> <pre><code>The authenticity of host '111.222.333.444 (111.222.333.444)' can't be established.\nRSA key fingerprint is f3:cf:58:ae:71:0b:c8:04:6f:34:a3:b2:e4:1e:0c:8b.\nAre you sure you want to continue connecting (yes/no)? \n</code></pre> <p>You can proceed past this, but note that you might see a similar message when using <code>sudo resticprofile</code>. You will need to accept this warning the first time you can connect. Your scheduled backups to an SSH destination will not work until this warning has been acknowledged.</p>","tags":["restic","resticprofile"]},{"location":"utilities/resticprofile/remotes/ssh_sftp.html#setup-ssh-remote-in-your-profilesyaml","title":"Setup SSH remote in your profiles.yaml","text":"<p>Create a profile like this in your <code>profiles.yaml</code> for resticprofile (note: replace <code>&lt;host-name-in-ssh-config&gt;</code> with the <code>Host</code> value from your <code>~/.ssh/config</code> file):</p> SSH/SFTP remote for resticprofile<pre><code>## ...\n#  other options like global, default, and groups\n\nremote_ssh:\n  inherit: default\n  ## After initializing with the master key, you can create a new key and switch to it with:\n  #    $&gt; restic -r sftp:&lt;host-name-in-ssh-config&gt;:/remote/path/to/restic-repo key add\n  repository: sftp:&lt;host-name-in-ssh-config&gt;:/remote/path/to/restic-repo\n  password-file: \"/home/user/.restic/passwords/remote_callisto_user\"\n\n  backup:\n    verbose: true\n    source:\n      - \"/local/path/to/backup\"\n    read-concurrency: 4\n    skip-if-unchanged: true\n    exclude:\n      - \".tmp/\"\n      - \".cache/\"\n    exclude-cloud-files: true\n    group-by: \"tags,host,paths\"\n    schedule: \"daily\"\n    schedule-permission: \"system\"\n    schedule-priority: \"standard\"\n    schedule-lock-mode: default\n    schedule-lock-wait: 15m30s\n\n  tags:\n    - remote\n    - remote_callisto\n    - git\n    - privileged\n</code></pre> <p>Now you can run <code>resticprofile -c ~/profiles.yaml --name remote_ssh backup</code> (or with <code>sudo</code> for root/privileged backups).</p>","tags":["restic","resticprofile"]},{"location":"utilities/rsync/index.html","title":"Rsync","text":"<p><code>rsync</code> is a useful utility for synchronizing files. It can synch between hosts with SSH, locally between 2 directories, and more. A hosted version exists at <code>rsync.net</code>, offering a reliable, flexible solution for synchronizing files to a trusted remote.</p> <p>This page focuses on the <code>rsync</code> CLI utility for Linux.</p>","tags":["linux","utilities","bash"]},{"location":"utilities/rsync/index.html#installation","title":"Installation","text":"<p>Installing <code>rsync</code> on Linux is easy, the package exists in most repositories:</p> Install rsync<pre><code>## Debian/Ubuntu\nsudo apt install -y rsync\n\n## RedHat/Fedora/OpenSuSE\nsudo dnf install -y rsync\n\n## Alpine\nsudo apk add rsync\n</code></pre>","tags":["linux","utilities","bash"]},{"location":"utilities/rsync/index.html#usage","title":"Usage","text":"<p>Check <code>rsync</code>'s version with <code>rsync --version</code>. The commands in this documentation do not cover the full functionality of <code>rsync</code>. Rather, they're a reflection of how I've used the tool.</p>","tags":["linux","utilities","bash"]},{"location":"utilities/rsync/index.html#rsync-args","title":"Rsync Args","text":"<p>Note</p> <p>This list is not exhaustive. It's a cheat sheet I've made for myself. If I haven't used an arg, it will not be listed below.</p> <p>See a full list of <code>rsync</code> args, or check out an <code>rsync</code> cheat-sheet.</p> arg description -r Recursive copy (unnecessary with -a) -a Archive mode, includes recursive transfer -z Compress the data -v Verbose/detailed info during transfer -h Human readable output","tags":["linux","utilities","bash"]},{"location":"utilities/rsync/index.html#replace-cp-command-with-rsync-for-faster-transfers","title":"Replace cp command with rsync for faster transfers","text":"<p>Edit your <code>~/.bash_aliases</code> file:</p> ~/.bash_aliases<pre><code>## other aliases\n\n## Replace cp with rsync if rsync is installed\nif [ -x /usr/bin/rsync ]; then\n  alias cp=\"rsync --progress -auHxvz \"\nfi\n</code></pre>","tags":["linux","utilities","bash"]},{"location":"utilities/rsync/index.html#examples","title":"Examples","text":"","tags":["linux","utilities","bash"]},{"location":"utilities/rsync/index.html#sync-local-path-to-remote","title":"Sync local path to remote","text":"rsync local path to remote<pre><code>## Show a progress bar, archive &amp; compress data during transfer, show verbose &amp; human-readable output\nrsync -avzh --progress /local/path user@remote:/remote/path/\n</code></pre>","tags":["linux","utilities","bash"]},{"location":"utilities/rsync/index.html#sync-remote-path-to-local","title":"Sync remote path to local","text":"rsync remote path to local<pre><code>## Show a progress bar, archive &amp; compress data during transfer, show verbose &amp; human-readable output\nrsync -avzh --progress user@remote:/remote/path/ /local/path\n</code></pre>","tags":["linux","utilities","bash"]},{"location":"utilities/ssh/index.html","title":"Secure Shell (SSH)","text":"","tags":["utilities","ssh"]},{"location":"utilities/ssh/index.html#understanding-the-difference-between-public-and-private-keys","title":"Understanding the difference between public and private keys","text":"<p>I had a hard time understanding what to do with my private/public keys when I was learning SSH. I don't know why it was a difficult concept for me, but I have worked with enough other people who were confused in the same way I was that I think it's worth it to just spell out what to do with each key.</p>","tags":["utilities","ssh"]},{"location":"utilities/ssh/index.html#private-key-id_rsa","title":"Private key (id_rsa)","text":"<p>Your private key (default name is <code>id_rsa</code>) should NEVER leave the server it was created on, and should not be accessible to any other user (<code>chmod 600</code>). There are exceptions to this, such as when uploading a keypair to an Azure or Hashicorp vault, or providing to a Docker container. But in general, when creating SSH tunnels between machines, the private key is meant to stay on the machine it was created on.</p>","tags":["utilities","ssh"]},{"location":"utilities/ssh/index.html#public-key-id_rsapub","title":"Public key (id_rsa.pub)","text":"<p>Your public key can be freely shared. You can repeatedly derive the same public key from a given private key, but a private key cannot be derived from a public key.</p>","tags":["utilities","ssh"]},{"location":"utilities/ssh/index.html#more-on-how-it-works","title":"More on how it works","text":"<p>The math/algorithm involved allows your public key to encrypt data that only the private key it was derived from can decrypt. This is a one-way operation, meaning a message encrypted by a public key *cannot be decrypted by that same public key (or any other). Only the paired private key can decrypt messages encrypted by the public key.</p> Which is better, <code>ed25519</code> or <code>rsa</code>? <p>An SSH key can use one of 4 algorithms to generate a key: <code>RSA</code>, <code>ECDSA</code>, <code>Ed25519</code>, and <code>DSA</code> (but don't use <code>DSA</code>!). The default is <code>rsa 2048</code>, but it is recommended to use a higher key size like <code>4096</code>. The minimum is <code>1024</code>. RSA is the most compatible key type, it will work with just about every SSH server, but has a larger key size and the potential to be cracked by quantum computing if you use the standard <code>2048</code> key size.</p> <p>It is a hotly debated topic which is more secure, <code>ed25519</code> or <code>rsa</code>. This StackExchange answer lays out a high level overview of why <code>ed25519</code> may be better in theory, but after a certain point you are competing against \"cannot now and will not for the foreseeable future be broken,\" which becomes a meaningless endeavor.</p> <p>Short version: use <code>rsa</code> with <code>4096</code> bytes.</p> <p>Read more about SSH key algorithms on the Arch Linux wiki</p> <p>The public key is what you can send to the remote you want to connect to. For example, to add an SSH key to a Github repository, you copy the contents of your <code>id_rsa.pub</code> and paste them into Github, then tell your SSH client to use your <code>id_rsa</code> private key. When you connect, Github compares your public and private key and allows you to connect if you use the private key the public was derived from.</p> <p>Note</p> <p>Your keys are never sent in full, instead an algorithm reassembles a key that will be identical when derived from the same public/private key. This type of encryption is known as asymettrical encryption. </p> <p>When you're setting up an SSH connection to a remote host, you copy the public key using a command like <code>ssh-copy-id -i .ssh/id_rsa.pub user@host</code>, and you can then use your private key to connect instead of a password with <code>ssh -i .ssh/id_rsa user@host</code>.</p>","tags":["utilities","ssh"]},{"location":"utilities/ssh/index.html#create-an-ssh-key-pair","title":"Create an SSH key pair","text":"<p>You can create a keypair using the <code>ssh-keygen</code> utility. This is installed with SSH (<code>openssh-server</code> on Linux, see installation instructions for Windows), and is available cross-platform.</p> <p>Note</p> <p>You can run <code>$ ssh-keygen --help</code> on any platform to see a list of available commands. <code>--help</code> is not a valid flag, so you will see a warning <code>unknown option -- -</code>, but the purpose of this is to show available commands.</p> <p>If you run <code>ssh-keygen</code> without any arguments, you will be guided through a series of prompts, after which 2 files will be created (assuming you chose the defaults): <code>id_rsa</code> (your private key) and <code>id_rsa.pub</code> (your public key).</p> <p>You can also pass some parameters to automatically answer certain prompts:</p> <ul> <li>The <code>-f</code> parameter specifies the output file. This will skip the prompt <code>Enter file in which to save the key</code><ul> <li><code>$ ssh-keygen -f /path/to/&lt;ssh_key_filename&gt;</code></li> <li>When using <code>-f</code>, a private and public (<code>.pub</code>) key will be created</li> </ul> </li> <li>The <code>-t</code> parameter allows you to specify a key type<ul> <li>To generate an rsa key (the default key type): <code>$ ssh-keygen -t rsa</code></li> <li>Other types of key types are:<ul> <li><code>dsa</code></li> <li><code>ecdsa</code></li> <li><code>ecdsa-sk</code></li> <li><code>ed25519</code></li> <li><code>ed25519-sk</code></li> </ul> </li> </ul> </li> <li>The <code>-b</code> option allows you to specify the number of bits. For rsa keys, the minimum is <code>1024</code> and the default  is <code>3072</code>.</li> <li>You can set a stronger value like <code>4096</code> with: <code>$ ssh-keygen -b 4096</code></li> </ul> <p>Example ssh-keygen commands</p> Generate 4096-bit RSA key at ~/.ssh/example_key (and ~/.ssh/example_key.pub)<pre><code>## Generate a 4096-bit RSA key named example_key\nssh-keygen -t rsa -b 4096 -f ~/.ssh/example_key\n</code></pre> Skip prompt for password (Linux)<pre><code>## Generate a 4096-bit RSA key named example_key, skip password prompt\nssh-keygen -t rsa -b 4096 -f ~/.ssh/id_rsa -N \"\"\n</code></pre> Generate an ed25519 key<pre><code>## Generate an ed25519 key, skip password prompt.\n#  Default key name: id_ed25519\nssh-keygen -t ed25519 -f ~/.ssh/id_ed25519 -N \"\"\n</code></pre>","tags":["utilities","ssh"]},{"location":"utilities/ssh/index.html#install-an-ssh-key-on-a-remote-machine-for-passwordless-ssh-login","title":"Install an SSH key on a remote machine for passwordless ssh login","text":"<p>You can (and should) use your SSH key to authenticate as a user on a remote system. There are 2 ways of adding keys for this type of authentication.</p> Edit your ~/.ssh/config file <p>To connect to a remote server using a key, you have to pass <code>-i path/to/&lt;key_name&gt;</code> every time, i.e. <code>ssh -i ~/.ssh/id_rsa</code>.</p> <p>You can create/modify a <code>~/.ssh/config</code> file to set which user and key to use for various remote hosts. The <code>ssh</code> command can use this file to configure connections with SSH keys so you don't have to specify <code>$ ssh -i /path/to/key</code> each time you connect to a remote you've already copied a key to.</p> ~/.ssh/config<pre><code>## You can set $Host to whatever you want.\n#  You will connect with: ssh $Host\nHost example.com\n    ## The actual FQDN/IP address of the server\n    HostName example.com\n    ## If the remote SSH server is running on a\n    #  port other than the default 22, set here\n    # Port 222\n    ## The remote user your key is paired to\n    User test\n    ## The public key exists on the remote.\n    #  You provide the private key to complete the pair\n    IdentityFile ~/.ssh/&lt;your_ssh_key&gt;\n    ## On Windows, set \"ForwardAgent yes\" for VSCode remote editing.\n    #  Uncomment the line below if on Windows\n    # ForwardAgent yes\n</code></pre> <p>For example, after copying a public key to Github, add this to your <code>~/.ssh/config</code>:</p> Github SSH configuration<pre><code>Host github.com\n    HostName github.com\n    User git\n    IdentityFile ~/.ssh/id_rsa\n</code></pre> <p>Now, instead of connecting like <code>ssh -i ~/.ssh/id_rsa git@github.com:&lt;user&gt;/&lt;repo&gt;</code>, you can just run <code>ssh github.com</code>.</p> <p>Note that your <code>Host</code> and <code>HostName</code> do not need to match. For example, say you have a machine named <code>callisto</code> with an IP address of <code>192.168.1.12</code>, you can connect using <code>ssh callisto</code> by adding this to your <code>~/.ssh/config</code>:</p> Add entry for callisto<pre><code>Host callisto\n    HostName 192.168.1.12\n    User &lt;user-on-callisto&gt;\n    IdentityFile ~/.ssh/id_rsa\n    ## Set this if you are on Windows\n    ForwardAgent yes\n</code></pre>","tags":["utilities","ssh"]},{"location":"utilities/ssh/index.html#method-1-using-ssh-copy-id","title":"Method 1: Using ssh-copy-id","text":"<p>You can copy your public key to a remote using the <code>ssh-copy-id</code> utility.</p> Add an SSH key for user 'test' on host 'example'<pre><code>## Note: If you get a message about trusting the host, hit yes.\n#  You will need to type the remote user's password the first time\n$ ssh-copy-id -i ~/.ssh/your_key.pub test@example\n</code></pre>","tags":["utilities","ssh"]},{"location":"utilities/ssh/index.html#method-2-manual-method","title":"Method 2: Manual method","text":"<p>You can also manually copy your public keyfile (<code>.pub</code>) to a remote host and <code>cat</code> the contents into <code>~/.ssh/authorized_keys</code>. The most straightforward way of accomplishing this is to use <code>scp</code> to copy the keyfile to your remote host, typing the password to authenticate, then following up by logging in directly with <code>ssh</code>.</p> <p>Instructions:</p> <ul> <li>Copy your <code>.pub</code> keyfile<ul> <li><code>$ scp /path/on/local/to/id_rsa.pub &lt;remote-user&gt;@example.com:/home/&lt;remote-user&gt;</code></li> </ul> </li> <li>SSH into the remote<ul> <li><code>$ ssh &lt;remote-user&gt;@example.com</code></li> <li>You will need to type the user's password this time, but once the key is added you can simply use <code>$ ssh example.com</code></li> <li>Make sure you configure a <code>~/.ssh/config</code> file, using the instruction in the note in \"Install an SSH key on a Remote Machine for passwordless login\"</li> </ul> </li> <li>Move the <code>.pub</code> keyfile from the user's home into <code>.ssh</code><ul> <li>If the <code>.ssh</code> directory does not exist, create it with <code>mkdir .ssh</code></li> <li><code>$ mv id_rsa.pub .ssh</code></li> </ul> </li> <li>Change directory to <code>.ssh</code> and <code>cat</code> the contents of <code>id_rsa.pub</code> into <code>authorized_keys</code><ul> <li><code>$ cd .ssh &amp;&amp; cat id_rsa.pub authorized_keys</code></li> </ul> </li> <li>Remove the <code>id_rsa.pub</code> key. Now that it's in <code>authorized_keys</code>, you don't need the keyfile on the remote machine anymore.</li> </ul>","tags":["utilities","ssh"]},{"location":"utilities/ssh/index.html#ssh-chmod-permissions","title":"~/.ssh chmod permissions","text":"<p>It is crucial your <code>chmod</code> permissions are set properly on the <code>~/.ssh</code> directory. Invalid permissions will lead to errors when trying to <code>ssh</code> into remote machines.</p> <p>Check the table below for the <code>chmod</code> values you should use. To set a value (for example on the <code>.ssh</code> directory itself and the keypair):</p> set chmod<pre><code>$ chmod 700 ~/.ssh\n$ chmod 644 ~/.ssh/id_rsa{.pub}\n</code></pre> Dir/File Man Page Recommended Permission Mandatory Permission <code>~/.ssh/</code> There is no general requirement to keep the entire contents of this directory secret, but the recommended permissions are read/write/execute for the user, and not accessible by others. 700 <code>~/.ssh/authorized_keys</code> This file is not highly sensitive, but the recommended permissions are read/write for the user, and not accessible by others| 600 <code>~/.ssh/config</code> Because of the potential for abuse, this file must have strict permissions: read/write for the user, and not writable by others 600 <code>~/.ssh/identity</code> <code>~/.ssh/id_dsa</code> <code>~/.ssh/id_rsa</code> These files contain sensitive data and should be readable by the user but not accessible by others (read/write/execute) 600 <code>~/.ssh/identity.pub</code> <code>~/.ssh/id_dsa.pub</code> <code>~/.ssh/id_rsa.pub</code> Contains the public key for authentication. These files are not sensitive and can (but need not) be readable by anyone. 644 <p>(table data source: Superuser.com answer)</p>","tags":["utilities","ssh"]},{"location":"windows/index.html","title":"Windows","text":"<p>Notes/guides for Windows troubleshooting, administration, and development.</p>","tags":["windows"]},{"location":"windows/wsl/index.html","title":"Windows Subsystem for Linux (WSL)","text":"<p>Windows Subsystem for Linux (WSL) is a utility for running Linux under Windows. It's an impressive project that works very well, allowing you to have a nearly fully-featured Bash prompt (including systemd, if enabled) on a Windows machine.</p> <p>VSCode even supports developing inside a WSL machine, using the Remote Explorer plugin.</p>","tags":["windows","linux","wsl"]},{"location":"windows/wsl/backup-and-restore.html","title":"Backup &amp; Restore WSL distributions","text":"<p>WSL distributions can be backed up and restored using the <code>--export</code> and <code>--import</code> flags. This is useful for moving WSL distributions to new machines, creating backups before making modifications, and for restoring from \"base\" images.</p> <p>The WSL backup files are in <code>.tar</code> format.</p>","tags":["windows","wsl"]},{"location":"windows/wsl/backup-and-restore.html#backup-wsl","title":"Backup WSL","text":"<p>Backing up a WSL distribution involves compressing the entire virtual disk to a <code>.tar</code> file. The general command format is: <code>wsl --export &lt;distribution-name&gt; C:\\path\\to\\&lt;backup-name&gt;.tar</code>.</p> Backup distribution named 'debian' to C:\\wsl_backups<pre><code>wsl --export debian C:\\wsl_backups\\debian.tar\n</code></pre>","tags":["windows","wsl"]},{"location":"windows/wsl/backup-and-restore.html#restore-wsl","title":"Restore WSL","text":"<p>Restoring a WSL distribution involves decompressing an existing <code>.tar</code> file into a full clone of the source. The general command format is: <code>wsl --import &lt;new-distribution-name&gt; C:\\path\\to\\new-distribution-name C:\\path\\to\\&lt;old-distribution-name&gt;.tar</code></p> Create distribution named debian-new from C:\\wsl_backups\\debian.tar<pre><code>wsl --import debian-new C:\\wsl\\debian-new C:\\wsl_backups\\debian.tar\n</code></pre>","tags":["windows","wsl"]},{"location":"windows/wsl/backup-and-restore.html#clone-existing-wsl-into-new-image","title":"Clone existing WSL into new image","text":"<p>A useful practice is to create a \"base\" image that you can use to spawn off new WSL distributions as-needed. If you want a specific set of configurations, installed packages, etc, you can create a WSL distribution where you make all of these changes, then take a backup which you can repeatedly restore from for new distributions.</p> <p>For example, say you have a WSL distribution named <code>deb-base</code>. In this distribution, you have modified the <code>/etc/wsl.conf</code> file, installed <code>git</code>, <code>docker</code>, and <code>neovim</code>. You've modified the <code>~/.config/nvim</code> directory, setting up a customized environment for the <code>neovim</code> app. You've also installed <code>tmux</code> and modified <code>~/.tmux.config</code>.</p> <p>You do not want to repeat these steps each time you create a new WSL distribution. Pretend you now want to create a distribution named <code>deb-pydev</code>, where you will install Astral's <code>uv</code> project manager.</p> <ul> <li>First, create a backup of <code>deb-base</code><ul> <li><code>wsl --export deb-base C:\\wsl_backups\\deb-base.tar</code></li> </ul> </li> <li>Then, create a new distribution named <code>deb-pydev</code>, with the WSL VM's data stored in <code>C:\\wsl\\deb-pydev</code><ul> <li><code>wsl --import deb-pydev C:\\wsl\\deb-pydev C:\\wsl_backup\\deb-base.tar</code></li> </ul> </li> </ul>","tags":["windows","wsl"]},{"location":"windows/wsl/configure-wsl.html","title":"Configuring WSL","text":"<p>WSL configuration is done in 1 of 2 places:</p> <ul> <li>On the host side, editing <code>C:\\Users\\&lt;username&gt;\\.wslconfig</code><ul> <li>This file configures the WSL machine, and applies to all distributions installed.</li> <li>This is where you set options like <code>guiApplications</code>, <code>localhostForwarding</code>, etc.</li> </ul> </li> <li>On the WSL side, by editing <code>/etc/wsl.conf</code><ul> <li>Configures options for the specific distribution you edit this file from.</li> <li>Set options like the default user, enable systemd, and more</li> </ul> </li> </ul>","tags":["windows","wsl"]},{"location":"windows/wsl/configure-wsl.html#host-configurations-cuserswslconfig","title":"Host configurations (C:\\Users\\.wslconfig) <p>Global configurations</p> <ul> <li>Microsoft Docs: Global WSL configuration</li> </ul>","text":"","tags":["windows","wsl"]},{"location":"windows/wsl/configure-wsl.html#enable-gui-apps-in-wsl","title":"Enable GUI apps in WSL","text":"<p>You can add support for graphical programs (instead of just a Bash CLI) by enabling <code>guiApplications</code></p> Enable GUI support for WSL<pre><code>[wsl2]\nguiApplications=true\n</code></pre>","tags":["windows","wsl"]},{"location":"windows/wsl/configure-wsl.html#enabledisable-windows-firewall-rules-in-wsl","title":"Enable/disable Windows firewall rules in WSL","text":"<p>WSL can use the Windows Firewall rules when <code>firewall</code> is enabled.</p> Enable Windows Firewall rules in WSL<pre><code>[wsl2]\nfirewall=true\n</code></pre> Disable Windows Firewall rules in WSL<pre><code>[wsl2]\nfirewall=false\n</code></pre>","tags":["windows","wsl"]},{"location":"windows/wsl/configure-wsl.html#limit-wsl-memory","title":"Limit WSL memory","text":"Limit global WSL memory<pre><code>[wsl2]\nmemory=4GB\n</code></pre>","tags":["windows","wsl"]},{"location":"windows/wsl/configure-wsl.html#set-wsl-swap-amount","title":"Set WSL swap amount","text":"Set global WSL swap memory<pre><code>[wsl2]\nswap=8GB\n</code></pre> <p>You can also set a swap file disk on the host. The default is <code>%USERPROFILE%\\AppData\\Local\\Temp\\swap.vhdx</code>.</p> Set swap file<pre><code>swapfile=C:\\\\temp\\\\wsl-swap.vhdx\n</code></pre>","tags":["windows","wsl"]},{"location":"windows/wsl/configure-wsl.html#disable-wsl-page-reporting","title":"Disable WSL page reporting","text":"<p>Disabling page reporting for WSL causes it to retain all allocated memory claimed from Windows, releasing none back when free. NOT RECOMMENDED</p> Disable page reporting<pre><code>[wsl2]\npageReporting=false\n</code></pre>","tags":["windows","wsl"]},{"location":"windows/wsl/configure-wsl.html#forward-windows-host-network-connection-to-wsl","title":"Forward Windows host network connection to WSL","text":"<p>Turn on default connection to bind WSL 2 localhost to Windows localhost. Setting is ignored when <code>networkingMode=mirrored</code></p> Forward host localnet<pre><code>[wsl2]\nlocalhostforwarding=true\n</code></pre>","tags":["windows","wsl"]},{"location":"windows/wsl/configure-wsl.html#enabledisable-nested-virtualization-ie-docker-in-wsl","title":"Enable/disable nested virtualization, i.e. Docker in WSL","text":"Enable nested virtualization<pre><code>[wsl2]\nnestedVirtualization=true\n</code></pre> Disable nested virtualization<pre><code>[wsl2]\nnestedVirtualization=false\n</code></pre>","tags":["windows","wsl"]},{"location":"windows/wsl/configure-wsl.html#wsl-distribution-configurations-etcwslconf","title":"WSL distribution configurations (/etc/wsl.conf) <p>Configurations per-distribution.</p> <ul> <li>Microsoft Docs: wsl-config</li> </ul>","text":"","tags":["windows","wsl"]},{"location":"windows/wsl/configure-wsl.html#disable-joining-windows-path","title":"Disable joining Windows path","text":"<p>WSL will attempt to join the Windows <code>PATH</code> variable with its own <code>$PATH</code>. This can lead to unexpected behavior, like if <code>pyenv</code> is installed in both Windows and WSL.</p> <p>To fix this, disable the <code>appendWindowsPath</code> flag in the <code>[interop]</code> section of <code>/etc/wsl.conf</code></p> Disable Windows PATH join<pre><code>## /etc/wsl.conf\n\n...\n\n[interop]\nappendWindowsPath = false\n\n...\n</code></pre>","tags":["windows","wsl"]},{"location":"windows/wsl/configure-wsl.html#set-default-user","title":"Set default user","text":"<p>Note</p> <p>To set a default user inside the WSL distribution, the user account must exist. When you first run a WSL container, you will be prompted to create a user account.</p> <p>You can create additional users with the <code>useradd</code> command:</p> Create new Linux user in WSL container<pre><code>sudo adduser &lt;username&gt;\n</code></pre> Set default WSL user<pre><code>[user]\ndefault=&lt;username&gt;\n</code></pre>","tags":["windows","wsl"]},{"location":"windows/wsl/configure-wsl.html#enable-systemd","title":"Enable systemd","text":"Enable systemd in WSL<pre><code>[boot]\nsystemd=true\n</code></pre>","tags":["windows","wsl"]},{"location":"windows/wsl/configure-wsl.html#enabledisable-automounting-of-windows-drives","title":"Enable/disable automounting of Windows drives","text":"<p>By default, Windows will mount all fixed drives (i.e. <code>C:\\</code>, <code>D:\\</code>, etc) in the container at <code>/mnt/&lt;driveletter&gt;</code>. This feature can be controlled with the <code>enabled</code> flag in <code>[automount]</code></p> Enable automounting Windows drives in WSL<pre><code>[automount]\nenabled=true\n</code></pre> Disable automounting Windows drives in WSL<pre><code>[automount]\nenabled=false\n</code></pre>","tags":["windows","wsl"]},{"location":"windows/wsl/configure-wsl.html#control-mounts-from-within-wsls-etcfstab","title":"Control mounts from within WSL's /etc/fstab","text":"<p>To mount extra paths inside the WSL container, i.e. an SMB share, you can modify the <code>/etc/fstab</code> the same way you would on a \"full\" Linux install, but you must also enable <code>mountFsTab</code>.</p> Enable auto-mount WSL's /etc/fstab<pre><code>[automount]\nmountFsTab=true\n</code></pre> Disable auto-mount WSL's /etc/fstab<pre><code>[automount]\nmountFsTab=false\n</code></pre>","tags":["windows","wsl"]},{"location":"windows/wsl/configure-wsl.html#control-default-root-directory","title":"Control default root directory","text":"<p>When starting up a WSL distribution, your terminal's CWD will be the path you ran <code>wsl</code> from in Windows. For example, if you are in <code>C:\\Users\\&lt;user&gt;</code> and run <code>wsl</code>, the WSL distribution's prompt will be <code>/mnt/c/Users/&lt;user&gt;</code>.</p> <p>The default WSL root directory is <code>/mnt</code>. To set a different path, edit the <code>root</code> flag in the <code>[automount]</code> section.</p> Change default WSL root directory<pre><code>[automount]\nroot=/home/&lt;user&gt;\n</code></pre>","tags":["windows","wsl"]},{"location":"windows/wsl/configure-wsl.html#set-a-hostname-for-wsl-distribution","title":"Set a hostname for WSL distribution","text":"<p>By default, the WSL distribution's hostname will be the same as the Windows host. This can be modified by changing the <code>hostname</code> flag in the <code>[network]</code> section.</p> Set WSL hostname<pre><code>[network]\nhostname=\"your-hostname\"\n</code></pre>","tags":["windows","wsl"]},{"location":"windows/wsl/install-wsl.html","title":"Install WSL on Windows","text":"<ul> <li>\ud83d\udcda Microsoft Docs: Install WSL</li> </ul> <p>Note</p> <p>Most recent versions of Windows will come with <code>wsl</code> pre-installed. Run the <code>wsl --version</code> command to check if you already have WSL.</p> <p>If you see a version when you run <code>wsl --version</code>, you can simply run <code>wsl --install</code> to install an Ubuntu image. If you want to use a different version of Linux, you can run <code>wsl --install &lt;distro-name&gt;</code>.</p> <p>For example, to install Debian:</p> Install a WSL distribution<pre><code>wsl --install debian\n</code></pre> <p>Run with no distribution name to install Ubuntu</p> Install Ubuntu in WSL<pre><code>wsl --install\n</code></pre> <p>To change the default distribution that executes when you run <code>wsl</code> with no <code>-d &lt;distribution-name&gt;</code>, use <code>--setdefault</code></p> Set default distribution for WSL<pre><code>wsl --setdefault &lt;distribution-name&gt;\n</code></pre>","tags":["windows","wsl","linux"]},{"location":"windows/wsl/install-wsl.html#install-on-older-versions-of-windows","title":"Install on older versions of Windows","text":"<p>On older versions of Windows, if this command fails, you can install <code>wsl</code> with the following steps:</p> <ul> <li>Enable Windows Subsystem for Linux:</li> </ul> Enable WSL<pre><code>dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart\n</code></pre> <ul> <li>Enable virtual machine feature</li> </ul> Enable virtual machine<pre><code>dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart\n</code></pre> <ul> <li>Download and install the WSL2 Linux kernel update package</li> <li>WSL2 Linux Kernel Download</li> <li> <p>Run the installer after download</p> </li> <li> <p>Set WSL version 2 as default</p> </li> </ul> Set WSL2 as default version<pre><code>wsl --set-default-version 2\n</code></pre> <ul> <li>Install a distribution, i.e. Debian</li> </ul> Install Debian Linux in WSL<pre><code>wsl --install -d Debian\n</code></pre>","tags":["windows","wsl","linux"]},{"location":"windows/wsl/troubleshooting.html","title":"Troubleshooting WSL","text":"<p>When I run into issues with WSL and solve them, I add the problem &amp; solution to this page. Notes may be sparse, I normally only care about what I need to copy/paste to get things up and running.</p>","tags":["windows","wsl"]},{"location":"windows/wsl/troubleshooting.html#problem-signature-errors-while-working-with-azure-libraries","title":"Problem: Signature errors while working with Azure libraries","text":"<p>While working with Azure libraries in WSL, you might see errors about signatures. Try the following.</p>","tags":["windows","wsl"]},{"location":"windows/wsl/troubleshooting.html#solution-set-ntpdate","title":"Solution: Set ntpdate","text":"<p>Edit <code>/etc/wsl.conf</code> and modify the <code>[boot]</code> section as follows. If you do not see a <code>[boot]</code> section, simply create it, and if any of the options below are already present, do not repeat them:</p> Modify /etc/wsl.conf to fix Azure tool signature errors<pre><code>## /etc/wsl.conf\n\n...\n\n[boot]\nsystemd=true\ncommand=\"ntpdate ntp.ubuntu.com\"\n\n...\n</code></pre> <p>After modifying this file you will need to restart WSL. You can do this with <code>wsl --shutdown</code>, then re-launching your WSL distribution.</p>","tags":["windows","wsl"]},{"location":"windows/wsl/troubleshooting.html#problem-ping-doesnt-work-in-wsl","title":"Problem: Ping doesn't work in WSL","text":"<p>When trying to run <code>ping</code> in a WSL distribution, you may see an error like this:</p> Ping error in WSL container<pre><code>ping: socktype: SOCK_RAW\nping: socket: Operation not permitted\nping: =&gt; missing cap_net_raw+p capability or setuid?\n</code></pre> <p>The problem here is the line <code>missing cap_net_raw+p capability</code>.</p>","tags":["windows","wsl"]},{"location":"windows/wsl/troubleshooting.html#fix-add-cap_net_raw-permission","title":"Fix: Add cap_net_raw permission","text":"<p>In the WSL container, run this comand:</p> Add cap_net_raw capability to WSL distribution<pre><code>sudo setcap cap_net_raw+p /bin/ping\n</code></pre>","tags":["windows","wsl"]},{"location":"windows/wsl/troubleshooting.html#fix-for-all-wsl2-distributions","title":"Fix for all WSL2 distributions","text":"<p>To fix this for all distributions, you can modify the <code>kernelCommandLine</code> flag in the <code>[wsl2]</code> section of <code>%USERPROFILE\\.wslconfig</code>.</p> Fix ping for all WSL2 distributions<pre><code>## %USERPROFILE\\.wslconfig\n\n[wsl2]\nkernelCommandLine = sysctl.net.ipv4.ping_group_range=\\\"0 2147483647\\\"\n</code></pre>","tags":["windows","wsl"]},{"location":"windows/wsl/troubleshooting.html#problem-wsl-is-completely-frozen","title":"Problem: WSL is completely frozen","text":"<p>This happens sometimes when I'm using the VSCode remote extension to connect to a WSL distribution and the computer goes to sleep. WSL becomes completely responsive, ignoring all commands including <code>wsl --shutdown</code>.</p>","tags":["windows","wsl"]},{"location":"windows/wsl/troubleshooting.html#fix-kill-the-wslserviceexe-task","title":"Fix: Kill the wslservice.exe task","text":"Kill wslservice.exe<pre><code>taskkill /f /im wslservice.exe\n</code></pre>","tags":["windows","wsl"]},{"location":"windows/wsl/troubleshooting.html#problem-git-is-not-working-in-wsl","title":"Problem: Git is not working in WSL","text":"<p>When <code>git</code> is installed on both the Windows and WSL side, you will often run into errors, specifically around authentication.</p>","tags":["windows","wsl"]},{"location":"windows/wsl/troubleshooting.html#fix-use-windows-git-credential-manager-in-wsl","title":"Fix: Use Windows git credential manager in WSL","text":"<p>Run the following 2 commands, the first on the Windows side and the second in a WSL distribution.</p> Run on Windows host<pre><code>git config --global credential.helper wincred\n</code></pre> Run in WSL distribution<pre><code>git config --global credential.helper \"/mnt/c/Program\\ Files/Git/mingw64/bin/git-credential-manager.exe\n</code></pre> <p>If your Git remote is Azuree DevOps, you also need to run:</p> Enable support for Azure DevOps repositories<pre><code>git config --global credential.https://dev.azure.com.useHttpPath true\n</code></pre>","tags":["windows","wsl"]},{"location":"tags.html","title":"tags","text":""},{"location":"tags.html#adblocking","title":"adblocking","text":"<ul> <li>Adguard home</li> </ul>"},{"location":"tags.html#adguard","title":"adguard","text":"<ul> <li>Adguard home</li> </ul>"},{"location":"tags.html#alembic","title":"alembic","text":"<ul> <li>Alembic</li> <li>Alembic</li> <li>Alembic env.py file</li> </ul>"},{"location":"tags.html#ansible","title":"ansible","text":"<ul> <li>Ansible</li> <li>Ansible</li> <li>Inventories</li> <li>SSH Setup</li> </ul>"},{"location":"tags.html#arch","title":"arch","text":"<ul> <li>Tips &amp; Tricks</li> </ul>"},{"location":"tags.html#args","title":"args","text":"<ul> <li>Argument Parsing</li> <li>Argparse</li> <li>Cyclopts</li> </ul>"},{"location":"tags.html#automation","title":"automation","text":"<ul> <li>Custom Nox Sessions</li> <li>Automation</li> <li>N8n</li> <li>Prefect server</li> <li>Ansible</li> <li>Inventories</li> <li>SSH Setup</li> </ul>"},{"location":"tags.html#backup","title":"backup","text":"<ul> <li>Restic</li> <li>Restic Cheatsheet</li> <li>Ignore/exclude files</li> <li>Handling repository passwords</li> <li>Scripts &amp; Schedules</li> <li>Restic Setup</li> <li>Resticprofile</li> </ul>"},{"location":"tags.html#bash","title":"bash","text":"<ul> <li>Tips &amp; Tricks</li> <li>Bash</li> <li>Bash Snippets</li> <li>One liners</li> <li>Scripts</li> <li>Parse CLI args</li> <li>Ntfy</li> <li>Restic</li> <li>Resticprofile</li> <li>Rsync</li> </ul>"},{"location":"tags.html#beszel","title":"beszel","text":"<ul> <li>Beszel</li> </ul>"},{"location":"tags.html#celery","title":"celery","text":"<ul> <li>Dynaconf Celery configuration</li> </ul>"},{"location":"tags.html#cheatsheet","title":"cheatsheet","text":"<ul> <li>Restic Cheatsheet</li> </ul>"},{"location":"tags.html#cli","title":"cli","text":"<ul> <li>Argument Parsing</li> <li>Argparse</li> <li>Cyclopts</li> </ul>"},{"location":"tags.html#configuration","title":"configuration","text":"<ul> <li>Profiles</li> <li>Logging</li> <li>Dynaconf</li> <li>The ~/.pypirc file</li> </ul>"},{"location":"tags.html#context-managers","title":"context-managers","text":"<ul> <li>Context Managers</li> </ul>"},{"location":"tags.html#controllers","title":"controllers","text":"<ul> <li>Controller class snippets</li> <li>Minio</li> </ul>"},{"location":"tags.html#cyclopts","title":"cyclopts","text":"<ul> <li>Cyclopts</li> <li>Building a CLI app with cyclopts</li> <li>Call a cyclopts CLI from another Python script</li> </ul>"},{"location":"tags.html#data","title":"data","text":"<ul> <li>Prefect server</li> </ul>"},{"location":"tags.html#database","title":"database","text":"<ul> <li>Alembic</li> <li>Sqlite</li> <li>Convert JSON to SQLite</li> <li>Dynaconf database configuration</li> <li>Alembic</li> <li>Alembic env.py file</li> <li>Sqlalchemy</li> <li>app/core/database</li> <li>Databases</li> <li>Influxdb</li> <li>Mariadb</li> <li>Nocodb</li> <li>Postgres</li> <li>Redis</li> </ul>"},{"location":"tags.html#dataclasses","title":"dataclasses","text":"<ul> <li>Dataclasses</li> </ul>"},{"location":"tags.html#debian","title":"debian","text":"<ul> <li>Tips &amp; Tricks</li> <li>Debian Networking</li> </ul>"},{"location":"tags.html#django","title":"django","text":"<ul> <li>Django</li> </ul>"},{"location":"tags.html#docker","title":"docker","text":"<ul> <li>The Docker CLI</li> <li>Docker Compose</li> <li>Tips &amp; Tricks</li> <li>The Dockerfile</li> <li>Template</li> <li>Docker</li> <li>Automation</li> <li>N8n</li> <li>Prefect server</li> <li>Databases</li> <li>Influxdb</li> <li>Mariadb</li> <li>Nocodb</li> <li>Postgres</li> <li>Prometheus</li> <li>Redis</li> <li>Paperless ngx</li> <li>Openvscode server</li> <li>Mosquitto</li> <li>Rabbitmq</li> <li>Beszel</li> <li>Adguard home</li> <li>Nginx proxy manager</li> <li>Unifi controller</li> <li>Wg easy</li> <li>Portainer</li> <li>Python</li> <li>Minio s3 storage</li> </ul>"},{"location":"tags.html#dynaconf","title":"dynaconf","text":"<ul> <li>Dynaconf</li> <li>Dynaconf</li> <li>Custom configs</li> <li>Dynaconf Celery configuration</li> <li>Dynaconf database configuration</li> <li>Dynaconf FastAPI configuration</li> <li>Dynaconf Minio configuration</li> <li>Dynaconf Uvicorn Configuration</li> </ul>"},{"location":"tags.html#environment","title":"environment","text":"<ul> <li>Use pyenv for easier Python version management</li> <li>Dynaconf</li> </ul>"},{"location":"tags.html#fastapi","title":"fastapi","text":"<ul> <li>Fastapi</li> <li>Dynaconf FastAPI configuration</li> </ul>"},{"location":"tags.html#fedora","title":"fedora","text":"<ul> <li>Tips &amp; Tricks</li> </ul>"},{"location":"tags.html#git","title":"git","text":"<ul> <li>Git</li> <li>Rewrite Git History</li> <li>Git sparse checkouts</li> <li>Base Python .gitignore</li> </ul>"},{"location":"tags.html#http","title":"http","text":"<ul> <li>Ntfy</li> </ul>"},{"location":"tags.html#ide","title":"ide","text":"<ul> <li>Ad-Hoc, Jupyter-like code \"cells\"</li> </ul>"},{"location":"tags.html#imports","title":"imports","text":"<ul> <li>Package imports</li> </ul>"},{"location":"tags.html#influxdb","title":"influxdb","text":"<ul> <li>Influxdb</li> </ul>"},{"location":"tags.html#jupyter","title":"jupyter","text":"<ul> <li>Ad-Hoc, Jupyter-like code \"cells\"</li> <li>Jupyter</li> <li>Sessions</li> <li>nbstripout</li> </ul>"},{"location":"tags.html#linux","title":"linux","text":"<ul> <li>Linux</li> <li>Tips &amp; Tricks</li> <li>Bash</li> <li>Debian Networking</li> <li>Logrotate</li> <li>Restic</li> <li>Restic Cheatsheet</li> <li>Ignore/exclude files</li> <li>Handling repository passwords</li> <li>Scripts &amp; Schedules</li> <li>Restic Setup</li> <li>Resticprofile</li> <li>Rsync</li> <li>Wsl</li> <li>Install WSL on Windows</li> </ul>"},{"location":"tags.html#llm","title":"llm","text":"<ul> <li>Ollama</li> </ul>"},{"location":"tags.html#logging","title":"logging","text":"<ul> <li>Logging</li> <li>configuring a logging.Logger for nox</li> <li>Logrotate</li> </ul>"},{"location":"tags.html#mac","title":"mac","text":"<ul> <li>Restic</li> <li>Restic Cheatsheet</li> <li>Ignore/exclude files</li> <li>Handling repository passwords</li> <li>Scripts &amp; Schedules</li> <li>Restic Setup</li> <li>Resticprofile</li> </ul>"},{"location":"tags.html#mariadb","title":"mariadb","text":"<ul> <li>Mariadb</li> </ul>"},{"location":"tags.html#messaging","title":"messaging","text":"<ul> <li>Mosquitto</li> <li>Rabbitmq</li> </ul>"},{"location":"tags.html#minio","title":"minio","text":"<ul> <li>Dynaconf Minio configuration</li> <li>Minio</li> </ul>"},{"location":"tags.html#mkdocs","title":"mkdocs","text":"<ul> <li>Mkdocs</li> <li>MkDocs</li> </ul>"},{"location":"tags.html#monitoring","title":"monitoring","text":"<ul> <li>Beszel</li> </ul>"},{"location":"tags.html#mosquitto","title":"mosquitto","text":"<ul> <li>Mosquitto</li> </ul>"},{"location":"tags.html#mqtt","title":"mqtt","text":"<ul> <li>Mosquitto</li> <li>Rabbitmq</li> </ul>"},{"location":"tags.html#mysql","title":"mysql","text":"<ul> <li>Mariadb</li> </ul>"},{"location":"tags.html#networking","title":"networking","text":"<ul> <li>Debian Networking</li> <li>Wg easy</li> </ul>"},{"location":"tags.html#nginx","title":"nginx","text":"<ul> <li>Nginx proxy manager</li> </ul>"},{"location":"tags.html#nocodb","title":"nocodb","text":"<ul> <li>Nocodb</li> </ul>"},{"location":"tags.html#nocode","title":"nocode","text":"<ul> <li>N8n</li> <li>Nocodb</li> </ul>"},{"location":"tags.html#nox","title":"nox","text":"<ul> <li>Nox</li> <li>Custom Nox Sessions</li> <li>configuring a logging.Logger for nox</li> <li>Nox extra module</li> <li>Sessions</li> <li>Ansible</li> <li>Django</li> <li>MkDocs</li> <li>nbstripout</li> <li>pre-commit</li> <li>pytest</li> </ul>"},{"location":"tags.html#nox_extra","title":"nox_extra","text":"<ul> <li>Nox extra module</li> </ul>"},{"location":"tags.html#one-liners","title":"one-liners","text":"<ul> <li>One liners</li> <li>One liners</li> <li>Scripts</li> </ul>"},{"location":"tags.html#pdm","title":"pdm","text":"<ul> <li>Use PDM to manage your Python projects &amp; dependencies</li> <li>Pdm</li> <li>pyproject.toml scripts</li> </ul>"},{"location":"tags.html#portainer","title":"portainer","text":"<ul> <li>Portainer</li> </ul>"},{"location":"tags.html#postgres","title":"postgres","text":"<ul> <li>Postgres</li> </ul>"},{"location":"tags.html#powershell","title":"powershell","text":"<ul> <li>Powershell</li> <li>Profiles</li> <li>Snippets</li> <li>Powershell Snippets</li> <li>One liners</li> <li>Scripts</li> <li>Ntfy</li> </ul>"},{"location":"tags.html#pre-commit","title":"pre-commit","text":"<ul> <li>Sessions</li> <li>nbstripout</li> <li>pre-commit</li> <li>Pre commit</li> </ul>"},{"location":"tags.html#programming","title":"programming","text":"<ul> <li>Argument Parsing</li> <li>Argparse</li> <li>Cyclopts</li> <li>Alembic</li> </ul>"},{"location":"tags.html#prometheus","title":"prometheus","text":"<ul> <li>Prometheus</li> </ul>"},{"location":"tags.html#pyenv","title":"pyenv","text":"<ul> <li>Use pyenv for easier Python version management</li> </ul>"},{"location":"tags.html#pypi","title":"pypi","text":"<ul> <li>The ~/.pypirc file</li> </ul>"},{"location":"tags.html#pytest","title":"pytest","text":"<ul> <li>Sessions</li> <li>nbstripout</li> <li>pytest</li> <li>Pytest</li> <li>Pytest fixture templates</li> </ul>"},{"location":"tags.html#python","title":"python","text":"<ul> <li>Ad-Hoc, Jupyter-like code \"cells\"</li> <li>Jupyter</li> <li>Mkdocs</li> <li>Python</li> <li>Dataclasses</li> <li>Logging</li> <li>Package imports</li> <li>Use PDM to manage your Python projects &amp; dependencies</li> <li>Python project structure</li> <li>Use pyenv for easier Python version management</li> <li>Using rich to enhance your console output</li> <li>Use virtualenv to manage dependencies</li> <li>Argument Parsing</li> <li>Argparse</li> <li>Cyclopts</li> <li>Building a CLI app with cyclopts</li> <li>Call a cyclopts CLI from another Python script</li> <li>Context Managers</li> <li>SQLAlchemy</li> <li>DBConfig settings class</li> <li>Table Mixin Classes</li> <li>Annotated columns &amp; custom types</li> <li>SQLAlchemy dependency methods</li> <li>Repositories</li> <li>SQLAlchemy Base class</li> <li>Validators</li> <li>Alembic</li> <li>Dynaconf</li> <li>Fastapi</li> <li>Nox</li> <li>Custom Nox Sessions</li> <li>configuring a logging.Logger for nox</li> <li>Nox extra module</li> <li>Sessions</li> <li>Ansible</li> <li>Django</li> <li>MkDocs</li> <li>nbstripout</li> <li>pre-commit</li> <li>pytest</li> <li>Sqlite</li> <li>Convert JSON to SQLite</li> <li>Standard project files</li> <li>Python</li> <li>Base Python .gitignore</li> <li>The ~/.pypirc file</li> <li>Dynaconf</li> <li>Custom configs</li> <li>Dynaconf Celery configuration</li> <li>Dynaconf database configuration</li> <li>Dynaconf FastAPI configuration</li> <li>Dynaconf Minio configuration</li> <li>Dynaconf Uvicorn Configuration</li> <li>Alembic</li> <li>Alembic env.py file</li> <li>Pdm</li> <li>pyproject.toml scripts</li> <li>Pytest</li> <li>Pytest fixture templates</li> <li>Ruff</li> <li>Sqlalchemy</li> <li>app/core/database</li> <li>Python Snippets</li> <li>Controller class snippets</li> <li>Minio</li> <li>Path snippets</li> <li>Prefect server</li> <li>Python</li> <li>Ntfy</li> </ul>"},{"location":"tags.html#python-stdlib","title":"python-stdlib","text":"<ul> <li>Argparse</li> </ul>"},{"location":"tags.html#rabbitmq","title":"rabbitmq","text":"<ul> <li>Rabbitmq</li> </ul>"},{"location":"tags.html#redis","title":"redis","text":"<ul> <li>Redis</li> </ul>"},{"location":"tags.html#reference","title":"reference","text":"<ul> <li>Tips &amp; Tricks</li> <li>Tips &amp; Tricks</li> </ul>"},{"location":"tags.html#restic","title":"restic","text":"<ul> <li>Restic Cheatsheet</li> <li>Ignore/exclude files</li> <li>Handling repository passwords</li> <li>Scripts &amp; Schedules</li> <li>Restic Setup</li> <li>Resticprofile</li> <li>Cleanup</li> <li>Excluding paths in a backup</li> <li>profiles.yaml</li> <li>Setup</li> <li>Remotes</li> <li>pcloud</li> <li>SSH/SFTP</li> </ul>"},{"location":"tags.html#resticprofile","title":"resticprofile","text":"<ul> <li>Resticprofile</li> <li>Cleanup</li> <li>Excluding paths in a backup</li> <li>profiles.yaml</li> <li>Setup</li> <li>Remotes</li> <li>pcloud</li> <li>SSH/SFTP</li> </ul>"},{"location":"tags.html#rich","title":"rich","text":"<ul> <li>Using rich to enhance your console output</li> </ul>"},{"location":"tags.html#ruff","title":"ruff","text":"<ul> <li>Ruff</li> </ul>"},{"location":"tags.html#s3","title":"s3","text":"<ul> <li>Minio</li> <li>Minio s3 storage</li> </ul>"},{"location":"tags.html#scripts","title":"scripts","text":"<ul> <li>Scripts</li> <li>Parse CLI args</li> </ul>"},{"location":"tags.html#snippet","title":"snippet","text":"<ul> <li>Ad-Hoc, Jupyter-like code \"cells\"</li> </ul>"},{"location":"tags.html#snippets","title":"snippets","text":"<ul> <li>Snippets</li> <li>Snippets</li> <li>Bash Snippets</li> <li>One liners</li> <li>Scripts</li> <li>Parse CLI args</li> <li>Powershell Snippets</li> <li>One liners</li> <li>Scripts</li> <li>Python Snippets</li> <li>Controller class snippets</li> <li>Minio</li> <li>Path snippets</li> </ul>"},{"location":"tags.html#sqlalchemy","title":"sqlalchemy","text":"<ul> <li>SQLAlchemy</li> <li>DBConfig settings class</li> <li>Table Mixin Classes</li> <li>Annotated columns &amp; custom types</li> <li>SQLAlchemy dependency methods</li> <li>Repositories</li> <li>SQLAlchemy Base class</li> <li>Validators</li> <li>Sqlalchemy</li> <li>app/core/database</li> </ul>"},{"location":"tags.html#sqlite","title":"sqlite","text":"<ul> <li>Sqlite</li> <li>Convert JSON to SQLite</li> </ul>"},{"location":"tags.html#ssh","title":"ssh","text":"<ul> <li>Ansible</li> <li>SSH Setup</li> <li>Ssh</li> </ul>"},{"location":"tags.html#standard-project-files","title":"standard-project-files","text":"<ul> <li>Nox</li> <li>configuring a logging.Logger for nox</li> <li>Nox extra module</li> <li>Sessions</li> <li>Ansible</li> <li>Django</li> <li>MkDocs</li> <li>nbstripout</li> <li>pre-commit</li> <li>pytest</li> <li>Standard project files</li> <li>Pre commit</li> <li>Python</li> <li>Base Python .gitignore</li> <li>The ~/.pypirc file</li> <li>Dynaconf</li> <li>Custom configs</li> <li>Dynaconf Celery configuration</li> <li>Dynaconf database configuration</li> <li>Dynaconf FastAPI configuration</li> <li>Dynaconf Minio configuration</li> <li>Dynaconf Uvicorn Configuration</li> <li>Alembic</li> <li>Alembic env.py file</li> <li>Pdm</li> <li>pyproject.toml scripts</li> <li>Pytest</li> <li>Pytest fixture templates</li> <li>Ruff</li> <li>Sqlalchemy</li> <li>app/core/database</li> </ul>"},{"location":"tags.html#stdlib","title":"stdlib","text":"<ul> <li>Dataclasses</li> <li>Logging</li> <li>Package imports</li> </ul>"},{"location":"tags.html#storage","title":"storage","text":"<ul> <li>Minio s3 storage</li> </ul>"},{"location":"tags.html#template","title":"template","text":"<ul> <li>Portainer</li> </ul>"},{"location":"tags.html#templates","title":"templates","text":"<ul> <li>Template</li> <li>Docker</li> <li>Automation</li> <li>N8n</li> <li>Prefect server</li> <li>Databases</li> <li>Influxdb</li> <li>Mariadb</li> <li>Nocodb</li> <li>Postgres</li> <li>Prometheus</li> <li>Redis</li> <li>Paperless ngx</li> <li>Openvscode server</li> <li>Mosquitto</li> <li>Rabbitmq</li> <li>Beszel</li> <li>Adguard home</li> <li>Nginx proxy manager</li> <li>Unifi controller</li> <li>Wg easy</li> <li>Python</li> <li>Minio s3 storage</li> </ul>"},{"location":"tags.html#terminal","title":"terminal","text":"<ul> <li>Alacritty</li> </ul>"},{"location":"tags.html#unifi","title":"unifi","text":"<ul> <li>Unifi controller</li> </ul>"},{"location":"tags.html#utilities","title":"utilities","text":"<ul> <li>Cyclopts</li> <li>Building a CLI app with cyclopts</li> <li>Call a cyclopts CLI from another Python script</li> <li>Utilities</li> <li>Alacritty</li> <li>Logrotate</li> <li>Ntfy</li> <li>Ollama</li> <li>Restic</li> <li>Restic Cheatsheet</li> <li>Ignore/exclude files</li> <li>Handling repository passwords</li> <li>Scripts &amp; Schedules</li> <li>Restic Setup</li> <li>Resticprofile</li> <li>Rsync</li> <li>Ssh</li> </ul>"},{"location":"tags.html#uvicorn","title":"uvicorn","text":"<ul> <li>Dynaconf Uvicorn Configuration</li> </ul>"},{"location":"tags.html#virtualenv","title":"virtualenv","text":"<ul> <li>Use virtualenv to manage dependencies</li> </ul>"},{"location":"tags.html#vpn","title":"vpn","text":"<ul> <li>Wg easy</li> </ul>"},{"location":"tags.html#vscode","title":"vscode","text":"<ul> <li>Ad-Hoc, Jupyter-like code \"cells\"</li> </ul>"},{"location":"tags.html#windows","title":"windows","text":"<ul> <li>Powershell</li> <li>Profiles</li> <li>Snippets</li> <li>Restic</li> <li>Restic Cheatsheet</li> <li>Ignore/exclude files</li> <li>Handling repository passwords</li> <li>Scripts &amp; Schedules</li> <li>Restic Setup</li> <li>Resticprofile</li> <li>Windows</li> <li>Wsl</li> <li>Backup &amp; Restore WSL distributions</li> <li>Configuring WSL</li> <li>Install WSL on Windows</li> <li>Troubleshooting WSL</li> </ul>"},{"location":"tags.html#wireguard","title":"wireguard","text":"<ul> <li>Wg easy</li> </ul>"},{"location":"tags.html#wsl","title":"wsl","text":"<ul> <li>Wsl</li> <li>Backup &amp; Restore WSL distributions</li> <li>Configuring WSL</li> <li>Install WSL on Windows</li> <li>Troubleshooting WSL</li> </ul>"}]}